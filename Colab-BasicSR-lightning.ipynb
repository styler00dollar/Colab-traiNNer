{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-BasicSR-lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsxYKkWLXju5"
      },
      "source": [
        "# Colab-BasicSR (pytorch lightning)\n",
        "\n",
        "[This tutorial](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09), [this issue](https://stackoverflow.com/questions/65387967/misconfigurationerror-no-tpu-devices-were-found-even-when-tpu-is-connected-in)  and [this Colab](https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/03-basic-gan.ipynb#scrollTo=3vKszYf6y1Vv) were very helpful. This Colab does support single-GPU, multi-GPU and TPU training.\n",
        "\n",
        "Can use various loss functions and has the context_encoder discriminator as default. Currently there are only various inpainting generators from [my BasicSR fork](https://github.com/styler00dollar/Colab-BasicSR).\n",
        "\n",
        "What is not included inside this Colab, but is included in [my normal BasicSR Colab](https://colab.research.google.com/github/styler00dollar/Colab-BasicSR/blob/master/Colab-BasicSR.ipynb):\n",
        "- [ESRGAN](https://github.com/styler00dollar/Colab-BasicSR/blob/master/codes/models/SRRaGAN_model.py)\n",
        "- [edge-informed-sisr](https://github.com/knazeri/edge-informed-sisr/blob/master/src/models.py)\n",
        "- [USRNet](https://github.com/cszn/KAIR/blob/master/models/network_usrnet.py)\n",
        "- [OFT Dataloader](https://github.com/styler00dollar/Colab-BasicSR/tree/master/codes/data)\n",
        "- SWA\n",
        "- Some Discriminators\n",
        "- Some loss functions\n",
        "- DiffAug / Mixup\n",
        "\n",
        "What currently is here but not inside the other Colab:\n",
        "- Custom mask loading\n",
        "\n",
        "Sidenotes:\n",
        "- Does validation on set validation frequency and epoch end\n",
        "- Training can resume from checkpoint, but iteration count will be reset to 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPB6KmEOPQW"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBmBFu3vTlTX",
        "cellView": "form"
      },
      "source": [
        "#@title GPU\n",
        "!pip install pytorch-lightning -U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWDUI7DqflrY",
        "cellView": "form"
      },
      "source": [
        "#@title TPU  (restart runtime afterwards)\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
        "#!pip install pytorch-lightning\n",
        "!pip install lightning-flash\n",
        "\n",
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py > /dev/null\n",
        "!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev > /dev/null\n",
        "!pip install pytorch-lightning > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0I03TFfDaqtR"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzLIdqOkNSsT"
      },
      "source": [
        "Paths:\n",
        "```\n",
        "/content/data (rgb data)\n",
        "/content/masks (1 channel masks, black = mask, white = original image)\n",
        "/content/validation (images for validation)\n",
        "/content/validation_output (validation destination, will be created if not present)\n",
        "/content/test (rgb data)\n",
        "/content/test_output (test output, will be created if not present)\n",
        "```\n",
        "By default, random masks will have 50% chance and custom masks will have 50% chance. Current validation does not rely on metrics and will take a green masked LR image as input, but metrics are added and only need a custom dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fSZHn8ayFM"
      },
      "source": [
        "# copy data somehow\n",
        "!mkdir '/content/data'\n",
        "!mkdir '/content/data/images'\n",
        "!cp \"/content/drive/MyDrive/classification_v2.7z\" \"/content/data/images/data.7z\"\n",
        "%cd /content/data/images\n",
        "!7z x \"data.7z\"\n",
        "!rm -rf /content/data/images/data.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQYzPLBdbD-w",
        "cellView": "form"
      },
      "source": [
        "#@title getting pytorch-loss-functions\n",
        "%cd /content\n",
        "!git clone https://github.com/styler00dollar/pytorch-loss-functions pytorchloss\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-vTO-jUfMhT"
      },
      "source": [
        "# restart from here if you reset your notebook\n",
        "%cd /content/pytorchloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EfvO0fBULzf",
        "cellView": "form"
      },
      "source": [
        "#@title utils.py\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def list2nparray(lst, dtype=None):\n",
        "    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n",
        "    if isinstance(lst, np.ndarray):\n",
        "        return lst\n",
        "    assert isinstance(lst, (list, tuple)), 'bad type: {}'.format(type(lst))\n",
        "    assert lst, 'attempt to convert empty list to np array'\n",
        "    if isinstance(lst[0], np.ndarray):\n",
        "        dim1 = lst[0].shape\n",
        "        assert all(i.shape == dim1 for i in lst)\n",
        "        if dtype is None:\n",
        "            dtype = lst[0].dtype\n",
        "            assert all(i.dtype == dtype for i in lst), \\\n",
        "                'bad dtype: {} {}'.format(dtype, set(i.dtype for i in lst))\n",
        "    elif isinstance(lst[0], (int, float, complex, np.number)):\n",
        "        return np.array(lst, dtype=dtype)\n",
        "    else:\n",
        "        dim1 = list2nparray(lst[0])\n",
        "        if dtype is None:\n",
        "            dtype = dim1.dtype\n",
        "        dim1 = dim1.shape\n",
        "    shape = [len(lst)] + list(dim1)\n",
        "    rst = np.empty(shape, dtype=dtype)\n",
        "    for idx, i in enumerate(lst):\n",
        "        rst[idx] = i\n",
        "    return rst\n",
        "\n",
        "\n",
        "def get_img_list(path):\n",
        "    return sorted(list(Path(path).glob('*.png'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpg'))) + \\\n",
        "        sorted(list(Path(path).glob('*.jpeg')))\n",
        "\n",
        "\n",
        "def gen_miss(img, mask, output):\n",
        "\n",
        "    imgs = get_img_list(img)\n",
        "    masks = get_img_list(mask)\n",
        "    print('Total images:', len(imgs), len(masks))\n",
        "\n",
        "    out = Path(output)\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for i, (img, mask) in tqdm(enumerate(zip(imgs, masks))):\n",
        "        path = out.joinpath('miss_%04d.png' % (i+1))\n",
        "        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "        mask = cv2.imread(str(mask), cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img.shape[:2][::-1])\n",
        "        mask = mask[..., np.newaxis]\n",
        "        miss = img * (mask > 127) + 255 * (mask <= 127)\n",
        "        cv2.imwrite(str(path), miss)\n",
        "\n",
        "def merge_imgs(dirs, output, row=1, gap=2, res=512):\n",
        "\n",
        "    image_list = [get_img_list(path) for path in dirs]\n",
        "    img_count = [len(image) for image in image_list]\n",
        "    print('Total images:', img_count)\n",
        "    assert min(img_count) > 0, 'Please check the path of empty folder.'\n",
        "\n",
        "    output_dir = Path(output)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    n_img = len(dirs)\n",
        "    row = row\n",
        "    column = (n_img - 1) // row + 1\n",
        "    print('Row:', row)\n",
        "    print('Column:', column)\n",
        "\n",
        "    for i, unit in tqdm(enumerate(zip(*image_list))):\n",
        "        name = output_dir.joinpath('merge_%04d.png' % i)\n",
        "        merge = np.ones([\n",
        "            res*row + (row+1)*gap, res*column + (column+1)*gap, 3], np.uint8) * 255\n",
        "        for j, img in enumerate(unit):\n",
        "            r = j // column\n",
        "            c = j - r * column\n",
        "            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
        "            if img.shape[:2] != (res, res):\n",
        "                img = cv2.resize(img, (res, res))\n",
        "            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n",
        "            merge[start_h: start_h + res, start_w: start_w + res] = img\n",
        "        cv2.imwrite(str(name), merge)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzAz39HJuJGy",
        "cellView": "form"
      },
      "source": [
        "#@title metrics.py (removing lpips import)\n",
        "%%writefile /content/pytorchloss/metrics.py\n",
        "#https://github.com/huster-wgm/Pytorch-metrics/blob/master/metrics.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\"\"\"\n",
        "  @Email:  guangmingwu2010@gmail.com \\\n",
        "           guozhilingty@gmail.com\n",
        "  @Copyright: go-hiroaki & Chokurei\n",
        "  @License: MIT\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import lpips\n",
        "\n",
        "eps = 1e-6\n",
        "\n",
        "def _binarize(y_data, threshold):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : [float] 4-d tensor in [batch_size, channels, img_rows, img_cols]\n",
        "        threshold : [float] [0.0, 1.0]\n",
        "    return 4-d binarized y_data\n",
        "    \"\"\"\n",
        "    y_data[y_data < threshold] = 0.0\n",
        "    y_data[y_data >= threshold] = 1.0\n",
        "    return y_data\n",
        "\n",
        "def _argmax(y_data, dim):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_data : 4-d tensor in [batch_size, chs, img_rows, img_cols]\n",
        "        dim : int\n",
        "    return 3-d [int] y_data\n",
        "    \"\"\"\n",
        "    return torch.argmax(y_data, dim).int()\n",
        "\n",
        "\n",
        "def _get_tp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "        y_pred : [int] 3-d in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_positive\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_fp(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_positive\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * y_pred).float()\n",
        "\n",
        "\n",
        "def _get_tn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] true_negative\n",
        "    \"\"\"\n",
        "    return torch.sum((1 - y_true) * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_fn(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        y_pred : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "    return [float] false_negative\n",
        "    \"\"\"\n",
        "    return torch.sum(y_true * (1 - y_pred)).float()\n",
        "\n",
        "\n",
        "def _get_weights(y_true, nb_ch):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        y_true : 3-d ndarray in [batch_size, img_rows, img_cols]\n",
        "        nb_ch : int\n",
        "    return [float] weights\n",
        "    \"\"\"\n",
        "    batch_size, img_rows, img_cols = y_true.shape\n",
        "    pixels = batch_size * img_rows * img_cols\n",
        "    weights = [torch.sum(y_true==ch).item() / pixels for ch in range(nb_ch)]\n",
        "    return weights\n",
        "\n",
        "\n",
        "class CFMatrix(object):\n",
        "    def __init__(self, des=None):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ConfusionMatrix\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return confusion matrix\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 4).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch), :] = [nb_tp, nb_fp, nb_tn, nb_fn]\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class OAAcc(object):\n",
        "    def __init__(self, des=\"Overall Accuracy\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"OAcc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (tp+tn)/total\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "\n",
        "        nb_tp_tn = torch.sum(y_true == y_pred).float()\n",
        "        mperforms = nb_tp_tn / (batch_size * img_rows * img_cols)\n",
        "        performs = None\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Precision(object):\n",
        "    def __init__(self, des=\"Precision\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Prec\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fp)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Recall(object):\n",
        "    def __init__(self, des=\"Recall\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Reca\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return tp/(tp+fn)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            mperforms = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                performs[int(ch)] = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class F1Score(object):\n",
        "    def __init__(self, des=\"F1Score\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"F1Sc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return 2*precision*recall/(precision+recall)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "            _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "            mperforms = 2 * _precision * _recall / (_precision + _recall + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                _precision = nb_tp / (nb_tp + nb_fp + esp)\n",
        "                _recall = nb_tp / (nb_tp + nb_fn + esp)\n",
        "                performs[int(ch)] = 2 * _precision * \\\n",
        "                    _recall / (_precision + _recall + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Kappa(object):\n",
        "    def __init__(self, des=\"Kappa\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Kapp\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return (Po-Pe)/(1-Pe)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            nb_tp = _get_tp(y_pred, y_true)\n",
        "            nb_fp = _get_fp(y_pred, y_true)\n",
        "            nb_tn = _get_tn(y_pred, y_true)\n",
        "            nb_fn = _get_fn(y_pred, y_true)\n",
        "            nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "            Po = (nb_tp + nb_tn) / nb_total\n",
        "            Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn) +\n",
        "                  (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "            mperforms = (Po - Pe) / (1 - Pe + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                nb_tp = _get_tp(y_pred_ch, y_true_ch)\n",
        "                nb_fp = _get_fp(y_pred_ch, y_true_ch)\n",
        "                nb_tn = _get_tn(y_pred_ch, y_true_ch)\n",
        "                nb_fn = _get_fn(y_pred_ch, y_true_ch)\n",
        "                nb_total = nb_tp + nb_fp + nb_tn + nb_fn\n",
        "                Po = (nb_tp + nb_tn) / nb_total\n",
        "                Pe = ((nb_tp + nb_fp) * (nb_tp + nb_fn)\n",
        "                      + (nb_fn + nb_tn) * (nb_fp + nb_tn)) / (nb_total**2)\n",
        "                performs[int(ch)] = (Po - Pe) / (1 - Pe + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class Jaccard(object):\n",
        "    def __init__(self, des=\"Jaccard\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Jacc\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, threshold=0.5):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, chs, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return intersection / (sum-intersection)\n",
        "        \"\"\"\n",
        "        batch_size, chs, img_rows, img_cols = y_true.shape\n",
        "        device = y_true.device\n",
        "        if chs == 1:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "            y_true = _binarize(y_true, threshold)\n",
        "            _intersec = torch.sum(y_true * y_pred).float()\n",
        "            _sum = torch.sum(y_true + y_pred).float()\n",
        "            mperforms = _intersec / (_sum - _intersec + esp)\n",
        "            performs = None\n",
        "        else:\n",
        "            y_pred = _argmax(y_pred, 1)\n",
        "            y_true = _argmax(y_true, 1)\n",
        "            performs = torch.zeros(chs, 1).to(device)\n",
        "            weights = _get_weights(y_true, chs)\n",
        "            for ch in range(chs):\n",
        "                y_true_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_pred_ch = torch.zeros(batch_size, img_rows, img_cols)\n",
        "                y_true_ch[y_true == ch] = 1\n",
        "                y_pred_ch[y_pred == ch] = 1\n",
        "                _intersec = torch.sum(y_true_ch * y_pred_ch).float()\n",
        "                _sum = torch.sum(y_true_ch + y_pred_ch).float()\n",
        "                performs[int(ch)] = _intersec / (_sum - _intersec + esp)\n",
        "            mperforms = sum([i*j for (i, j) in zip(performs, weights)])\n",
        "        return mperforms, performs\n",
        "\n",
        "\n",
        "class MSE(object):\n",
        "    def __init__(self, des=\"Mean Square Error\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return mean_squared_error, smaller the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        return torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "\n",
        "class PSNR(object):\n",
        "    def __init__(self, des=\"Peak Signal to Noise Ratio\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"PSNR\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            threshold : [0.0, 1.0]\n",
        "        return PSNR, larger the better\n",
        "        \"\"\"\n",
        "        if threshold:\n",
        "            y_pred = _binarize(y_pred, threshold)\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return 10 * torch.log10(1 / mse)\n",
        "\n",
        "\n",
        "class SSIM(object):\n",
        "    '''\n",
        "    modified from https://github.com/jorge-pessoa/pytorch-msssim\n",
        "    '''\n",
        "    def __init__(self, des=\"structural similarity index\"):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SSIM\"\n",
        "\n",
        "    def gaussian(self, w_size, sigma):\n",
        "        gauss = torch.Tensor([math.exp(-(x - w_size//2)**2/float(2*sigma**2)) for x in range(w_size)])\n",
        "        return gauss/gauss.sum()\n",
        "\n",
        "    def create_window(self, w_size, channel=1):\n",
        "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
        "        return window\n",
        "\n",
        "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            w_size : int, default 11\n",
        "            size_average : boolean, default True\n",
        "            full : boolean, default False\n",
        "        return ssim, larger the better\n",
        "        \"\"\"\n",
        "        # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
        "        if torch.max(y_pred) > 128:\n",
        "            max_val = 255\n",
        "        else:\n",
        "            max_val = 1\n",
        "\n",
        "        if torch.min(y_pred) < -0.5:\n",
        "            min_val = -1\n",
        "        else:\n",
        "            min_val = 0\n",
        "        L = max_val - min_val\n",
        "\n",
        "        padd = 0\n",
        "        (_, channel, height, width) = y_pred.size()\n",
        "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
        "\n",
        "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
        "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        sigma1_sq = F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "        C1 = (0.01 * L) ** 2\n",
        "        C2 = (0.03 * L) ** 2\n",
        "\n",
        "        v1 = 2.0 * sigma12 + C2\n",
        "        v2 = sigma1_sq + sigma2_sq + C2\n",
        "        cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "        if size_average:\n",
        "            ret = ssim_map.mean()\n",
        "        else:\n",
        "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "        if full:\n",
        "            return ret, cs\n",
        "        return ret\n",
        "\n",
        "\n",
        "class LPIPS(object):\n",
        "    '''\n",
        "    borrowed from https://github.com/richzhang/PerceptualSimilarity\n",
        "    '''\n",
        "    def __init__(self, cuda, des=\"Learned Perceptual Image Patch Similarity\", version=\"0.1\"):\n",
        "        self.des = des\n",
        "        self.version = version\n",
        "        self.model = lpips.PerceptualLoss(model='net-lin',net='alex',use_gpu=cuda)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LPIPS\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true, normalized=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            normalized : change [0,1] => [-1,1] (default by LPIPS)\n",
        "        return LPIPS, smaller the better\n",
        "        \"\"\"\n",
        "        if normalized:\n",
        "            y_pred = y_pred * 2.0 - 1.0\n",
        "            y_true = y_true * 2.0 - 1.0\n",
        "        return self.model.forward(y_pred, y_true)\n",
        "\n",
        "\n",
        "class AE(object):\n",
        "    \"\"\"\n",
        "    Modified from matlab : colorangle.m, MATLAB V2019b\n",
        "    angle = acos(RGB1' * RGB2 / (norm(RGB1) * norm(RGB2)));\n",
        "    angle = 180 / pi * angle;\n",
        "    \"\"\"\n",
        "    def __init__(self, des='average Angular Error'):\n",
        "        self.des = des\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AE\"\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            y_true : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "            y_pred : 4-d ndarray in [batch_size, channels, img_rows, img_cols]\n",
        "        return average AE, smaller the better\n",
        "        \"\"\"\n",
        "        dotP = torch.sum(y_pred * y_true, dim=1)\n",
        "        Norm_pred = torch.sqrt(torch.sum(y_pred * y_pred, dim=1))\n",
        "        Norm_true = torch.sqrt(torch.sum(y_true * y_true, dim=1))\n",
        "        ae = 180 / math.pi * torch.acos(dotP / (Norm_pred * Norm_true + eps))\n",
        "        return ae.mean(1).mean(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for ch in [3, 1]:\n",
        "        batch_size, img_row, img_col = 1, 224, 224\n",
        "        y_true = torch.rand(batch_size, ch, img_row, img_col)\n",
        "        noise = torch.zeros(y_true.size()).data.normal_(0, std=0.1)\n",
        "        y_pred = y_true + noise\n",
        "        for cuda in [False, True]:\n",
        "            if cuda:\n",
        "                y_pred = y_pred.cuda()\n",
        "                y_true = y_true.cuda()\n",
        "\n",
        "            print('#'*20, 'Cuda : {} ; size : {}'.format(cuda, y_true.size()))\n",
        "            ########### similarity metrics\n",
        "            metric = MSE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = PSNR()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = SSIM()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = LPIPS(cuda)\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            metric = AE()\n",
        "            acc = metric(y_pred, y_true).item()\n",
        "            print(\"{} ==> {}\".format(repr(metric), acc))\n",
        "\n",
        "            ########### accuracy metrics\n",
        "            metric = OAAcc()\n",
        "            maccu, accu = metric(y_pred, y_true)\n",
        "            print('mAccu:', maccu, 'Accu', accu)\n",
        "\n",
        "            metric = Precision()\n",
        "            mprec, prec = metric(y_pred, y_true)\n",
        "            print('mPrec:', mprec, 'Prec', prec)\n",
        "\n",
        "            metric = Recall()\n",
        "            mreca, reca = metric(y_pred, y_true)\n",
        "            print('mReca:', mreca, 'Reca', reca)\n",
        "\n",
        "            metric = F1Score()\n",
        "            mf1sc, f1sc = metric(y_pred, y_true)\n",
        "            print('mF1sc:', mf1sc, 'F1sc', f1sc)\n",
        "\n",
        "            metric = Kappa()\n",
        "            mkapp, kapp = metric(y_pred, y_true)\n",
        "            print('mKapp:', mkapp, 'Kapp', kapp)\n",
        "\n",
        "            metric = Jaccard()\n",
        "            mjacc, jacc = metric(y_pred, y_true)\n",
        "            print('mJacc:', mjacc, 'Jacc', jacc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HozYJLs0B67",
        "cellView": "form"
      },
      "source": [
        "#@title partialconv.py\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "#\n",
        "# Source: https://github.com/NVIDIA/partialconv/blob/master/models/partialconv2d.py\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, cuda\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False  \n",
        "\n",
        "        if 'return_mask' in kwargs:\n",
        "            self.return_mask = kwargs['return_mask']\n",
        "            kwargs.pop('return_mask')\n",
        "        else:\n",
        "            self.return_mask = False\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "            \n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None, None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask_in=None):\n",
        "        assert len(input.shape) == 4\n",
        "        if mask_in is not None or self.last_size != tuple(input.shape):\n",
        "            self.last_size = tuple(input.shape)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask_in is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                else:\n",
        "                    mask = mask_in\n",
        "                        \n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                #make sure the value of self.mask_ratio for the entries in the interior (no need for padding) have value 1. If not, you replace with the line below.\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        # if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "        #     self.update_mask.to(input)\n",
        "        #     self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-5xc1MR0F-K",
        "cellView": "form"
      },
      "source": [
        "#@title deformconv2d.py\n",
        "import torch.nn as nn\n",
        "import torchvision.ops as O\n",
        "\n",
        "\n",
        "class DeformConv2d(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True):\n",
        "        super(DeformConv2d, self).__init__()\n",
        "\n",
        "        self.conv_offset = nn.Conv2d(in_nc, 2 * (kernel_size**2), kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.conv_offset.weight.data.zero_()\n",
        "        self.conv_offset.bias.data.zero_()\n",
        "\n",
        "        self.dcn_conv = O.DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset = self.conv_offset(x)\n",
        "        return self.dcn_conv(x, offset=offset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_-I2mZx9Ru"
      },
      "source": [
        "Inside the model it is possible to configure loss functions and weights. Warning: Don't use AMP with StyleLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bwPuRzjW40R",
        "cellView": "form"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ccjLoMPGoT",
        "cellView": "form"
      },
      "source": [
        "#@title data.py\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, root, transform=None, size=256):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mask_dir = '/content/masks'\n",
        "        self.files = glob.glob(self.mask_dir + '/**/*.png', recursive=True)\n",
        "        files_jpg = glob.glob(self.mask_dir + '/**/*.jpg', recursive=True)\n",
        "        self.files.extend(files_jpg)\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        sample = Image.open(sample_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(np.array(sample), cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)/255\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        if random.uniform(0, 1) < 0.5:\n",
        "          # generating mask automatically with 50% chance\n",
        "          mask = DS.random_mask(height=self.size, width=self.size)\n",
        "          mask = torch.from_numpy(mask)\n",
        "\n",
        "        else:\n",
        "          # load random mask from folder\n",
        "          mask = cv2.imread(random.choice([x for x in self.files]), cv2.IMREAD_UNCHANGED)\n",
        "          mask = cv2.resize(mask, (self.size,self.size), interpolation=cv2.INTER_NEAREST)\n",
        "          mask = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)/255\n",
        "\n",
        "        #sample = torch.from_numpy(sample)\n",
        "        sample = transforms.ToTensor()(sample)\n",
        "\n",
        "        # apply mask\n",
        "        masked = sample * mask\n",
        "        return masked, mask, sample\n",
        "\n",
        "        # EdgeConnect\n",
        "        #return masked, mask, sample, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return masked, mask, sample, edges\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_mask(height=256, width=256,\n",
        "                    min_stroke=1, max_stroke=4,\n",
        "                    min_vertex=1, max_vertex=12,\n",
        "                    min_brush_width_divisor=16, max_brush_width_divisor=10):\n",
        "        mask = np.ones((height, width))\n",
        "\n",
        "        min_brush_width = height // min_brush_width_divisor\n",
        "        max_brush_width = height // max_brush_width_divisor\n",
        "        max_angle = 2*np.pi\n",
        "        num_stroke = np.random.randint(min_stroke, max_stroke+1)\n",
        "        average_length = np.sqrt(height*height + width*width) / 8\n",
        "\n",
        "        for _ in range(num_stroke):\n",
        "            num_vertex = np.random.randint(min_vertex, max_vertex+1)\n",
        "            start_x = np.random.randint(width)\n",
        "            start_y = np.random.randint(height)\n",
        "\n",
        "            for _ in range(num_vertex):\n",
        "                angle = np.random.uniform(max_angle)\n",
        "                length = np.clip(np.random.normal(average_length, average_length//2), 0, 2*average_length)\n",
        "                brush_width = np.random.randint(min_brush_width, max_brush_width+1)\n",
        "                end_x = (start_x + length * np.sin(angle)).astype(np.int32)\n",
        "                end_y = (start_y + length * np.cos(angle)).astype(np.int32)\n",
        "\n",
        "                cv2.line(mask, (start_y, start_x), (end_y, end_x), 0., brush_width)\n",
        "\n",
        "                start_x, start_y = end_x, end_y\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.fliplr(mask)\n",
        "        if np.random.random() < 0.5:\n",
        "            mask = np.flipud(mask)\n",
        "        return mask.reshape((1,)+mask.shape).astype(np.float32) \n",
        "\n",
        "\n",
        "\n",
        "class DS_green_from_mask(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []\n",
        "        for root, _, fnames in sorted(os.walk(root)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                self.samples.append(path)\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"Found 0 files in subfolders of: \" + root)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.samples[index]\n",
        "        #sample = Image.open(sample_path).convert('RGB')\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # if edges are required\n",
        "        grayscale = cv2.cvtColor(sample, cv2.COLOR_RGB2GRAY)\n",
        "        edges = cv2.Canny(grayscale,100,150)\n",
        "        grayscale = torch.from_numpy(grayscale).unsqueeze(0)\n",
        "        edges = torch.from_numpy(edges).unsqueeze(0)\n",
        "\n",
        "        green_mask = 1-np.all(sample == [0,255,0], axis=-1).astype(int)\n",
        "        green_mask = torch.from_numpy(green_mask).unsqueeze(0)\n",
        "        sample = torch.from_numpy(sample.astype(np.float32)).permute(2, 0, 1)/255\n",
        "        sample = sample * green_mask\n",
        "\n",
        "        # train_batch[0] = masked\n",
        "        # train_batch[1] = mask\n",
        "        # train_batch[2] = path\n",
        "        return sample, green_mask, sample_path \n",
        "\n",
        "        # EdgeConnect\n",
        "        #return sample, green_mask, sample_path, edges, grayscale\n",
        "\n",
        "        # PRVS\n",
        "        #return sample, green_mask, sample_path, edges\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWnqGVcPcvm",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DFNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = 256\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "            #transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.DFNetdataset_train = DS(self.training_dir, img_tf, self.size)\n",
        "        self.DFNetdataset_validation = DS_green_from_mask(self.validation_dir, img_tf)\n",
        "        self.DFNetdataset_test = DS_green_from_mask(self.test_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DFNetdataset_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNKbsu4VrQD4",
        "cellView": "form"
      },
      "source": [
        "#@title discriminator.py\n",
        "\"\"\"\n",
        "models.py (21-12-20)\n",
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/context_encoder/models.py\n",
        "\"\"\"\n",
        "class context_encoder(pl.LightningModule):\n",
        "    def __init__(self, channels=3):\n",
        "        super(context_encoder, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        layers = []\n",
        "        in_filters = channels\n",
        "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
        "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
        "            in_filters = out_filters\n",
        "\n",
        "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAV2wG-rbLY",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss, StyleLoss\n",
        "from vic.perceptual_loss import PerceptualLoss\n",
        "from metrics import *\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    ############################\n",
        "    # generators with one output, no AMP means nan loss during training\n",
        "    # DFNet\n",
        "    #self.netG = DFNet(c_img=3, c_mask=1, c_alpha=3,\n",
        "    #        mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "    #        en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3, 3, 3, 3, 3, 3, 3, 3],\n",
        "    #        blend_layers=[0, 1, 2, 3, 4, 5], conv_type='partial')\n",
        "    \n",
        "    # AdaFill\n",
        "    #self.netG = InpaintNet()\n",
        "\n",
        "    # MEDFE (batch_size: 1, no AMP)\n",
        "    #self.netG = MEDFEGenerator()\n",
        "\n",
        "    # RFR (no AMP)\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = RFRNet(conv_type='partial')\n",
        "\n",
        "    # LBAM\n",
        "    #self.netG = LBAMModel(inputChannels=4, outputChannels=3)\n",
        "\n",
        "    # DMFN\n",
        "    #self.netG = InpaintingGenerator(in_nc=4, out_nc=3,nf=64,n_res=8,\n",
        "    #      norm='in', activation='relu')\n",
        "\n",
        "    # partial\n",
        "    #self.netG = Model()\n",
        "\n",
        "    # RN\n",
        "    #self.netG = G_Net(input_channels=3, residual_blocks=8, threshold=0.8)\n",
        "    # using rn init to avoid errors\n",
        "    #RN_arch = rn_initialize_weights(self.netG, scale=0.1)\n",
        "\n",
        "\n",
        "    ############################\n",
        "\n",
        "    # generators with two outputs\n",
        "\n",
        "    # deepfillv1\n",
        "    #self.netG = InpaintSANet()\n",
        "\n",
        "    # deepfillv2\n",
        "    # conv_type = partial or deform\n",
        "    #self.netG = GatedGenerator(in_channels=4, out_channels=3, \n",
        "    #  latent_channels=64, pad_type='zero', activation='lrelu', norm='in', conv_type = 'partial')\n",
        "\n",
        "    # [BROKEN?] crfill\n",
        "    #self.netG = InpaintGenerator(cnum=48)\n",
        "    \n",
        "    # Adaptive\n",
        "    self.netG = PyramidNet(in_channels=3, residual_blocks=1, init_weights='True')\n",
        "\n",
        "    ############################\n",
        "    # exotic generators\n",
        "\n",
        "    # Pluralistic\n",
        "    #self.netG = PluralisticGenerator(ngf_E=opt_net['ngf_E'], z_nc_E=opt_net['z_nc_E'], img_f_E=opt_net['img_f_E'], layers_E=opt_net['layers_E'], norm_E=opt_net['norm_E'], activation_E=opt_net['activation_E'],\n",
        "    #            ngf_G=opt_net['ngf_G'], z_nc_G=opt_net['z_nc_G'], img_f_G=opt_net['img_f_G'], L_G=opt_net['L_G'], output_scale_G=opt_net['output_scale_G'], norm_G=opt_net['norm_G'], activation_G=opt_net['activation_G'])\n",
        "\n",
        "    \n",
        "    # EdgeConnect\n",
        "    #conv_type_edge: 'normal' # normal | partial | deform (has no spectral_norm)\n",
        "    #self.netG = EdgeConnectModel(residual_blocks_edge=8,\n",
        "    #        residual_blocks_inpaint=8, use_spectral_norm=True,\n",
        "    #        conv_type_edge='normal', conv_type_inpaint='normal')\n",
        "\n",
        "    # FRRN (no AMP)\n",
        "    #self.netG = FRRNet()\n",
        "\n",
        "    # PRVS (no AMP)\n",
        "    #self.netG = PRVSNet()\n",
        "\n",
        "    # CSA\n",
        "    #self.netG = InpaintNet(c_img=3, norm='instance', act_en='leaky_relu', \n",
        "    #                           act_de='relu')\n",
        "\n",
        "    ############################\n",
        "    #weights_init(self.netG, 'kaiming')\n",
        "\n",
        "    self.netD = context_encoder()\n",
        "    weights_init(self.netD, 'kaiming')\n",
        "\n",
        "\n",
        "    # loss functions\n",
        "    self.l1 = nn.L1Loss()\n",
        "    l_hfen_type = L1CosineSim()\n",
        "    self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "    self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "    self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "    self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "    self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "    self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "    self.OFLoss = OFLoss()\n",
        "    self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "    self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "    self.StyleLoss = StyleLoss()\n",
        "    self.TVLoss = TVLoss(tv_type='tv', p = 1)\n",
        "    self.PerceptualLoss = PerceptualLoss(model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0], model_path=None)\n",
        "    layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "        distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "        use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "    self.MSELoss = torch.nn.MSELoss()\n",
        "    self.L1Loss = nn.L1Loss()\n",
        "\n",
        "    # metrics\n",
        "    self.psnr_metric = PSNR()\n",
        "    self.ssim_metric = SSIM()\n",
        "    self.ae_metric = AE()\n",
        "    self.mse_metric = MSE()\n",
        "\n",
        "\n",
        "  def forward(self, image, masks):\n",
        "      return self.netG(image, masks)\n",
        "\n",
        "  #def adversarial_loss(self, y_hat, y):\n",
        "  #    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      # train_batch[0][0] = batch_size\n",
        "      # train_batch[0] = masked\n",
        "      # train_batch[1] = mask\n",
        "      # train_batch[2] = original\n",
        "\n",
        "      # train generator\n",
        "      ############################\n",
        "      # generate fake (1 output)\n",
        "      out = self(train_batch[0],train_batch[1])\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # generate fake (2 outputs)\n",
        "      #out, other_img = self(train_batch[0],train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # exotic generators\n",
        "      # CSA\n",
        "      #coarse_result, out, csa, csa_d = self(train_batch[0],train_batch[1])\n",
        "      \n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #out, other_img = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "      \n",
        "      # PVRS\n",
        "      #out, _ ,edge_small, edge_big = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "      # FRRN\n",
        "      #out, mid_x, mid_mask = self(train_batch[0], train_batch[1])\n",
        "\n",
        "      # masking, taking original content from HR\n",
        "      #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "      ############################\n",
        "      # loss calculation\n",
        "      total_loss = 0\n",
        "      \"\"\"\n",
        "      HFENLoss_forward = self.HFENLoss(out, train_batch[0])\n",
        "      total_loss += HFENLoss_forward\n",
        "      ElasticLoss_forward = self.ElasticLoss(out, train_batch[0])\n",
        "      total_loss += ElasticLoss_forward\n",
        "      RelativeL1_forward = self.RelativeL1(out, train_batch[0])\n",
        "      total_loss += RelativeL1_forward\n",
        "      \"\"\"\n",
        "      #print(out)\n",
        "      L1CosineSim_forward = 5*self.L1CosineSim(out, train_batch[2])\n",
        "      total_loss += L1CosineSim_forward\n",
        "      self.log('loss/L1CosineSim', L1CosineSim_forward)\n",
        "\n",
        "      \"\"\"\n",
        "      ClipL1_forward = self.ClipL1(out, train_batch[0])\n",
        "      total_loss += ClipL1_forward\n",
        "      FFTloss_forward = self.FFTloss(out, train_batch[0])\n",
        "      total_loss += FFTloss_forward\n",
        "      OFLoss_forward = self.OFLoss(out)\n",
        "      total_loss += OFLoss_forward\n",
        "      GPLoss_forward = self.GPLoss(out, train_batch[0])\n",
        "      total_loss += GPLoss_forward\n",
        "      \n",
        "      CPLoss_forward = 0.1*self.CPLoss(out, train_batch[0])\n",
        "      total_loss += CPLoss_forward\n",
        "      \n",
        "\n",
        "      Contextual_Loss_forward = self.Contextual_Loss(out, train_batch[0])\n",
        "      total_loss += Contextual_Loss_forward\n",
        "      self.log('loss/contextual', Contextual_Loss_forward)\n",
        "      \"\"\"\n",
        "\n",
        "      #style_forward = 240*self.StyleLoss(out, train_batch[2])\n",
        "      #total_loss += style_forward\n",
        "      #self.log('loss/style', style_forward)\n",
        "\n",
        "      tv_forward = 0.0000005*self.TVLoss(out)\n",
        "      total_loss += tv_forward\n",
        "      self.log('loss/tv', tv_forward)\n",
        "\n",
        "      perceptual_forward = 2*self.PerceptualLoss(out, train_batch[2])\n",
        "      total_loss += perceptual_forward\n",
        "      self.log('loss/perceptual', perceptual_forward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #########################\n",
        "      # exotic loss\n",
        "\n",
        "      # if model has two output, also calculate loss for such an image\n",
        "      # example with just l1 loss\n",
        "      \n",
        "      #l1_stage1 = self.L1Loss(other_img, train_batch[0])\n",
        "      #self.log('loss/l1_stage1', l1_stage1)\n",
        "      #total_loss += l1_stage1\n",
        "\n",
        "\n",
        "      # pennet loss\n",
        "      \"\"\"\n",
        "      if other_img is not None:\n",
        "        pyramid_loss = 0\n",
        "        for _, f in enumerate(other_img):\n",
        "          pyramid_loss += 0.07*self.L1Loss(f, torch.nn.functional.interpolate(train_batch[2], size=f.size()[2:4], mode='bilinear', align_corners=True))\n",
        "\n",
        "      self.log('loss/pyramid_loss', pyramid_loss)\n",
        "      \"\"\"\n",
        "\n",
        "      # CSA Loss\n",
        "      \"\"\"\n",
        "      recon_loss = self.L1Loss(coarse_result, train_batch[2]) + self.L1Loss(out, train_batch[2])\n",
        "      cons = ConsistencyLoss()\n",
        "      cons_loss = cons(csa, csa_d, train_batch[2], train_batch[1])\n",
        "      self.log('loss/recon_loss', recon_loss)\n",
        "      total_loss += recon_loss\n",
        "      self.log('loss/cons_loss', cons_loss)\n",
        "      total_loss += cons_loss\n",
        "      \"\"\"\n",
        "\n",
        "      # EdgeConnect\n",
        "      # train_batch[3] = edges\n",
        "      # train_batch[4] = grayscale\n",
        "      #l1_edge = self.L1Loss(other_img, train_batch[3])\n",
        "      #self.log('loss/l1_edge', l1_edge)\n",
        "      #total_loss += l1_edge\n",
        "\n",
        "      # PVRS\n",
        "      \"\"\"\n",
        "      edge_big_l1 = self.L1Loss(edge_big, train_batch[3])\n",
        "      edge_small_l1 = self.L1Loss(edge_small, torch.nn.functional.interpolate(train_batch[3], scale_factor = 0.5))\n",
        "      self.log('loss/edge_big_l1', edge_big_l1)\n",
        "      total_loss += edge_big_l1\n",
        "      self.log('loss/edge_small_l1', edge_small_l1)\n",
        "      total_loss += edge_small_l1\n",
        "      \"\"\"\n",
        "\n",
        "      # FRRN\n",
        "      \"\"\"\n",
        "      mid_l1_loss = 0\n",
        "      for idx in range(len(mid_x) - 1):\n",
        "          mid_l1_loss += self.L1Loss(mid_x[idx] * mid_mask[idx], train_batch[2] * mid_mask[idx])\n",
        "      self.log('loss/mid_l1_loss', mid_l1_loss)\n",
        "      total_loss += mid_l1_loss\n",
        "      \"\"\"\n",
        "\n",
        "      self.log('loss/g_loss', total_loss)\n",
        "\n",
        "      #return total_loss\n",
        "      #########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # train discriminator\n",
        "      Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "      valid = Variable(Tensor(out.shape).fill_(1.0), requires_grad=False)\n",
        "      fake = Variable(Tensor(out.shape).fill_(0.0), requires_grad=False)\n",
        "      dis_real_loss = self.MSELoss(train_batch[2], valid)\n",
        "      dis_fake_loss = self.MSELoss(out, fake)\n",
        "\n",
        "      d_loss = (dis_real_loss + dis_fake_loss) / 2\n",
        "      self.log('loss/d_loss', d_loss)\n",
        "\n",
        "      return total_loss+d_loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.netG.parameters(), lr=2e-3)\n",
        "      return optimizer\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "\n",
        "    #########################\n",
        "    # generate fake (one output generator)\n",
        "    out = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    #########################\n",
        "    # generate fake (two output generator)\n",
        "  \n",
        "    #out, _ = self(train_batch[0],train_batch[1])\n",
        "\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "    #########################\n",
        "    # CSA\n",
        "    #_, out, _, _ = self(train_batch[0],train_batch[1])\n",
        "    # masking, taking original content from HR\n",
        "    #out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    # EdgeConnect\n",
        "    # train_batch[3] = edges\n",
        "    # train_batch[4] = grayscale\n",
        "    #out, _ = self.netG(train_batch[0], train_batch[3], train_batch[4], train_batch[1])\n",
        "\n",
        "    # PVRS\n",
        "    #out, _ ,_, _ = self.netG(train_batch[0], train_batch[1], train_batch[3])\n",
        "\n",
        "    # FRRN\n",
        "    #out, _, _ = self(train_batch[0], train_batch[1])\n",
        "\n",
        "    \"\"\"\n",
        "    # Validation metrics work, but they need an origial source image, which is\n",
        "    # not implemented. Change dataloader to provide LR and HR if you want metrics.\n",
        "    self.log('metrics/PSNR', self.psnr_metric(train_batch[2], out))\n",
        "    self.log('metrics/SSIM', self.ssim_metric(train_batch[2], out))\n",
        "    self.log('metrics/MSE', self.mse_metric(train_batch[2], out))\n",
        "    self.log('metrics/LPIPS', self.PerceptualLoss(out, train_batch[2]))\n",
        "    \"\"\"\n",
        "\n",
        "    validation_output = '/content/validation_output/'\n",
        "\n",
        "    # train_batch[3] can contain multiple files, depending on the batch_size\n",
        "    for f in train_batch[2]:\n",
        "      # data is processed as a batch, to save indididual files, a counter is used\n",
        "      counter = 0\n",
        "      if not os.path.exists(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0])):\n",
        "        os.makedirs(os.path.join(validation_output, os.path.splitext(os.path.basename(f))[0]))\n",
        "\n",
        "      filename_with_extention = os.path.basename(f)\n",
        "      filename = os.path.splitext(filename_with_extention)[0]\n",
        "      save_image(out[counter], os.path.join(validation_output, filename, str(self.trainer.global_step) + '.png'))\n",
        "\n",
        "      counter += 1\n",
        "\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "    # train_batch[0] = masked\n",
        "    # train_batch[1] = mask\n",
        "    # train_batch[2] = path\n",
        "    test_output = '/content/test_output/'\n",
        "    if not os.path.exists(test_output):\n",
        "      os.makedirs(test_output)\n",
        "\n",
        "    out = self(train_batch[0].unsqueeze(0),train_batch[1].unsqueeze(0))\n",
        "    out = train_batch[0]*(train_batch[1])+out*(1-train_batch[1])\n",
        "\n",
        "    save_image(out, os.path.join(test_output, os.path.splitext(os.path.basename(train_batch[2]))[0] + '.png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEn4yCogbFaW",
        "cellView": "form"
      },
      "source": [
        "#@title checkpoint.py\n",
        "#https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    \"\"\"\n",
        "    Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n",
        "    based on validation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_step_frequency,\n",
        "        prefix=\"Checkpoint\",\n",
        "        use_modelcheckpoint_filename=False,\n",
        "        save_path = '/content/'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            save_step_frequency: how often to save in steps\n",
        "            prefix: add a prefix to the name, only used if\n",
        "                use_modelcheckpoint_filename=False\n",
        "            use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n",
        "                default filename, don't use ours.\n",
        "        \"\"\"\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "        self.prefix = prefix\n",
        "        self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            if self.use_modelcheckpoint_filename:\n",
        "                filename = trainer.checkpoint_callback.filename\n",
        "            else:\n",
        "                filename = f\"{self.prefix}_{epoch}_{global_step}.ckpt\"\n",
        "            #ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            ckpt_path = os.path.join(self.save_path, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)\n",
        "            # run validation once checkpoint was made\n",
        "            trainer.run_evaluation()\n",
        "\n",
        "#Trainer(callbacks=[CheckpointEveryNSteps()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzBj6jlz5_Ze"
      },
      "source": [
        "# Generators\n",
        "\n",
        "Only run of these cells and confure further inside ``CustomTrainClass``. If you run more, it could maybe cause problems. You should restart the notebook once this happens.\n",
        "\n",
        "Sidenote: Some files use ``.type(torch.cuda.FloatTensor)`` to avoid crashing. You could also try ``.type(torch.cuda.HalfTensor)``, but this is untested behaviour, but might help with AMP. ``[no AMP]`` indicates ``loss=nan`` if you actually try to use AMP.\n",
        "\n",
        "With one output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3w2bvCS6AoH",
        "cellView": "form"
      },
      "source": [
        "#@title [AdaFill_arch.py](https://github.com/ChajinShin/AdaFill-Image_Inpainting) (2021)\n",
        "\"\"\"\n",
        "network.py (3-2-20)\n",
        "https://github.com/ChajinShin/AdaFill-Image_Inpainting/blob/main/Model/AdaFill/src/network.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class ResModule(pl.LightningModule):\n",
        "    def __init__(self, num_features, normalization):\n",
        "        super(ResModule, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            normalization(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1),\n",
        "            normalization(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "\n",
        "class InpaintNet(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(InpaintNet, self).__init__()\n",
        "        cnum = 64\n",
        "        num_of_resblock = 8\n",
        "        normalization_config = 'batch_norm'\n",
        "\n",
        "        if normalization_config == 'batch_norm':\n",
        "            normalization = nn.BatchNorm2d\n",
        "        elif normalization_config == 'instance_norm':\n",
        "            normalization = nn.InstanceNorm2d\n",
        "        else:\n",
        "            raise ValueError(\"batch normalization or instance normalization is only available\")\n",
        "\n",
        "        self.enc_conv1 = nn.Conv2d(in_channels=4, out_channels=cnum, kernel_size=3, stride=1, padding=1)  # cnum, 128, 128\n",
        "        self.enc_norm1 = normalization(num_features=cnum)\n",
        "        self.enc_activation1 = nn.ReLU(inplace=True)\n",
        "        self.enc_conv2 = nn.Conv2d(in_channels=cnum, out_channels=2*cnum, kernel_size=3, stride=2, padding=1)  # 2cnum, 64, 64\n",
        "        self.enc_norm2 = normalization(num_features=2*cnum)\n",
        "        self.enc_activation2 = nn.ReLU(inplace=True)\n",
        "        self.enc_conv3 = nn.Conv2d(in_channels=2*cnum, out_channels=4*cnum, kernel_size=3, stride=2, padding=1)  # 4cnum, 32, 32\n",
        "\n",
        "        res = [ResModule(4*cnum, normalization) for _ in range(num_of_resblock)]\n",
        "        self.res_module = nn.Sequential(\n",
        "            *res\n",
        "        )\n",
        "\n",
        "        self.dec_norm1 = normalization(4*cnum)\n",
        "        self.dec_activation1 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv1 = nn.Conv2d(in_channels=4*cnum, out_channels=2*cnum, kernel_size=3, stride=1, padding=1)    # 2*cnum, 64, 64\n",
        "        self.dec_norm2 = normalization(num_features=2*cnum)\n",
        "        self.dec_activation2 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv2 = nn.Conv2d(in_channels=2*cnum, out_channels=cnum, kernel_size=3, stride=1, padding=1)   # cnum, 128, 128\n",
        "        self.dec_norm3 = normalization(num_features=cnum)\n",
        "        self.dec_activation3 = nn.ReLU(inplace=True)\n",
        "        self.dec_conv3 = nn.Conv2d(in_channels=cnum, out_channels=3, kernel_size=3, stride=1, padding=1)    # 3, 128, 128\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        x = torch.cat((image, mask), 1)\n",
        "        # --------- encoder ----------------\n",
        "        x = self.enc_conv1(x)\n",
        "        x = self.enc_norm1(x)\n",
        "        x = self.enc_activation1(x)\n",
        "        size_1x = [x.size(2), x.size(3)]\n",
        "\n",
        "        x = self.enc_conv2(x)\n",
        "        x = self.enc_norm2(x)\n",
        "        x = self.enc_activation2(x)\n",
        "        size_2x = [x.size(2), x.size(3)]\n",
        "\n",
        "        x = self.enc_conv3(x)\n",
        "\n",
        "        # --------- res module ----------------\n",
        "        x = self.res_module(x)\n",
        "\n",
        "        # --------- decoder ----------------\n",
        "        x = self.dec_norm1(x)\n",
        "        x = self.dec_activation1(x)\n",
        "        x = nn.functional.interpolate(x, size=size_2x)\n",
        "        x = self.dec_conv1(x)\n",
        "\n",
        "        x = self.dec_norm2(x)\n",
        "        x = self.dec_activation2(x)\n",
        "        x = nn.functional.interpolate(x, size=size_1x)\n",
        "        x = self.dec_conv2(x)\n",
        "\n",
        "        x = self.dec_norm3(x)\n",
        "        x = self.dec_activation3(x)\n",
        "        x = self.dec_conv3(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ849AKI6Nvr",
        "cellView": "form"
      },
      "source": [
        "#@title [MEDFE_arch.py](https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE) [no AMP] (2020)\n",
        "\"\"\"\n",
        "Encoder.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/Encoder.py\n",
        "\n",
        "Decoder.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/Decoder.py\n",
        "\n",
        "networks.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/9adf8898a142784976bb3e162a9fd864c224e01e/models/networks.py\n",
        "\n",
        "MEDFE.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/master/models/MEDFE.py\n",
        "\n",
        "PCconv.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/models/PCconv.py\n",
        "\n",
        "Selfpatch.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/util/Selfpatch.py\n",
        "\n",
        "util.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/dd838b01d9786dc2c67de5d71869e5a60da28eb9/util/util.py\n",
        "\n",
        "InnerCos.py (25-12-20)\n",
        "https://github.com/KumapowerLIU/Rethinking-Inpainting-MEDFE/blob/c7156eab4a9890888fa86e641cd685e21b78c31e/models/InnerCos.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from torch.autograd import Variable\n",
        "import collections\n",
        "import inspect, re\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class InnerCos(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(InnerCos, self).__init__()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.target = None\n",
        "        self.down_model = nn.Sequential(\n",
        "            nn.Conv2d(256, 3, kernel_size=1,stride=1, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def set_target(self, targetde, targetst):\n",
        "        self.targetst = F.interpolate(targetst, size=(32, 32), mode='bilinear')\n",
        "        self.targetde = F.interpolate(targetde, size=(32, 32), mode='bilinear')\n",
        "\n",
        "    def get_target(self):\n",
        "        return self.target\n",
        "\n",
        "    def forward(self, in_data):\n",
        "        loss_co = in_data[1]\n",
        "        self.ST = self.down_model(loss_co[0])\n",
        "        self.DE = self.down_model(loss_co[1])\n",
        "        #self.loss = self.criterion(self.ST, self.targetst)+self.criterion(self.DE, self.targetde)\n",
        "        self.output = in_data[0]\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, retain_graph=True):\n",
        "\n",
        "        self.loss.backward(retain_graph=retain_graph)\n",
        "        return self.loss\n",
        "\n",
        "    def __repr__(self):\n",
        "\n",
        "        return self.__class__.__name__\n",
        "\n",
        "# Converts a Tensor into a Numpy array\n",
        "# |imtype|: the desired type of the converted numpy array\n",
        "def tensor2im(image_tensor, imtype=np.uint8):\n",
        "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3,1,1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "def diagnose_network(net, name='network'):\n",
        "    mean = 0.0\n",
        "    count = 0\n",
        "    for param in net.parameters():\n",
        "        if param.grad is not None:\n",
        "            mean += torch.mean(torch.abs(param.grad.data))\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        mean = mean / count\n",
        "    print(name)\n",
        "    print(mean)\n",
        "\n",
        "def binary_mask(in_mask, threshold):\n",
        "    assert in_mask.dim() == 2, \"mask must be 2 dimensions\"\n",
        "\n",
        "    output = torch.ByteTensor(in_mask.size())\n",
        "    output = (output > threshold).float().mul_(1)\n",
        "\n",
        "    return output\n",
        "\n",
        "def gussin(v):\n",
        "    outk = []\n",
        "    v = v\n",
        "    for i in range(32):\n",
        "        for k in range(32):\n",
        "\n",
        "            out = []\n",
        "            for x in range(32):\n",
        "                row = []\n",
        "                for y in range(32):\n",
        "                    cord_x = i\n",
        "                    cord_y = k\n",
        "                    dis_x = np.abs(x - cord_x)\n",
        "                    dis_y = np.abs(y - cord_y)\n",
        "                    dis_add = -(dis_x * dis_x + dis_y * dis_y)\n",
        "                    dis_add = dis_add / (2 * v * v)\n",
        "                    dis_add = math.exp(dis_add) / (2 * math.pi * v * v)\n",
        "\n",
        "                    row.append(dis_add)\n",
        "                out.append(row)\n",
        "\n",
        "            outk.append(out)\n",
        "\n",
        "    out = np.array(outk)\n",
        "    f = out.sum(-1).sum(-1)\n",
        "    q = []\n",
        "    for i in range(1024):\n",
        "        g = out[i] / f[i]\n",
        "        q.append(g)\n",
        "    out = np.array(q)\n",
        "    return torch.from_numpy(out)\n",
        "\n",
        "def cal_feat_mask(inMask, conv_layers, threshold):\n",
        "    assert inMask.dim() == 4, \"mask must be 4 dimensions\"\n",
        "    assert inMask.size(0) == 1, \"the first dimension must be 1 for mask\"\n",
        "    inMask = inMask.float()\n",
        "    convs = []\n",
        "    inMask = Variable(inMask, requires_grad = False)\n",
        "    for id_net in range(conv_layers):\n",
        "        conv = nn.Conv2d(1,1,4,2,1, bias=False)\n",
        "        conv.weight.data.fill_(1/16)\n",
        "        convs.append(conv)\n",
        "    lnet = nn.Sequential(*convs)\n",
        "    if inMask.is_cuda:\n",
        "\n",
        "        lnet = lnet.cuda()\n",
        "    output = lnet(inMask)\n",
        "    output = (output > threshold).float().mul_(1)\n",
        "\n",
        "    return output\n",
        "\n",
        "def cal_mask_given_mask_thred(img, mask, patch_size, stride, mask_thred):\n",
        "    assert img.dim() == 3, 'img has to be 3 dimenison!'\n",
        "    assert mask.dim() == 2, 'mask has to be 2 dimenison!'\n",
        "    dim = img.dim()\n",
        "    #math.floor \n",
        "    _, H, W = img.size(dim-3), img.size(dim-2), img.size(dim-1)\n",
        "    nH = int(math.floor((H-patch_size)/stride + 1))\n",
        "    nW = int(math.floor((W-patch_size)/stride + 1))\n",
        "    N = nH*nW\n",
        "\n",
        "    flag = torch.zeros(N).long()\n",
        "    offsets_tmp_vec = torch.zeros(N).long()\n",
        "    #list\n",
        "\n",
        "    nonmask_point_idx_all = torch.zeros(N).long()\n",
        "\n",
        "    tmp_non_mask_idx = 0\n",
        "\n",
        "\n",
        "    mask_point_idx_all = torch.zeros(N).long()\n",
        "\n",
        "    tmp_mask_idx = 0\n",
        "    #\n",
        "    for i in range(N):\n",
        "        h = int(math.floor(i/nW))\n",
        "        w = int(math.floor(i%nW))\n",
        "        # print(h, w)\n",
        "        #11\n",
        "        mask_tmp = mask[h*stride:h*stride + patch_size,\n",
        "                        w*stride:w*stride + patch_size]\n",
        "\n",
        "\n",
        "        if torch.sum(mask_tmp) < mask_thred:\n",
        "            nonmask_point_idx_all[tmp_non_mask_idx] = i\n",
        "            tmp_non_mask_idx += 1\n",
        "        else:\n",
        "            mask_point_idx_all[tmp_mask_idx] = i\n",
        "            tmp_mask_idx += 1\n",
        "            flag[i] = 1\n",
        "            offsets_tmp_vec[i] = -1\n",
        "    # print(flag)  #checked\n",
        "    # print(offsets_tmp_vec) # checked\n",
        "\n",
        "    non_mask_num = tmp_non_mask_idx\n",
        "    mask_num = tmp_mask_idx\n",
        "\n",
        "    nonmask_point_idx = nonmask_point_idx_all.narrow(0, 0, non_mask_num)\n",
        "    mask_point_idx=mask_point_idx_all.narrow(0, 0, mask_num)\n",
        "\n",
        "    # get flatten_offsets\n",
        "    flatten_offsets_all = torch.LongTensor(N).zero_()\n",
        "    for i in range(N):\n",
        "        offset_value = torch.sum(offsets_tmp_vec[0:i+1])\n",
        "        if flag[i] == 1:\n",
        "            offset_value = offset_value + 1\n",
        "        # print(i+offset_value)\n",
        "        flatten_offsets_all[i+offset_value] = -offset_value\n",
        "\n",
        "    flatten_offsets = flatten_offsets_all.narrow(0, 0, non_mask_num)\n",
        "\n",
        "    # print('flatten_offsets')\n",
        "    # print(flatten_offsets)   # checked\n",
        "\n",
        "\n",
        "    # print('nonmask_point_idx')\n",
        "    # print(nonmask_point_idx)  #checked\n",
        "\n",
        "    return flag, nonmask_point_idx, flatten_offsets, mask_point_idx\n",
        "\n",
        "\n",
        "# sp_x: LongTensor\n",
        "# sp_y: LongTensor\n",
        "def cal_sps_for_Advanced_Indexing(h, w):\n",
        "    sp_y = torch.arange(0, w).long()\n",
        "    sp_y = torch.cat([sp_y]*h)\n",
        "\n",
        "    lst = []\n",
        "    for i in range(h):\n",
        "        lst.extend([i]*w)\n",
        "    sp_x = torch.from_numpy(np.array(lst))\n",
        "    return sp_x, sp_y\n",
        "\n",
        "\"\"\"\n",
        "def save_image(image_numpy, image_path):\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)\n",
        "\"\"\"\n",
        "def info(object, spacing=10, collapse=1):\n",
        "    \"\"\"Print methods and doc strings.\n",
        "    Takes module, class, list, dictionary, or string.\"\"\"\n",
        "    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n",
        "    processFunc = collapse and (lambda s: \" \".join(s.split())) or (lambda s: s)\n",
        "    print( \"\\n\".join([\"%s %s\" %\n",
        "                     (method.ljust(spacing),\n",
        "                      processFunc(str(getattr(object, method).__doc__)))\n",
        "                     for method in methodList]) )\n",
        "\n",
        "def varname(p):\n",
        "    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n",
        "        m = re.search(r'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)', line)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "\n",
        "def print_numpy(x, val=True, shp=False):\n",
        "    x = x.astype(np.float64)\n",
        "    if shp:\n",
        "        print('shape,', x.shape)\n",
        "    if val:\n",
        "        x = x.flatten()\n",
        "        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n",
        "            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n",
        "\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "\n",
        "class Selfpatch(object):\n",
        "    def buildAutoencoder(self, target_img, target_img_2, target_img_3, patch_size=1, stride=1):\n",
        "        nDim = 3\n",
        "        assert target_img.dim() == nDim, 'target image must be of dimension 3.'\n",
        "        C = target_img.size(0)\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor if torch.cuda.is_available else torch.Tensor\n",
        "\n",
        "        patches_features = self._extract_patches(target_img, patch_size, stride)\n",
        "        patches_features_f = self._extract_patches(target_img_3, patch_size, stride)\n",
        "\n",
        "        patches_on = self._extract_patches(target_img_2, 1, stride)\n",
        "\n",
        "        return patches_features_f, patches_features, patches_on\n",
        "\n",
        "    def build(self, target_img,  patch_size=5, stride=1):\n",
        "        nDim = 3\n",
        "        assert target_img.dim() == nDim, 'target image must be of dimension 3.'\n",
        "        C = target_img.size(0)\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor if torch.cuda.is_available else torch.Tensor\n",
        "\n",
        "        patches_features = self._extract_patches(target_img, patch_size, stride)\n",
        "\n",
        "        return patches_features\n",
        "\n",
        "    def _build(self, patch_size, stride, C, target_patches, npatches, normalize, interpolate, type):\n",
        "        # for each patch, divide by its L2 norm.\n",
        "        if type == 1:\n",
        "            enc_patches = target_patches.clone()\n",
        "            for i in range(npatches):\n",
        "                enc_patches[i] = enc_patches[i]*(1/(enc_patches[i].norm(2)+1e-8))\n",
        "\n",
        "            conv_enc = nn.Conv2d(npatches, npatches, kernel_size=1, stride=stride, bias=False, groups=npatches)\n",
        "            conv_enc.weight.data = enc_patches\n",
        "            return conv_enc\n",
        "\n",
        "        # normalize is not needed, it doesn't change the result!\n",
        "            if normalize:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            if interpolate:\n",
        "                raise NotImplementedError\n",
        "        else:\n",
        "\n",
        "            conv_dec = nn.ConvTranspose2d(npatches, C, kernel_size=patch_size, stride=stride, bias=False)\n",
        "            conv_dec.weight.data = target_patches\n",
        "            return conv_dec\n",
        "\n",
        "    def _extract_patches(self, img, patch_size, stride):\n",
        "        n_dim = 3\n",
        "        assert img.dim() == n_dim, 'image must be of dimension 3.'\n",
        "        kH, kW = patch_size, patch_size\n",
        "        dH, dW = stride, stride\n",
        "        input_windows = img.unfold(1, kH, dH).unfold(2, kW, dW)\n",
        "        i_1, i_2, i_3, i_4, i_5 = input_windows.size(0), input_windows.size(1), input_windows.size(2), input_windows.size(3), input_windows.size(4)\n",
        "        input_windows = input_windows.permute(1,2,0,3,4).contiguous().view(i_2*i_3, i_1, i_4, i_5)\n",
        "        patches_all = input_windows\n",
        "        return patches_all\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SE MODEL\n",
        "class SELayer(pl.LightningModule):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c, 1, 1)\n",
        "        y = self.fc(y)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class Convnorm(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, sample='none-3', activ='leaky'):\n",
        "        super().__init__()\n",
        "        self.bn = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "\n",
        "        if sample == 'down-3':\n",
        "            self.conv = nn.Conv2d(in_ch, out_ch, 3, 2, 1, bias=False)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch, out_ch, 3, 1)\n",
        "        if activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = input\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        if hasattr(self, 'activation'):\n",
        "            out = self.activation(out[0])\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCBActiv(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='leaky',\n",
        "                 conv_bias=False, innorm=False, inner=False, outer=False):\n",
        "        super().__init__()\n",
        "        if sample == 'same-5':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 5, 1, 2, bias=conv_bias)\n",
        "        elif sample == 'same-7':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 7, 1, 3, bias=conv_bias)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias)\n",
        "        else:\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias)\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.innorm = innorm\n",
        "        self.inner = inner\n",
        "        self.outer = outer\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = input\n",
        "        if self.inner:\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "\n",
        "        elif self.innorm:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            out[0] = self.activation(out[0])\n",
        "        elif self.outer:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "        else:\n",
        "            out = self.conv(out)\n",
        "            out[0] = self.bn(out[0])\n",
        "            if hasattr(self, 'activation'):\n",
        "                out[0] = self.activation(out[0])\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvDown(pl.LightningModule):\n",
        "    def __init__(self, in_c, out_c, kernel, stride, padding=0, dilation=1, groups=1, bias=False, layers=1, activ=True):\n",
        "        super().__init__()\n",
        "        nf_mult = 1\n",
        "        nums = out_c / 64\n",
        "        sequence = []\n",
        "\n",
        "        for i in range(1, layers + 1):\n",
        "            nf_mult_prev = nf_mult\n",
        "            if nums == 8:\n",
        "                if in_c == 512:\n",
        "\n",
        "                    nfmult = 1\n",
        "                else:\n",
        "                    nf_mult = 2\n",
        "\n",
        "            else:\n",
        "                nf_mult = min(2 ** i, 8)\n",
        "            if kernel != 1:\n",
        "\n",
        "                if activ == False and layers == 1:\n",
        "                    sequence += [\n",
        "                        nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                  kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                        nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                    ]\n",
        "                else:\n",
        "                    sequence += [\n",
        "                        nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                  kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                        nn.InstanceNorm2d(nf_mult * in_c),\n",
        "                        nn.LeakyReLU(0.2, True)\n",
        "                    ]\n",
        "\n",
        "            else:\n",
        "\n",
        "                sequence += [\n",
        "                    nn.Conv2d(in_c, out_c,\n",
        "                              kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                    nn.InstanceNorm2d(out_c),\n",
        "                    nn.LeakyReLU(0.2, True)\n",
        "                ]\n",
        "\n",
        "            if activ == False:\n",
        "                if i + 1 == layers:\n",
        "                    if layers == 2:\n",
        "                        sequence += [\n",
        "                            nn.Conv2d(nf_mult * in_c, nf_mult * in_c,\n",
        "                                      kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                        ]\n",
        "                    else:\n",
        "                        sequence += [\n",
        "                            nn.Conv2d(nf_mult_prev * in_c, nf_mult * in_c,\n",
        "                                      kernel_size=kernel, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.InstanceNorm2d(nf_mult * in_c)\n",
        "                        ]\n",
        "                    break\n",
        "\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class ConvUp(pl.LightningModule):\n",
        "    def __init__(self, in_c, out_c, kernel, stride, padding=0, dilation=1, groups=1, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_c, out_c, kernel,\n",
        "                              stride, padding, dilation, groups, bias)\n",
        "        self.bn = nn.InstanceNorm2d(out_c)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, size):\n",
        "        out = F.interpolate(input=input, size=size, mode='bilinear')\n",
        "        out = self.conv(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BASE(pl.LightningModule):\n",
        "    def __init__(self, inner_nc):\n",
        "        super(BASE, self).__init__()\n",
        "        se = SELayer(inner_nc, 16)\n",
        "        model = [se]\n",
        "        gus = gussin(1.5).cuda()\n",
        "        self.gus = torch.unsqueeze(gus, 1).double()\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.down = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, 1, 1, 0, bias=False),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        Nonparm = Selfpatch()\n",
        "        out_32 = self.model(x)\n",
        "        b, c, h, w = out_32.size()\n",
        "        gus = self.gus.float()\n",
        "        gus_out = out_32[0].expand(h * w, c, h, w)\n",
        "        gus_out = gus * gus_out\n",
        "        gus_out = torch.sum(gus_out, -1)\n",
        "        gus_out = torch.sum(gus_out, -1)\n",
        "        gus_out = gus_out.contiguous().view(b, c, h, w)\n",
        "        csa2_in = F.sigmoid(out_32)\n",
        "        csa2_f = torch.nn.functional.pad(csa2_in, (1, 1, 1, 1))\n",
        "        csa2_ff = torch.nn.functional.pad(out_32, (1, 1, 1, 1))\n",
        "        csa2_fff, csa2_f, csa2_conv = Nonparm.buildAutoencoder(csa2_f[0], csa2_in[0], csa2_ff[0], 3, 1)\n",
        "        csa2_conv = csa2_conv.expand_as(csa2_f)\n",
        "        csa_a = csa2_conv * csa2_f\n",
        "        csa_a = torch.mean(csa_a, 1)\n",
        "        a_c, a_h, a_w = csa_a.size()\n",
        "        csa_a = csa_a.contiguous().view(a_c, -1)\n",
        "        csa_a = F.softmax(csa_a, dim=1)\n",
        "        csa_a = csa_a.contiguous().view(a_c, 1, a_h, a_h)\n",
        "        out = csa_a * csa2_fff\n",
        "        out = torch.sum(out, -1)\n",
        "        out = torch.sum(out, -1)\n",
        "        out_csa = out.contiguous().view(b, c, h, w)\n",
        "        out_32 = torch.cat([gus_out, out_csa], 1)\n",
        "        out_32 = self.down(out_32)\n",
        "        return out_32\n",
        "\n",
        "\n",
        "class PartialConv(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__()\n",
        "        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                    stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                                   stride, padding, dilation, groups, False)\n",
        "\n",
        "        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n",
        "\n",
        "        # mask is not updated\n",
        "        for param in self.mask_conv.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, inputt):\n",
        "        # http://masc.cs.gmu.edu/wiki/partialconv\n",
        "        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n",
        "        # W^T* (M .* X) / sum(M) + b = [C(M .* X)  C(0)] / D(M) + C(0)\n",
        "\n",
        "        input = inputt[0]\n",
        "        mask = inputt[1].float().cuda()\n",
        "\n",
        "        output = self.input_conv(input * mask)\n",
        "        if self.input_conv.bias is not None:\n",
        "            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(\n",
        "                output)\n",
        "        else:\n",
        "            output_bias = torch.zeros_like(output)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_mask = self.mask_conv(mask)\n",
        "\n",
        "        no_update_holes = output_mask == 0\n",
        "        mask_sum = output_mask.masked_fill_(no_update_holes.bool(), 1.0)\n",
        "        output_pre = (output - output_bias) / mask_sum + output_bias\n",
        "        output = output_pre.masked_fill_(no_update_holes.bool(), 0.0)\n",
        "        new_mask = torch.ones_like(output)\n",
        "        new_mask = new_mask.masked_fill_(no_update_holes.bool(), 0.0)\n",
        "        out = []\n",
        "        out.append(output)\n",
        "        out.append(new_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCconv(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(PCconv, self).__init__()\n",
        "        self.down_128 = ConvDown(64, 128, 4, 2, padding=1, layers=2)\n",
        "        self.down_64 = ConvDown(128, 256, 4, 2, padding=1)\n",
        "        self.down_32 = ConvDown(256, 256, 1, 1)\n",
        "        self.down_16 = ConvDown(512, 512, 4, 2, padding=1, activ=False)\n",
        "        self.down_8 = ConvDown(512, 512, 4, 2, padding=1, layers=2, activ=False)\n",
        "        self.down_4 = ConvDown(512, 512, 4, 2, padding=1, layers=3, activ=False)\n",
        "        self.down = ConvDown(768, 256, 1, 1)\n",
        "        self.fuse = ConvDown(512, 512, 1, 1)\n",
        "        self.up = ConvUp(512, 256, 1, 1)\n",
        "        self.up_128 = ConvUp(512, 64, 1, 1)\n",
        "        self.up_64 = ConvUp(512, 128, 1, 1)\n",
        "        self.up_32 = ConvUp(512, 256, 1, 1)\n",
        "        self.base= BASE(512)\n",
        "        seuqence_3 = []\n",
        "        seuqence_5 = []\n",
        "        seuqence_7 = []\n",
        "        for i in range(5):\n",
        "            seuqence_3 += [PCBActiv(256, 256, innorm=True)]\n",
        "            seuqence_5 += [PCBActiv(256, 256, sample='same-5', innorm=True)]\n",
        "            seuqence_7 += [PCBActiv(256, 256, sample='same-7', innorm=True)]\n",
        "\n",
        "        self.cov_3 = nn.Sequential(*seuqence_3)\n",
        "        self.cov_5 = nn.Sequential(*seuqence_5)\n",
        "        self.cov_7 = nn.Sequential(*seuqence_7)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        mask =  cal_feat_mask(mask, 3, 1)\n",
        "        # input[2]:256 32 32\n",
        "        b, c, h, w = input[2].size()\n",
        "        mask_1 = torch.add(torch.neg(mask.float()), 1)\n",
        "        mask_1 = mask_1.expand(b, c, h, w)\n",
        "\n",
        "        x_1 = self.activation(input[0])\n",
        "        x_2 = self.activation(input[1])\n",
        "        x_3 = self.activation(input[2])\n",
        "        x_4 = self.activation(input[3])\n",
        "        x_5 = self.activation(input[4])\n",
        "        x_6 = self.activation(input[5])\n",
        "        # Change the shape of each layer and intergrate low-level/high-level features\n",
        "        x_1 = self.down_128(x_1)\n",
        "        x_2 = self.down_64(x_2)\n",
        "        x_3 = self.down_32(x_3)\n",
        "        x_4 = self.up(x_4, (32, 32))\n",
        "        x_5 = self.up(x_5, (32, 32))\n",
        "        x_6 = self.up(x_6, (32, 32))\n",
        "\n",
        "        # The first three layers are Texture/detail\n",
        "        # The last three layers are Structure\n",
        "        x_DE = torch.cat([x_1, x_2, x_3], 1)\n",
        "        x_ST = torch.cat([x_4, x_5, x_6], 1)\n",
        "\n",
        "        x_ST = self.down(x_ST)\n",
        "        x_DE = self.down(x_DE)\n",
        "        x_ST = [x_ST, mask_1]\n",
        "        x_DE = [x_DE, mask_1]\n",
        "\n",
        "        # Multi Scale PConv fill the Details\n",
        "        x_DE_3 = self.cov_3(x_DE)\n",
        "        x_DE_5 = self.cov_5(x_DE)\n",
        "        x_DE_7 = self.cov_7(x_DE)\n",
        "        x_DE_fuse = torch.cat([x_DE_3[0], x_DE_5[0], x_DE_7[0]], 1)\n",
        "        x_DE_fi = self.down(x_DE_fuse)\n",
        "\n",
        "        # Multi Scale PConv fill the Structure\n",
        "        x_ST_3 = self.cov_3(x_ST)\n",
        "        x_ST_5 = self.cov_5(x_ST)\n",
        "        x_ST_7 = self.cov_7(x_ST)\n",
        "        x_ST_fuse = torch.cat([x_ST_3[0], x_ST_5[0], x_ST_7[0]], 1)\n",
        "        x_ST_fi = self.down(x_ST_fuse)\n",
        "\n",
        "        x_cat = torch.cat([x_ST_fi, x_DE_fi], 1)\n",
        "        x_cat_fuse = self.fuse(x_cat)\n",
        "\n",
        "        # Feature equalizations\n",
        "        x_final = self.base(x_cat_fuse)\n",
        "\n",
        "        # Add back to the input\n",
        "        x_ST = x_final\n",
        "        x_DE = x_final\n",
        "        x_1 = self.up_128(x_DE, (128, 128)) + input[0]\n",
        "        x_2 = self.up_64(x_DE, (64, 64)) + input[1]\n",
        "        x_3 = self.up_32(x_DE, (32, 32)) + input[2]\n",
        "        x_4 = self.down_16(x_ST) + input[3]\n",
        "        x_5 = self.down_8(x_ST) + input[4]\n",
        "        x_6 = self.down_4(x_ST) + input[5]\n",
        "\n",
        "        out = [x_1, x_2, x_3, x_4, x_5, x_6]\n",
        "        loss = [x_ST_fi, x_DE_fi]\n",
        "        out_final = [out, loss]\n",
        "        return out_final\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Define the resnet block\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=False),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# define the Encoder unit\n",
        "class UnetSkipConnectionEBlock(pl.LightningModule):\n",
        "    def __init__(self, outer_nc, inner_nc, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d,\n",
        "                 use_dropout=False):\n",
        "        super(UnetSkipConnectionEBlock, self).__init__()\n",
        "        downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1)\n",
        "\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        downnorm = norm_layer(inner_nc, affine=True)\n",
        "        if outermost:\n",
        "            down = [downconv]\n",
        "            model = down\n",
        "        elif innermost:\n",
        "            down = [downrelu, downconv]\n",
        "            model = down\n",
        "        else:\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            if use_dropout:\n",
        "                model = down + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Encoder(pl.LightningModule):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, res_num=4, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # construct unet structure\n",
        "        Encoder_1 = UnetSkipConnectionEBlock(input_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, outermost=True)\n",
        "        Encoder_2 = UnetSkipConnectionEBlock(ngf, ngf * 2, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_3 = UnetSkipConnectionEBlock(ngf * 2, ngf * 4, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_4 = UnetSkipConnectionEBlock(ngf * 4, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_5 = UnetSkipConnectionEBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Encoder_6 = UnetSkipConnectionEBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout, innermost=True)\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(res_num):\n",
        "            block = ResnetBlock(ngf * 8, 2)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.Encoder_1 = Encoder_1\n",
        "        self.Encoder_2 = Encoder_2\n",
        "        self.Encoder_3 = Encoder_3\n",
        "        self.Encoder_4 = Encoder_4\n",
        "        self.Encoder_5 = Encoder_5\n",
        "        self.Encoder_6 = Encoder_6\n",
        "\n",
        "    def forward(self, input):\n",
        "        y_1 = self.Encoder_1(input)\n",
        "        y_2 = self.Encoder_2(y_1)\n",
        "        y_3 = self.Encoder_3(y_2)\n",
        "        y_4 = self.Encoder_4(y_3)\n",
        "        y_5 = self.Encoder_5(y_4)\n",
        "        y_6 = self.Encoder_6(y_5)\n",
        "        y_7 = self.middle(y_6)\n",
        "\n",
        "        return y_1, y_2, y_3, y_4, y_5, y_7\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class UnetSkipConnectionDBlock(pl.LightningModule):\n",
        "    def __init__(self, inner_nc, outer_nc, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d,\n",
        "                 use_dropout=False):\n",
        "        super(UnetSkipConnectionDBlock, self).__init__()\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc, affine=True)\n",
        "        upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                    kernel_size=4, stride=2,\n",
        "                                    padding=1)\n",
        "        up = [uprelu, upconv, upnorm]\n",
        "\n",
        "        if outermost:\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = up\n",
        "        elif innermost:\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = up\n",
        "        else:\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Decoder(pl.LightningModule):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # construct unet structure\n",
        "        Decoder_1 = UnetSkipConnectionDBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout,\n",
        "                                             innermost=True)\n",
        "        Decoder_2 = UnetSkipConnectionDBlock(ngf * 16, ngf * 8, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_3 = UnetSkipConnectionDBlock(ngf * 16, ngf * 4, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_4 = UnetSkipConnectionDBlock(ngf * 8, ngf * 2, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_5 = UnetSkipConnectionDBlock(ngf * 4, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        Decoder_6 = UnetSkipConnectionDBlock(ngf * 2, output_nc, norm_layer=norm_layer, use_dropout=use_dropout, outermost=True)\n",
        "\n",
        "        self.Decoder_1 = Decoder_1\n",
        "        self.Decoder_2 = Decoder_2\n",
        "        self.Decoder_3 = Decoder_3\n",
        "        self.Decoder_4 = Decoder_4\n",
        "        self.Decoder_5 = Decoder_5\n",
        "        self.Decoder_6 = Decoder_6\n",
        "\n",
        "    def forward(self, input_1, input_2, input_3, input_4, input_5, input_6):\n",
        "        y_1 = self.Decoder_1(input_6)\n",
        "        y_2 = self.Decoder_2(torch.cat([y_1, input_5], 1))\n",
        "        y_3 = self.Decoder_3(torch.cat([y_2, input_4], 1))\n",
        "        y_4 = self.Decoder_4(torch.cat([y_3, input_3], 1))\n",
        "        y_5 = self.Decoder_5(torch.cat([y_4, input_2], 1))\n",
        "        y_6 = self.Decoder_6(torch.cat([y_5, input_1], 1))\n",
        "        out = y_6\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PCblock(pl.LightningModule):\n",
        "    def __init__(self, stde_list):\n",
        "        super(PCblock, self).__init__()\n",
        "        self.pc_block = PCconv()\n",
        "        innerloss = InnerCos()\n",
        "        stde_list.append(innerloss)\n",
        "        loss = [innerloss]\n",
        "        self.loss=nn.Sequential(*loss)\n",
        "    def forward(self, input, mask):\n",
        "        out = self.pc_block(input, mask)\n",
        "        out = self.loss(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MEDFEGenerator(pl.LightningModule):\n",
        "    def __init__(self, input_nc=4, output_nc=3, ngf=64,  norm='batch', use_dropout=False, stde_list=[], norm_layer = nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "        self.netEN = Encoder(input_nc=input_nc, output_nc=output_nc, ngf=ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        self.netDE = Decoder(input_nc=input_nc, output_nc=output_nc, ngf=ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        self.netMEDFE = PCblock(stde_list)\n",
        "\n",
        "    def mask_process(self, mask):\n",
        "        mask = mask[0][0]\n",
        "        mask = torch.unsqueeze(mask, 0)\n",
        "        mask = torch.unsqueeze(mask, 1)\n",
        "        mask = mask.byte()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, images, masks):\n",
        "        #masks =torch.cat([masks,masks,masks],1)\n",
        "\n",
        "        fake_p_1, fake_p_2, fake_p_3, fake_p_4, fake_p_5, fake_p_6 = self.netEN(torch.cat([images, masks], 1))\n",
        "        x_out = self.netMEDFE([fake_p_1, fake_p_2, fake_p_3, fake_p_4, fake_p_5, fake_p_6], masks)\n",
        "        self.fake_out = self.netDE(x_out[0], x_out[1], x_out[2], x_out[3], x_out[4], x_out[5])\n",
        "\n",
        "        return self.fake_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIXC_Hs56XLJ",
        "cellView": "form"
      },
      "source": [
        "#@title [RFR_arch.py](https://github.com/jingyuanli001/RFR-Inpainting) [no AMP] (2020)\n",
        "\"\"\"\n",
        "RFRNet.py (18-12-20)\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/RFRNet.py\n",
        "\n",
        "partialconv2d.py (18-12-20) # using their partconv2d to avoid dimension errors\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/partialconv2d.py\n",
        "\n",
        "Attention.py (18-12-20)\n",
        "https://github.com/jingyuanli001/RFR-Inpainting/blob/master/modules/Attention.py\n",
        "\"\"\"\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class KnowledgeConsistentAttention(pl.LightningModule):\n",
        "    def __init__(self, patch_size = 3, propagate_size = 3, stride = 1):\n",
        "        super(KnowledgeConsistentAttention, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "        self.att_scores_prev = None\n",
        "        self.masks_prev = None\n",
        "        self.ratio = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        bz, nc, h, w = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background\n",
        "        conv_kernels_all = background.view(bz, nc, w * h, 1, 1)\n",
        "        conv_kernels_all = conv_kernels_all.permute(0, 2, 1, 3, 4)\n",
        "        output_tensor = []\n",
        "        att_score = []\n",
        "        for i in range(bz):\n",
        "            feature_map = foreground[i:i+1]\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels**2, [1, 2, 3], keepdim = True)**0.5\n",
        "            conv_kernels = conv_kernels/norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding = self.patch_size//2)\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.avg_pool2d(conv_result, 3, 1, padding = 1)*9\n",
        "            attention_scores = F.softmax(conv_result, dim = 1)\n",
        "            if self.att_scores_prev is not None:\n",
        "                attention_scores = (self.att_scores_prev[i:i+1]*self.masks_prev[i:i+1] + attention_scores * (torch.abs(self.ratio)+1e-7))/(self.masks_prev[i:i+1]+(torch.abs(self.ratio)+1e-7))\n",
        "            att_score.append(attention_scores)\n",
        "            feature_map = F.conv_transpose2d(attention_scores, conv_kernels, stride = 1, padding = self.patch_size//2)\n",
        "            final_output = feature_map\n",
        "            output_tensor.append(final_output)\n",
        "        self.att_scores_prev = torch.cat(att_score, dim = 0).view(bz, h*w, h, w)\n",
        "        self.masks_prev = masks.view(bz, 1, h, w)\n",
        "        return torch.cat(output_tensor, dim = 0)\n",
        "\n",
        "class AttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, inchannel, patch_size_list = [1], propagate_size_list = [3], stride_list = [1]):\n",
        "        assert isinstance(patch_size_list, list), \"patch_size should be a list containing scales, or you should use Contextual Attention to initialize your module\"\n",
        "        assert len(patch_size_list) == len(propagate_size_list) and len(propagate_size_list) == len(stride_list), \"the input_lists should have same lengths\"\n",
        "        super(AttentionModule, self).__init__()\n",
        "\n",
        "        self.att = KnowledgeConsistentAttention(patch_size_list[0], propagate_size_list[0], stride_list[0])\n",
        "        self.num_of_modules = len(patch_size_list)\n",
        "        self.combiner = nn.Conv2d(inchannel * 2, inchannel, kernel_size = 1)\n",
        "\n",
        "    def forward(self, foreground, mask):\n",
        "        outputs = self.att(foreground, mask)\n",
        "        outputs = torch.cat([outputs, foreground],dim = 1)\n",
        "        outputs = self.combiner(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False\n",
        "\n",
        "        self.return_mask = True\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "\n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "\n",
        "        if mask is not None or self.last_size != (input.data.shape[2], input.data.shape[3]):\n",
        "            self.last_size = (input.data.shape[2], input.data.shape[3])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "\n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "            self.update_mask.to(input)\n",
        "            self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class RFRModule(pl.LightningModule):\n",
        "    def __init__(self, layer_size=6, in_channel = 64):\n",
        "        super(RFRModule, self).__init__()\n",
        "        self.freeze_enc_bn = False\n",
        "        self.layer_size = layer_size\n",
        "        for i in range(3):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            out_channel = in_channel * 2\n",
        "            block = [nn.Conv2d(in_channel, out_channel, 3, 2, 1, bias = False),\n",
        "                     nn.BatchNorm2d(out_channel),\n",
        "                     nn.ReLU(inplace = True)]\n",
        "            in_channel = out_channel\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "\n",
        "        for i in range(3, 6):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            block = [nn.Conv2d(in_channel, out_channel, 3, 1, 2, dilation = 2, bias = False),\n",
        "                     nn.BatchNorm2d(out_channel),\n",
        "                     nn.ReLU(inplace = True)]\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "        self.att = AttentionModule(512)\n",
        "        for i in range(5, 3, -1):\n",
        "            name = 'dec_{:d}'.format(i)\n",
        "            block = [nn.Conv2d(in_channel + in_channel, in_channel, 3, 1, 2, dilation = 2, bias = False),\n",
        "                     nn.BatchNorm2d(in_channel),\n",
        "                     nn.LeakyReLU(0.2, inplace = True)]\n",
        "            setattr(self, name, nn.Sequential(*block))\n",
        "\n",
        "\n",
        "        block = [nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(512),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_3 = nn.Sequential(*block)\n",
        "\n",
        "        block = [nn.ConvTranspose2d(768, 256, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(256),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_2 = nn.Sequential(*block)\n",
        "\n",
        "        block = [nn.ConvTranspose2d(384, 64, 4, 2, 1, bias = False),\n",
        "                 nn.BatchNorm2d(64),\n",
        "                 nn.LeakyReLU(0.2, inplace = True)]\n",
        "        self.dec_1 = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "\n",
        "        h_dict['h_0']= input\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            h_dict[h_key] = getattr(self, l_key)(h_dict[h_key_prev])\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h = h_dict[h_key]\n",
        "        for i in range(self.layer_size - 1, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n",
        "            h = getattr(self, dec_l_key)(h)\n",
        "            if i == 3:\n",
        "                h = self.att(h, mask)\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class RFRNet(pl.LightningModule):\n",
        "    def __init__(self, conv_type):\n",
        "        super(RFRNet, self).__init__()\n",
        "\n",
        "        self.conv_type = conv_type\n",
        "        if self.conv_type == 'partial':\n",
        "          self.conv1 = PartialConv2d(3, 64, 7, 2, 3, multi_channel = True, bias = False)\n",
        "          self.conv2 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.conv21 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.conv22 = PartialConv2d(64, 64, 7, 1, 3, multi_channel = True, bias = False)\n",
        "          self.tail1 = PartialConv2d(67, 32, 3, 1, 1, multi_channel = True, bias = False)\n",
        "          # original code uses conv2d\n",
        "          self.out = nn.Conv2d(64,3,3,1,1, bias = False)\n",
        "        elif self.conv_type == 'deform':\n",
        "          self.conv1 = DeformConv2d(3, 64, 7, 2, 3)\n",
        "          self.conv2 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.conv21 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.conv22 = DeformConv2d(64, 64, 7, 1, 3)\n",
        "          self.tail1 = DeformConv2d(67, 32, 3, 1, 1)\n",
        "          # original code uses conv2d\n",
        "          self.out = nn.Conv2d(64,3,3,1,1, bias = False)\n",
        "        else:\n",
        "          print(\"conv_type not found\")\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn20 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.RFRModule = RFRModule()\n",
        "        self.Tconv = nn.ConvTranspose2d(64, 64, 4, 2, 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.tail2 = Bottleneck(32,8)\n",
        "\n",
        "    def forward(self, in_image, mask):\n",
        "        #in_image = torch.cat((in_image, mask), dim=1)\n",
        "        mask =torch.cat([mask,mask,mask],1)\n",
        "        if self.conv_type == 'partial':\n",
        "          x1, m1 = self.conv1(in_image.type(torch.cuda.FloatTensor), mask.type(torch.cuda.FloatTensor))\n",
        "        elif self.conv_type == 'deform':\n",
        "          x1 = self.conv1(in_image)\n",
        "          m1 = self.conv1(mask)\n",
        "\n",
        "        x1 = F.relu(self.bn1(x1), inplace = True)\n",
        "\n",
        "        if self.conv_type == 'partial':\n",
        "          x1, m1 = self.conv2(x1, m1)\n",
        "        elif self.conv_type == 'deform':\n",
        "          x1 = self.conv2(x1)\n",
        "          m1 = self.conv2(m1)\n",
        "\n",
        "        x1 = F.relu(self.bn20(x1), inplace = True)\n",
        "        x2 = x1\n",
        "        x2, m2 = x1, m1\n",
        "        n, c, h, w = x2.size()\n",
        "        feature_group = [x2.view(n, c, 1, h, w)]\n",
        "        mask_group = [m2.view(n, c, 1, h, w)]\n",
        "        self.RFRModule.att.att.att_scores_prev = None\n",
        "        self.RFRModule.att.att.masks_prev = None\n",
        "\n",
        "        for i in range(6):\n",
        "            if self.conv_type == 'partial':\n",
        "              x2, m2 = self.conv21(x2, m2)\n",
        "              x2, m2 = self.conv22(x2, m2)\n",
        "            elif self.conv_type == 'deform':\n",
        "              x2 = self.conv21(x2)\n",
        "              m2 = self.conv21(m2)\n",
        "              x2 = self.conv22(x2)\n",
        "              m2 = self.conv22(m2)\n",
        "\n",
        "            x2 = F.leaky_relu(self.bn2(x2), inplace = True)\n",
        "            x2 = self.RFRModule(x2, m2[:,0:1,:,:])\n",
        "            x2 = x2 * m2\n",
        "            feature_group.append(x2.view(n, c, 1, h, w))\n",
        "            mask_group.append(m2.view(n, c, 1, h, w))\n",
        "        x3 = torch.cat(feature_group, dim = 2)\n",
        "        m3 = torch.cat(mask_group, dim = 2)\n",
        "        amp_vec = m3.mean(dim = 2)\n",
        "        x3 = (x3*m3).mean(dim = 2) /(amp_vec+1e-7)\n",
        "        x3 = x3.view(n, c, h, w)\n",
        "        m3 = m3[:,:,-1,:,:]\n",
        "        x4 = self.Tconv(x3)\n",
        "        x4 = F.leaky_relu(self.bn3(x4), inplace = True)\n",
        "        m4 = F.interpolate(m3, scale_factor = 2)\n",
        "        x5 = torch.cat([in_image, x4], dim = 1)\n",
        "        m5 = torch.cat([mask, m4], dim = 1)\n",
        "\n",
        "        if self.conv_type == 'partial':\n",
        "          x5, _ = self.tail1(x5, m5)\n",
        "        elif self.conv_type == 'deform':\n",
        "          x5 = self.tail1(x5)\n",
        "\n",
        "        x5 = F.leaky_relu(x5, inplace = True)\n",
        "        x6 = self.tail2(x5)\n",
        "        x6 = torch.cat([x5,x6], dim = 1)\n",
        "        output = self.out(x6)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pri4vUtP7V-Y",
        "cellView": "form"
      },
      "source": [
        "#@title [DMFN_arch.py](https://github.com/Zheng222/DMFN) (2020)\n",
        "\"\"\"\n",
        "block.py (18-12-20)\n",
        "https://github.com/Zheng222/DMFN/blob/master/models/block.py\n",
        "\n",
        "architecture.py (18-12-20)\n",
        "https://github.com/Zheng222/DMFN/blob/master/models/architecture.py\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def conv_layer(in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1):\n",
        "    padding = int((kernel_size - 1) / 2) * dilation\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=True, dilation=dilation,\n",
        "                     groups=groups)\n",
        "\n",
        "\n",
        "def _norm(norm_type, nc):\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'bn':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "    elif norm_type == 'in':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [{:s}] is not found'.format(norm_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "def _activation(act_type, inplace=True, neg_slope=0.2, n_prelu=1):\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type == 'lrelu':\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [{:s}] is not found'.format(act_type))\n",
        "    return layer\n",
        "\n",
        "\n",
        "class conv_block(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n",
        "                 padding=0, norm='in', activation='relu', pad_type='zero'):\n",
        "        super(conv_block, self).__init__()\n",
        "        if pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        elif pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        if norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_nc, affine=False)\n",
        "        elif norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_nc, affine=True)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported norm type: {}\".format(norm)\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_nc, out_nc, kernel_size, stride, 0, dilation, groups, bias)  # padding=0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(self.pad(x))\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class upconv_block(pl.LightningModule):\n",
        "    def __init__(self, in_nc, out_nc, kernel_size=3, stride=1, bias=True,\n",
        "                 padding=0, pad_type='zero', norm='none', activation='relu'):\n",
        "        super(upconv_block, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_nc, out_nc, 4, 2, 1)\n",
        "        self.act = _activation('relu')\n",
        "        self.norm = _norm('in', out_nc)\n",
        "\n",
        "        self.conv = conv_block(out_nc, out_nc, kernel_size, stride, bias=bias, padding=padding, pad_type=pad_type,\n",
        "                               norm=norm, activation=activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.norm(self.deconv(x)))\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResBlock_new(pl.LightningModule):\n",
        "    def __init__(self, nc):\n",
        "        super(ResBlock_new, self).__init__()\n",
        "        self.c1 = conv_layer(nc, nc // 4, 3, 1)\n",
        "        self.d1 = conv_layer(nc // 4, nc // 4, 3, 1, 1)  # rate = 1\n",
        "        self.d2 = conv_layer(nc // 4, nc // 4, 3, 1, 2)  # rate = 2\n",
        "        self.d3 = conv_layer(nc // 4, nc // 4, 3, 1, 4)  # rate = 4\n",
        "        self.d4 = conv_layer(nc // 4, nc // 4, 3, 1, 8)  # rate = 8\n",
        "        self.act = _activation('relu')\n",
        "        self.norm = _norm('in', nc)\n",
        "        self.c2 = conv_layer(nc, nc, 3, 1)  # fusion\n",
        "\n",
        "    def forward(self, x):\n",
        "        output1 = self.act(self.norm(self.c1(x)))\n",
        "        d1 = self.d1(output1)\n",
        "        d2 = self.d2(output1)\n",
        "        d3 = self.d3(output1)\n",
        "        d4 = self.d4(output1)\n",
        "\n",
        "        add1 = d1 + d2\n",
        "        add2 = add1 + d3\n",
        "        add3 = add2 + d4\n",
        "        combine = torch.cat([d1, add1, add2, add3], 1)\n",
        "        output2 = self.c2(self.act(self.norm(combine)))\n",
        "        output = x + self.norm(output2)\n",
        "        return output\n",
        "\n",
        "import torch.nn as nn\n",
        "#from . import block as B\n",
        "\n",
        "class InpaintingGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_nc=4, out_nc=3, nf=64, n_res=8, norm='in', activation='relu'):\n",
        "        super(InpaintingGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(  # input: [4, 256, 256]\n",
        "            conv_block(in_nc, nf, 5, stride=1, padding=2, norm='none', activation=activation),  # [64, 256, 256]\n",
        "            conv_block(nf, 2 * nf, 3, stride=2, padding=1, norm=norm, activation=activation),  # [128, 128, 128]\n",
        "            conv_block(2 * nf, 2 * nf, 3, stride=1, padding=1, norm=norm, activation=activation),  # [128, 128, 128]\n",
        "            conv_block(2 * nf, 4 * nf, 3, stride=2, padding=1, norm=norm, activation=activation)  # [256, 64, 64]\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(n_res):\n",
        "            block = ResBlock_new(4 * nf)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            conv_block(4 * nf, 4 * nf, 3, stride=1, padding=1, norm=norm, activation=activation),  # [256, 64, 64]\n",
        "            upconv_block(4 * nf, 2 * nf, kernel_size=3, stride=1, padding=1, norm=norm, activation='relu'),\n",
        "            # [128, 128, 128]\n",
        "            upconv_block(2 * nf, nf, kernel_size=3, stride=1, padding=1, norm=norm, activation='relu'),\n",
        "            # [64, 256, 256]\n",
        "            conv_block(nf, out_nc, 3, stride=1, padding=1, norm='none', activation='tanh')  # [3, 256, 256]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4eBTBk7otF",
        "cellView": "form"
      },
      "source": [
        "#@title [RN_arch.py](https://github.com/geekyutao/RN/) (2020)\n",
        "\"\"\"\n",
        "networks.py (13-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/networks.py\n",
        "\n",
        "rn.py (13-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/rn.py\n",
        "\n",
        "module_util.py (15-12-20)\n",
        "https://github.com/geekyutao/RN/blob/a3cf1fccc08f22fcf4b336503a8853748720fd67/module_util.py\n",
        "\"\"\"\n",
        "\n",
        "from torchvision.transforms import *\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "logger = logging.getLogger('base')\n",
        "\n",
        "def rn_initialize_weights(net_l, scale=1):\n",
        "    if not isinstance(net_l, list):\n",
        "        net_l = [net_l]\n",
        "    for net in net_l:\n",
        "        for m in net.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale  # for residual block\n",
        "                if m.bias is not None:\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                try:\n",
        "                    init.constant_(m.weight, 1)\n",
        "                    init.normal_(m.bias, 0.0001)\n",
        "                except:\n",
        "                    print('This layer has no BN parameters:', m)\n",
        "    logger.info('RN Initialization method [kaiming]')\n",
        "\n",
        "\n",
        "\n",
        "class RN_binarylabel(pl.LightningModule):\n",
        "    def __init__(self, feature_channels):\n",
        "        super(RN_binarylabel, self).__init__()\n",
        "        self.bn_norm = nn.BatchNorm2d(feature_channels, affine=False, track_running_stats=False)\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        '''\n",
        "        input:  x: (B,C,M,N), features\n",
        "                label: (B,1,M,N), 1 for foreground regions, 0 for background regions\n",
        "        output: _x: (B,C,M,N)\n",
        "        '''\n",
        "        label = label.detach()\n",
        "\n",
        "        rn_foreground_region = self.rn(x * label, label)\n",
        "\n",
        "        rn_background_region = self.rn(x * (1 - label), 1 - label)\n",
        "\n",
        "        return rn_foreground_region + rn_background_region\n",
        "\n",
        "    def rn(self, region, mask):\n",
        "        '''\n",
        "        input:  region: (B,C,M,N), 0 for surroundings\n",
        "                mask: (B,1,M,N), 1 for target region, 0 for surroundings\n",
        "        output: rn_region: (B,C,M,N)\n",
        "        '''\n",
        "        shape = region.size()\n",
        "\n",
        "        sum = torch.sum(region, dim=[0,2,3])  # (B, C) -> (C)\n",
        "        Sr = torch.sum(mask, dim=[0,2,3])    # (B, 1) -> (1)\n",
        "        Sr[Sr==0] = 1\n",
        "        mu = (sum / Sr)     # (B, C) -> (C)\n",
        "\n",
        "        return self.bn_norm(region + (1 - mask) * mu[None,:,None,None]) * \\\n",
        "        (torch.sqrt(Sr / (shape[0] * shape[2] * shape[3])))[None,:,None,None]\n",
        "\n",
        "class RN_B(pl.LightningModule):\n",
        "    def __init__(self, feature_channels):\n",
        "        super(RN_B, self).__init__()\n",
        "        '''\n",
        "        input: tensor(features) x: (B,C,M,N)\n",
        "               condition Mask: (B,1,H,W): 0 for background, 1 for foreground\n",
        "        return: tensor RN_B(x): (N,C,M,N)\n",
        "        ---------------------------------------\n",
        "        args:\n",
        "            feature_channels: C\n",
        "        '''\n",
        "        # RN\n",
        "        self.rn = RN_binarylabel(feature_channels)    # need no external parameters\n",
        "\n",
        "        # gamma and beta\n",
        "        self.foreground_gamma = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.foreground_beta = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.background_gamma = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "        self.background_beta = nn.Parameter(torch.zeros(feature_channels), requires_grad=True)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # mask = F.adaptive_max_pool2d(mask, output_size=x.size()[2:])\n",
        "        mask = F.interpolate(mask, size=x.size()[2:], mode='nearest')   # after down-sampling, there can be all-zero mask\n",
        "\n",
        "        rn_x = self.rn(x, mask)\n",
        "\n",
        "        rn_x_foreground = (rn_x * mask) * (1 + self.foreground_gamma[None,:,None,None]) + self.foreground_beta[None,:,None,None]\n",
        "        rn_x_background = (rn_x * (1 - mask)) * (1 + self.background_gamma[None,:,None,None]) + self.background_beta[None,:,None,None]\n",
        "\n",
        "        return rn_x_foreground + rn_x_background\n",
        "\n",
        "class SelfAware_Affine(pl.LightningModule):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SelfAware_Affine, self).__init__()\n",
        "\n",
        "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.gamma_conv = nn.Conv2d(1, 1, kernel_size, padding=padding)\n",
        "        self.beta_conv = nn.Conv2d(1, 1, kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        importance_map = self.sigmoid(x)\n",
        "\n",
        "        gamma = self.gamma_conv(importance_map)\n",
        "        beta = self.beta_conv(importance_map)\n",
        "\n",
        "        return importance_map, gamma, beta\n",
        "\n",
        "class RN_L(pl.LightningModule):\n",
        "    def __init__(self, feature_channels, threshold=0.8):\n",
        "        super(RN_L, self).__init__()\n",
        "        '''\n",
        "        input: tensor(features) x: (B,C,M,N)\n",
        "        return: tensor RN_L(x): (B,C,M,N)\n",
        "        ---------------------------------------\n",
        "        args:\n",
        "            feature_channels: C\n",
        "        '''\n",
        "        # SelfAware_Affine\n",
        "        self.sa = SelfAware_Affine()\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # RN\n",
        "        self.rn = RN_binarylabel(feature_channels)    # need no external parameters\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        sa_map, gamma, beta = self.sa(x)     # (B,1,M,N)\n",
        "\n",
        "        # m = sa_map.detach()\n",
        "        if x.is_cuda:\n",
        "            mask = torch.zeros_like(sa_map).cuda()\n",
        "        else:\n",
        "            mask = torch.zeros_like(sa_map)\n",
        "        mask[sa_map.detach() >= self.threshold] = 1\n",
        "\n",
        "        rn_x = self.rn(x, mask.expand(x.size()))\n",
        "\n",
        "        rn_x = rn_x * (1 + gamma) + beta\n",
        "\n",
        "        return rn_x\n",
        "\n",
        "\n",
        "class G_Net(pl.LightningModule):\n",
        "    def __init__(self, input_channels=3, residual_blocks=8, threshold=0.8):\n",
        "        super(G_Net, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_prePad = nn.ReflectionPad2d(3)\n",
        "        self.encoder_conv1 = nn.Conv2d(in_channels=input_channels, out_channels=64, kernel_size=7, padding=0)\n",
        "        self.encoder_in1 = RN_B(feature_channels=64)\n",
        "        self.encoder_relu1 = nn.ReLU(True)\n",
        "        self.encoder_conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
        "        self.encoder_in2 = RN_B(feature_channels=128)\n",
        "        self.encoder_relu2 = nn.ReLU(True)\n",
        "        self.encoder_conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
        "        self.encoder_in3 = RN_B(feature_channels=256)\n",
        "        self.encoder_relu3 = nn.ReLU(True)\n",
        "\n",
        "\n",
        "        # Middle\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            # block = ResnetBlock(256, 2, use_spectral_norm=False)\n",
        "            block = saRN_ResnetBlock(256, dilation=2, threshold=threshold, use_spectral_norm=False)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            RN_L(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(128, 64*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2),\n",
        "            RN_L(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=input_channels, kernel_size=7, padding=0)\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def encoder(self, x, mask):\n",
        "        # float\n",
        "        x = x.type(torch.cuda.FloatTensor)\n",
        "        mask = mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        # half precision (not working?)\n",
        "        #x = x.type(torch.cuda.HalfTensor)\n",
        "        #mask = mask.type(torch.cuda.HalfTensor)\n",
        "\n",
        "        x = self.encoder_prePad(x)\n",
        "\n",
        "        x = self.encoder_conv1(x)\n",
        "        x = self.encoder_in1(x, mask)\n",
        "        x = self.encoder_relu2(x)\n",
        "\n",
        "        x = self.encoder_conv2(x)\n",
        "        x = self.encoder_in2(x, mask)\n",
        "        x = self.encoder_relu2(x)\n",
        "\n",
        "        x = self.encoder_conv3(x)\n",
        "        x = self.encoder_in3(x, mask)\n",
        "        x = self.encoder_relu3(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #gt = x\n",
        "        #x = (x * (1 - mask).float()) + mask\n",
        "        # input mask: 1 for hole, 0 for valid\n",
        "        x = self.encoder(x, mask)\n",
        "\n",
        "        x = self.middle(x)\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        x = (torch.tanh(x) + 1) / 2\n",
        "        # x = x*mask+gt*(1-mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1, use_spectral_norm=True):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=256, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "class saRN_ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation, threshold, use_spectral_norm=True):\n",
        "        super(saRN_ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=256, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            # nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "            RN_L(feature_channels=256, threshold=threshold),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            # nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            RN_L(feature_channels=dim, threshold=threshold),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        # skimage.io.imsave('block.png', out[0].detach().permute(1,2,0).cpu().numpy()[:,:,0])\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "def spectral_norm(module, mode=True):\n",
        "    if mode:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "\n",
        "    return module\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfjYt7dyOcvQ",
        "cellView": "form"
      },
      "source": [
        "#@title [DFNet_arch.py](https://github.com/hughplay/DFNet) (2019)\n",
        "# https://github.com/hughplay/DFNet\n",
        "# https://github.com/Yukariin/DFNet\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#from .convolutions import partialconv2d\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "\n",
        "def resize_like(x, target, mode='bilinear'):\n",
        "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
        "\n",
        "\n",
        "def get_norm(name, out_channels):\n",
        "    if name == 'batch':\n",
        "        norm = nn.BatchNorm2d(out_channels)\n",
        "    elif name == 'instance':\n",
        "        norm = nn.InstanceNorm2d(out_channels)\n",
        "    else:\n",
        "        norm = None\n",
        "    return norm\n",
        "\n",
        "\n",
        "def get_activation(name):\n",
        "    if name == 'relu':\n",
        "        activation = nn.ReLU()\n",
        "    elif name == 'elu':\n",
        "        activation == nn.ELU()\n",
        "    elif name == 'leaky_relu':\n",
        "        activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "    elif name == 'tanh':\n",
        "        activation = nn.Tanh()\n",
        "    elif name == 'sigmoid':\n",
        "        activation = nn.Sigmoid()\n",
        "    else:\n",
        "        activation = None\n",
        "    return activation\n",
        "\n",
        "\n",
        "class Conv2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding = self.conv_same_pad(kernel_size, stride)\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          # original\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = nn.Conv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "        elif conv_type == 'partial':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  PartialConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "        elif conv_type == 'deform':\n",
        "          if type(padding) is not tuple:\n",
        "              self.conv = PartialConv2d(\n",
        "                  in_channels, out_channels, kernel_size, stride, padding)\n",
        "          else:\n",
        "              self.conv = nn.Sequential(\n",
        "                  nn.ConstantPad2d(padding*2, 0),\n",
        "                  DeformConv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
        "              )\n",
        "\n",
        "\n",
        "    def conv_same_pad(self, ksize, stride):\n",
        "        if (ksize - stride) % 2 == 0:\n",
        "            return (ksize - stride) // 2\n",
        "        else:\n",
        "            left = (ksize - stride) // 2\n",
        "            right = left + 1\n",
        "            return left, right\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ConvTranspose2dSame(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n",
        "        self.trans_conv = nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size, stride,\n",
        "            padding, output_padding)\n",
        "\n",
        "    def deconv_same_pad(self, ksize, stride):\n",
        "        pad = (ksize - stride + 1) // 2\n",
        "        outpad = 2 * pad + stride - ksize\n",
        "        return pad, outpad\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.trans_conv(x)\n",
        "\n",
        "\n",
        "class UpBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        if mode == 'deconv':\n",
        "            self.up = ConvTranspose2dSame(\n",
        "                channel, channel, kernel_size, stride=scale)\n",
        "        else:\n",
        "            def upsample(x):\n",
        "                return F.interpolate(x, scale_factor=scale, mode=mode)\n",
        "            self.up = upsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "\n",
        "class EncodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, conv_type, kernel_size, stride,\n",
        "            normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_in = in_channels\n",
        "        self.c_out = out_channels\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class DecodeBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_from_up, c_from_down, conv_type, c_out, mode='nearest',\n",
        "            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_from_up = c_from_up\n",
        "        self.c_from_down = c_from_down\n",
        "        self.c_in = c_from_up + c_from_down\n",
        "        self.c_out = c_out\n",
        "\n",
        "        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Conv2dSame(self.c_in, self.c_out, conv_type, kernel_size, stride=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, self.c_out))\n",
        "        if activation:\n",
        "            layers.append(get_activation(activation))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, concat=None):\n",
        "        out = self.up(x)\n",
        "        if self.c_from_down > 0:\n",
        "            out = torch.cat([out, concat], dim=1)\n",
        "        out = self.decode(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BlendBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "            self, c_in, c_out, conv_type, ksize_mid=3, norm='batch', act='leaky_relu'):\n",
        "        super().__init__()\n",
        "        c_mid = max(c_in // 2, 32)\n",
        "        self.blend = nn.Sequential(\n",
        "            Conv2dSame(c_in, c_mid, conv_type, 1, 1),\n",
        "            get_norm(norm, c_mid),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_mid, c_out, conv_type, ksize_mid, 1),\n",
        "            get_norm(norm, c_out),\n",
        "            get_activation(act),\n",
        "            Conv2dSame(c_out, c_out, conv_type, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blend(x)\n",
        "\n",
        "\n",
        "class FusionBlock(pl.LightningModule):\n",
        "    def __init__(self, c_feat, conv_type, c_alpha=1):\n",
        "        super().__init__()\n",
        "        c_img = 3\n",
        "        self.map2img = nn.Sequential(\n",
        "            Conv2dSame(c_feat, c_img, conv_type, 1, 1),\n",
        "            nn.Sigmoid())\n",
        "        self.blend = BlendBlock(c_img*2, c_alpha, conv_type)\n",
        "\n",
        "    def forward(self, img_miss, feat_de):\n",
        "        img_miss = resize_like(img_miss, feat_de)\n",
        "        raw = self.map2img(feat_de)\n",
        "        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n",
        "        result = alpha * raw + (1 - alpha) * img_miss\n",
        "        return result, alpha, raw\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class DFNet(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self, c_img=3, c_mask=1, c_alpha=3,\n",
        "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
        "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n",
        "            blend_layers=[0, 1, 2, 3, 4, 5], conv_type = 'normal'):\n",
        "        super().__init__()\n",
        "\n",
        "        c_init = c_img + c_mask\n",
        "\n",
        "        self.n_en = len(en_ksize)\n",
        "        self.n_de = len(de_ksize)\n",
        "        assert self.n_en == self.n_de, (\n",
        "            'The number layer of Encoder and Decoder must be equal.')\n",
        "        assert self.n_en >= 1, (\n",
        "            'The number layer of Encoder and Decoder must be greater than 1.')\n",
        "\n",
        "        assert 0 in blend_layers, 'Layer 0 must be blended.'\n",
        "\n",
        "        self.en = []\n",
        "        c_in = c_init\n",
        "        self.en.append(\n",
        "            EncodeBlock(c_in, 64, conv_type, en_ksize[0], 2, None, None))\n",
        "        for k_en in en_ksize[1:]:\n",
        "            c_in = self.en[-1].c_out\n",
        "            c_out = min(c_in*2, 512)\n",
        "            self.en.append(EncodeBlock(\n",
        "                c_in, c_out, conv_type, k_en, stride=2,\n",
        "                normalization=norm, activation=act_en))\n",
        "\n",
        "        # register parameters\n",
        "        for i, en in enumerate(self.en):\n",
        "            self.__setattr__('en_{}'.format(i), en)\n",
        "\n",
        "        self.de = []\n",
        "        self.fuse = []\n",
        "        for i, k_de in enumerate(de_ksize):\n",
        "\n",
        "            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n",
        "            c_out = c_from_down = self.en[-i-1].c_in\n",
        "            layer_idx = self.n_de - i - 1\n",
        "\n",
        "            self.de.append(DecodeBlock(\n",
        "                c_from_up, c_from_down, conv_type, c_out, mode, k_de, scale=2,\n",
        "                normalization=norm, activation=act_de))\n",
        "            if layer_idx in blend_layers:\n",
        "                self.fuse.append(FusionBlock(c_out, conv_type, c_alpha))\n",
        "            else:\n",
        "                self.fuse.append(None)\n",
        "\n",
        "        # register parameters\n",
        "        for i, de in enumerate(self.de[::-1]):\n",
        "            self.__setattr__('de_{}'.format(i), de)\n",
        "        for i, fuse in enumerate(self.fuse[::-1]):\n",
        "            if fuse:\n",
        "                self.__setattr__('fuse_{}'.format(i), fuse)\n",
        "\n",
        "    def forward(self, img_miss, mask):\n",
        "\n",
        "        out = torch.cat([img_miss, mask], dim=1)\n",
        "        out_en = [out]\n",
        "\n",
        "        for encode in self.en:\n",
        "            out = encode(out)\n",
        "            out_en.append(out)\n",
        "\n",
        "        results = []\n",
        "        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n",
        "            out = decode(out, out_en[-i-2])\n",
        "            if fuse:\n",
        "                result, alpha, raw = fuse(img_miss, out)\n",
        "                results.append(result)\n",
        "        return results[::-1][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj9OiPPf6enW",
        "cellView": "form"
      },
      "source": [
        "#@title [LBAM_arch.py](https://github.com/Vious/LBAM_Pytorch) (2019)\n",
        "\"\"\"\n",
        "LBAMModel.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/master/models/LBAMModel.py\n",
        "\n",
        "forwardAttentionLayer.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/forwardAttentionLayer.py\n",
        "\n",
        "reverseAttentionLayer.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/reverseAttentionLayer.py\n",
        "\n",
        "ActivationFunction.py (18-12-20)\n",
        "https://github.com/Vious/LBAM_Pytorch/blob/98c2ae70486f4ba3ab86d4345e586e7841cfe343/models/ActivationFunction.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# asymmetric gaussian shaped activation function g_A\n",
        "class GaussActivation(pl.LightningModule):\n",
        "    def __init__(self, a, mu, sigma1, sigma2):\n",
        "        super(GaussActivation, self).__init__()\n",
        "\n",
        "        self.a = Parameter(torch.tensor(a, dtype=torch.float32))\n",
        "        self.mu = Parameter(torch.tensor(mu, dtype=torch.float32))\n",
        "        self.sigma1 = Parameter(torch.tensor(sigma1, dtype=torch.float32))\n",
        "        self.sigma2 = Parameter(torch.tensor(sigma2, dtype=torch.float32))\n",
        "\n",
        "\n",
        "    def forward(self, inputFeatures):\n",
        "\n",
        "        self.a.data = torch.clamp(self.a.data, 1.01, 6.0)\n",
        "        self.mu.data = torch.clamp(self.mu.data, 0.1, 3.0)\n",
        "        self.sigma1.data = torch.clamp(self.sigma1.data, 0.5, 2.0)\n",
        "        self.sigma2.data = torch.clamp(self.sigma2.data, 0.5, 2.0)\n",
        "\n",
        "        lowerThanMu = inputFeatures < self.mu\n",
        "        largerThanMu = inputFeatures >= self.mu\n",
        "\n",
        "        leftValuesActiv = self.a * torch.exp(- self.sigma1 * ( (inputFeatures - self.mu) ** 2 ) )\n",
        "        leftValuesActiv.masked_fill_(largerThanMu, 0.0)\n",
        "\n",
        "        rightValueActiv = 1 + (self.a - 1) * torch.exp(- self.sigma2 * ( (inputFeatures - self.mu) ** 2 ) )\n",
        "        rightValueActiv.masked_fill_(lowerThanMu, 0.0)\n",
        "\n",
        "        output = leftValuesActiv + rightValueActiv\n",
        "\n",
        "        return output\n",
        "\n",
        "# mask updating functions, we recommand using alpha that is larger than 0 and lower than 1.0\n",
        "class MaskUpdate(pl.LightningModule):\n",
        "    def __init__(self, alpha):\n",
        "        super(MaskUpdate, self).__init__()\n",
        "\n",
        "        self.updateFunc = nn.ReLU(True)\n",
        "        #self.alpha = Parameter(torch.tensor(alpha, dtype=torch.float32))\n",
        "        self.alpha = alpha\n",
        "    def forward(self, inputMaskMap):\n",
        "        \"\"\" self.alpha.data = torch.clamp(self.alpha.data, 0.6, 0.8)\n",
        "        print(self.alpha) \"\"\"\n",
        "\n",
        "        return torch.pow(self.updateFunc(inputMaskMap), self.alpha)\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "#from models.ActivationFunction import GaussActivation, MaskUpdate\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "\n",
        "# learnable reverse attention conv\n",
        "class ReverseMaskConv(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, kernelSize=4, stride=2,\n",
        "        padding=1, dilation=1, groups=1, convBias=False):\n",
        "        super(ReverseMaskConv, self).__init__()\n",
        "\n",
        "        self.reverseMaskConv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, \\\n",
        "            dilation, groups, bias=convBias)\n",
        "\n",
        "        #self.reverseMaskConv.apply(weights_init())\n",
        "\n",
        "        self.activationFuncG_A = GaussActivation(1.1, 1.0, 0.5, 0.5)\n",
        "        self.updateMask = MaskUpdate(0.8)\n",
        "\n",
        "    def forward(self, inputMasks):\n",
        "        maskFeatures = self.reverseMaskConv(inputMasks)\n",
        "\n",
        "        maskActiv = self.activationFuncG_A(maskFeatures)\n",
        "\n",
        "        maskUpdate = self.updateMask(maskFeatures)\n",
        "\n",
        "        return maskActiv, maskUpdate\n",
        "\n",
        "# learnable reverse attention layer, including features activation and batchnorm\n",
        "class ReverseAttention(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, bn=False, activ='leaky', \\\n",
        "        kernelSize=4, stride=2, padding=1, outPadding=0,dilation=1, groups=1,convBias=False, bnChannels=512):\n",
        "        super(ReverseAttention, self).__init__()\n",
        "\n",
        "        self.conv = nn.ConvTranspose2d(inputChannels, outputChannels, kernel_size=kernelSize, \\\n",
        "            stride=stride, padding=padding, output_padding=outPadding, dilation=dilation, groups=groups,bias=convBias)\n",
        "\n",
        "        #self.conv.apply(weights_init())\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(bnChannels)\n",
        "\n",
        "        if activ == 'leaky':\n",
        "            self.activ = nn.LeakyReLU(0.2, False)\n",
        "        elif activ == 'relu':\n",
        "            self.activ = nn.ReLU()\n",
        "        elif activ == 'sigmoid':\n",
        "            self.activ = nn.Sigmoid()\n",
        "        elif activ == 'tanh':\n",
        "            self.activ = nn.Tanh()\n",
        "        elif activ == 'prelu':\n",
        "            self.activ = nn.PReLU()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, ecFeaturesSkip, dcFeatures, maskFeaturesForAttention):\n",
        "        nextDcFeatures = self.conv(dcFeatures)\n",
        "\n",
        "        # note that encoder features are ahead, it's important tor make forward attention map ahead\n",
        "        # of reverse attention map when concatenate, we do it in the LBAM model forward function\n",
        "        concatFeatures = torch.cat((ecFeaturesSkip, nextDcFeatures), 1)\n",
        "\n",
        "        outputFeatures = concatFeatures * maskFeaturesForAttention\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            outputFeatures = self.bn(outputFeatures)\n",
        "        if hasattr(self, 'activ'):\n",
        "            outputFeatures = self.activ(outputFeatures)\n",
        "\n",
        "        return outputFeatures\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "#from models.ActivationFunction import GaussActivation, MaskUpdate\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "# learnable forward attention conv layer\n",
        "class ForwardAttentionLayer(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, kernelSize, stride,\n",
        "        padding, dilation=1, groups=1, bias=False):\n",
        "        super(ForwardAttentionLayer, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, dilation, \\\n",
        "            groups, bias)\n",
        "\n",
        "        if inputChannels == 4:\n",
        "            self.maskConv = nn.Conv2d(3, outputChannels, kernelSize, stride, padding, dilation, \\\n",
        "                groups, bias)\n",
        "        else:\n",
        "            self.maskConv = nn.Conv2d(inputChannels, outputChannels, kernelSize, stride, padding, \\\n",
        "                dilation, groups, bias)\n",
        "\n",
        "        #self.conv.apply(weights_init())\n",
        "        #self.maskConv.apply(weights_init())\n",
        "\n",
        "        self.activationFuncG_A = GaussActivation(1.1, 2.0, 1.0, 1.0)\n",
        "        self.updateMask = MaskUpdate(0.8)\n",
        "\n",
        "    def forward(self, inputFeatures, inputMasks):\n",
        "        convFeatures = self.conv(inputFeatures)\n",
        "        maskFeatures = self.maskConv(inputMasks)\n",
        "        #convFeatures_skip = convFeatures.clone()\n",
        "\n",
        "        maskActiv = self.activationFuncG_A(maskFeatures)\n",
        "        convOut = convFeatures * maskActiv\n",
        "\n",
        "        maskUpdate = self.updateMask(maskFeatures)\n",
        "\n",
        "        return convOut, maskUpdate, convFeatures, maskActiv\n",
        "\n",
        "# forward attention gather feature activation and batchnorm\n",
        "class ForwardAttention(pl.LightningModule):\n",
        "    def __init__(self, inputChannels, outputChannels, bn=False, sample='down-4', \\\n",
        "        activ='leaky', convBias=False):\n",
        "        super(ForwardAttention, self).__init__()\n",
        "\n",
        "        if sample == 'down-4':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 4, 2, 1, bias=convBias)\n",
        "        elif sample == 'down-5':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 5, 2, 2, bias=convBias)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 7, 2, 3, bias=convBias)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 3, 2, 1, bias=convBias)\n",
        "        else:\n",
        "            self.conv = ForwardAttentionLayer(inputChannels, outputChannels, 3, 1, 1, bias=convBias)\n",
        "\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(outputChannels)\n",
        "\n",
        "        if activ == 'leaky':\n",
        "            self.activ = nn.LeakyReLU(0.2, False)\n",
        "        elif activ == 'relu':\n",
        "            self.activ = nn.ReLU()\n",
        "        elif activ == 'sigmoid':\n",
        "            self.activ = nn.Sigmoid()\n",
        "        elif activ == 'tanh':\n",
        "            self.activ = nn.Tanh()\n",
        "        elif activ == 'prelu':\n",
        "            self.activ = nn.PReLU()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def forward(self, inputFeatures, inputMasks):\n",
        "        features, maskUpdated, convPreF, maskActiv = self.conv(inputFeatures, inputMasks)\n",
        "\n",
        "        if hasattr(self, 'bn'):\n",
        "            features = self.bn(features)\n",
        "        if hasattr(self, 'activ'):\n",
        "            features = self.activ(features)\n",
        "\n",
        "        return features, maskUpdated, convPreF, maskActiv\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "#from models.forwardAttentionLayer import ForwardAttention\n",
        "#from models.reverseAttentionLayer import ReverseAttention, ReverseMaskConv\n",
        "#from models.weightInitial import weights_init\n",
        "\n",
        "#VGG16 feature extract\n",
        "class VGG16FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(VGG16FeatureExtractor, self).__init__()\n",
        "        vgg16 = models.vgg16(pretrained=False)\n",
        "        vgg16.load_state_dict(torch.load('./vgg16-397923af.pth'))\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(3):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(3):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "class LBAMModel(pl.LightningModule):\n",
        "    def __init__(self, inputChannels=4, outputChannels=3):\n",
        "        super(LBAMModel, self).__init__()\n",
        "\n",
        "        # default kernel is of size 4X4, stride 2, padding 1,\n",
        "        # and the use of biases are set false in default ReverseAttention class.\n",
        "        self.ec1 = ForwardAttention(inputChannels, 64, bn=False)\n",
        "        self.ec2 = ForwardAttention(64, 128)\n",
        "        self.ec3 = ForwardAttention(128, 256)\n",
        "        self.ec4 = ForwardAttention(256, 512)\n",
        "\n",
        "        for i in range(5, 8):\n",
        "            name = 'ec{:d}'.format(i)\n",
        "            setattr(self, name, ForwardAttention(512, 512))\n",
        "\n",
        "        # reverse mask conv\n",
        "        self.reverseConv1 = ReverseMaskConv(3, 64)\n",
        "        self.reverseConv2 = ReverseMaskConv(64, 128)\n",
        "        self.reverseConv3 = ReverseMaskConv(128, 256)\n",
        "        self.reverseConv4 = ReverseMaskConv(256, 512)\n",
        "        self.reverseConv5 = ReverseMaskConv(512, 512)\n",
        "        self.reverseConv6 = ReverseMaskConv(512, 512)\n",
        "\n",
        "        self.dc1 = ReverseAttention(512, 512, bnChannels=1024)\n",
        "        self.dc2 = ReverseAttention(512 * 2, 512, bnChannels=1024)\n",
        "        self.dc3 = ReverseAttention(512 * 2, 512, bnChannels=1024)\n",
        "        self.dc4 = ReverseAttention(512 * 2, 256, bnChannels=512)\n",
        "        self.dc5 = ReverseAttention(256 * 2, 128, bnChannels=256)\n",
        "        self.dc6 = ReverseAttention(128 * 2, 64, bnChannels=128)\n",
        "        self.dc7 = nn.ConvTranspose2d(64 * 2, outputChannels, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, inputImgs, masks):\n",
        "        inputImgs = torch.cat((inputImgs, masks), 1).type(torch.cuda.FloatTensor)\n",
        "        masks = torch.cat([masks,masks,masks],1).type(torch.cuda.FloatTensor)\n",
        "\n",
        "        ef1, mu1, skipConnect1, forwardMap1 = self.ec1(inputImgs, masks)\n",
        "        ef2, mu2, skipConnect2, forwardMap2 = self.ec2(ef1, mu1)\n",
        "        ef3, mu3, skipConnect3, forwardMap3 = self.ec3(ef2, mu2)\n",
        "        ef4, mu4, skipConnect4, forwardMap4 = self.ec4(ef3, mu3)\n",
        "        ef5, mu5, skipConnect5, forwardMap5 = self.ec5(ef4, mu4)\n",
        "        ef6, mu6, skipConnect6, forwardMap6 = self.ec6(ef5, mu5)\n",
        "        ef7, _, _, _ = self.ec7(ef6, mu6)\n",
        "\n",
        "\n",
        "        reverseMap1, revMu1 = self.reverseConv1(1 - masks)\n",
        "        reverseMap2, revMu2 = self.reverseConv2(revMu1)\n",
        "        reverseMap3, revMu3 = self.reverseConv3(revMu2)\n",
        "        reverseMap4, revMu4 = self.reverseConv4(revMu3)\n",
        "        reverseMap5, revMu5 = self.reverseConv5(revMu4)\n",
        "        reverseMap6, _ = self.reverseConv6(revMu5)\n",
        "\n",
        "        concatMap6 = torch.cat((forwardMap6, reverseMap6), 1)\n",
        "        dcFeatures1 = self.dc1(skipConnect6, ef7, concatMap6)\n",
        "\n",
        "        concatMap5 = torch.cat((forwardMap5, reverseMap5), 1)\n",
        "        dcFeatures2 = self.dc2(skipConnect5, dcFeatures1, concatMap5)\n",
        "\n",
        "        concatMap4 = torch.cat((forwardMap4, reverseMap4), 1)\n",
        "        dcFeatures3 = self.dc3(skipConnect4, dcFeatures2, concatMap4)\n",
        "\n",
        "        concatMap3 = torch.cat((forwardMap3, reverseMap3), 1)\n",
        "        dcFeatures4 = self.dc4(skipConnect3, dcFeatures3, concatMap3)\n",
        "\n",
        "        concatMap2 = torch.cat((forwardMap2, reverseMap2), 1)\n",
        "        dcFeatures5 = self.dc5(skipConnect2, dcFeatures4, concatMap2)\n",
        "\n",
        "        concatMap1 = torch.cat((forwardMap1, reverseMap1), 1)\n",
        "        dcFeatures6 = self.dc6(skipConnect1, dcFeatures5, concatMap1)\n",
        "\n",
        "        dcFeatures7 = self.dc7(dcFeatures6)\n",
        "\n",
        "        output = (self.tanh(dcFeatures7) + 1) / 2\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LZk4iSggfC7e"
      },
      "source": [
        "#@title [Adaptive_arch.py](https://github.com/GuardSkill/AdaptiveGAN) (2019)\n",
        "\"\"\"\n",
        "blocks.py (13-12-20)\n",
        "https://github.com/GuardSkill/AdaptiveGAN/blob/429311f6d22948904429ff1c19b0d953bc26ba81/src/blocks.py\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class hswish(pl.LightningModule):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(pl.LightningModule):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(pl.LightningModule):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(in_size // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            # nn.BatchNorm2d(in_size),\n",
        "            hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "# class BottleneckBlock(pl.LightningModule):\n",
        "#     expansion = 2\n",
        "#\n",
        "#     def __init__(self, in_channels, out_channels, stride=1, dilation=1, use_spectral_norm=False, downsample=None):\n",
        "#         super(BottleneckBlock, self).__init__()\n",
        "#         self.downsample = downsample\n",
        "#         self.stride = stride\n",
        "#         self.conv_block = nn.Sequential(\n",
        "#             nn.ZeroPad2d(dilation),\n",
        "#             spectral_norm(\n",
        "#                 nn.Conv2d(in_channels=in_channels, out_channels=out_channels * 2, kernel_size=3, stride=stride,\n",
        "#                           padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "#             # nn.LeakyReLU(0.2, inplace=False),\n",
        "#             nn.Tanh(),\n",
        "#             nn.ZeroPad2d(1),\n",
        "#             spectral_norm(\n",
        "#                 nn.Conv2d(in_channels=out_channels * 2, out_channels=out_channels, kernel_size=3, stride=stride,\n",
        "#                           padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "#         )\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         residual = x\n",
        "#         if self.downsample is not None:\n",
        "#             residual = self.downsample(x)\n",
        "#         out = self.conv_block(x) + residual\n",
        "#         # out = nn.LeakyReLU(0.2, inplace=False)(out)\n",
        "#         out = nn.Tanh()(out)\n",
        "#         return out\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, stride, dilation=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se = SeModule(out_size)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.nolinear1 = nn.Tanh()\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride,\n",
        "                               padding=(kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2, dilation=dilation,\n",
        "                               groups=expand_size, bias=False)\n",
        "        self.nolinear2 = nn.Tanh()\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.nolinear1(self.conv1(x))\n",
        "        out = self.nolinear2(self.conv2(out))\n",
        "        out = self.conv3(out)\n",
        "        if self.se != None:\n",
        "            out = self.se(out)\n",
        "        out = out + self.shortcut(x) if self.stride == 1 else out\n",
        "        return out\n",
        "\n",
        "\n",
        "class LinkNet(pl.LightningModule):\n",
        "    def __init__(self, in_channels=3, residual_blocks=1, init_weights=True):\n",
        "        super(LinkNet, self).__init__()\n",
        "        self.conv1 =    nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        # self.conv1 =Block(3, 3, 8, 16, 2)\n",
        "        self.block1 = nn.Sequential(\n",
        "            *[Block(3, 16, 16, 16, 1) for i in range(residual_blocks)]\n",
        "            # residual_blocks1   kernel  input  expand output strike dilation\n",
        "        )\n",
        "        #  out 16\n",
        "        self.conv2 = Block(3, 16, 64, 24, 2)\n",
        "        self.block2 = nn.Sequential(\n",
        "            *[Block(3, 24, 72, 24, 1) for _ in range(residual_blocks * 2)]  # residual_blocks1\n",
        "        )\n",
        "        #  out 24\n",
        "\n",
        "        self.conv3 = Block(5, 24, 72, 40, 2)\n",
        "        self.block3 = nn.Sequential(\n",
        "            *[Block(5, 40, 120, 40, 1) for _ in range(residual_blocks * 3)]  # residual_blocks2\n",
        "        )\n",
        "        #  out 40\n",
        "\n",
        "        self.conv4 = Block(3, 40, 240, 80, 2, 1)\n",
        "        self.block4 = nn.Sequential(\n",
        "            *[Block(3, 80, 200, 80, 1, 4) for _ in range(residual_blocks * 4)]  # residual_blocks3\n",
        "        )\n",
        "        #  out 80\n",
        "\n",
        "        self.conv5 = Block(5, 80, 480, 160, 2)\n",
        "        self.block5 = nn.Sequential(\n",
        "            *[Block(5, 160, 672, 160, 1, 4) for _ in range(residual_blocks * 4)]\n",
        "        )\n",
        "        #  out 160\n",
        "\n",
        "        # self.up1 = nn.Sequential(\n",
        "        #     nn.Conv2d(16, 16, 1, 1, 0, bias=True),\n",
        "        #     nn.Tanh(),\n",
        "        #     # nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        # )\n",
        "\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Conv2d(24, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Conv2d(40, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 1, mode='bilinear')\n",
        "        )\n",
        "        self.up4 = nn.Sequential(\n",
        "            nn.Conv2d(80, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 2, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.up5 = nn.Sequential(\n",
        "            nn.Conv2d(160, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "            nn.Upsample(scale_factor=2 << 3, mode='bilinear')\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            *[Block(5, 80, 160, 80, 1) for _ in range(residual_blocks * 4)],  # 3x3 original:residual_blocks*2\n",
        "            Block(5, 80, 48, 32, 1)\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.block_fusion = nn.Sequential(\n",
        "            *[Block(5, 32, 48, 32, 1) for _ in range(residual_blocks * 4)]  # 3x3 original:residual_blocks*2\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "        self.final = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.final =Block(5, 32, 48, 3,1)      #kernel_size, in_size, expand_size, out_size, stride\n",
        "        #  out 160\n",
        "\n",
        "    #     self.init_params()\n",
        "    #\n",
        "    # def init_params(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Conv2d):\n",
        "    #             init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.BatchNorm2d):\n",
        "    #             init.constant_(m.weight, 1)\n",
        "    #             init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.Linear):\n",
        "    #             init.normal_(m.weight, std=0.001)\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block1(self.conv1(x))\n",
        "        x2 = self.block2(self.conv2(x1))\n",
        "        x3 = self.block3(self.conv3(x2))\n",
        "        x4 = self.block4(self.conv4(x3))\n",
        "        x5 = self.block5(self.conv5(x4))\n",
        "        # x=x1+self.up2(x2)+self.up3(x3)+self.up4(x4)+self.up5(x5)\n",
        "        x = torch.cat([x1, self.up2(x2), self.up3(x3), self.up4(x4), self.up5(x5)], 1)\n",
        "        x = self.block_fusion(self.fusion(x))\n",
        "        x = self.final(x)\n",
        "        out = (torch.tanh(x) + 1) / 2\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidNet(pl.LightningModule):\n",
        "    def __init__(self, in_channels=3, residual_blocks=1, init_weights=True):\n",
        "        super(PyramidNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            *[Block(3, 16, 16, 16, 1) for i in range(residual_blocks)]\n",
        "            # residual_blocks1   kernel  input  expand output strike dilation\n",
        "        )\n",
        "        #  out 16\n",
        "        self.conv2 = Block(3, 16, 64, 24, 2)\n",
        "        self.block2 = nn.Sequential(\n",
        "            *[Block(3, 24, 72, 24, 1) for _ in range(residual_blocks * 2)]  # residual_blocks1\n",
        "        )\n",
        "        #  out 24\n",
        "\n",
        "        self.conv3 = Block(5, 24, 72, 40, 2)\n",
        "        self.block3 = nn.Sequential(\n",
        "            *[Block(5, 40, 120, 40, 1) for _ in range(residual_blocks * 3)]  # residual_blocks2\n",
        "        )\n",
        "        #  out 40\n",
        "\n",
        "        self.conv4 = Block(3, 40, 240, 80, 2, 2)\n",
        "        self.block4 = nn.Sequential(\n",
        "            *[Block(3, 80, 200, 80, 1, 4) for _ in range(residual_blocks * 4)]  # residual_blocks3\n",
        "        )\n",
        "        #  out 80\n",
        "\n",
        "        self.conv5 = Block(5, 80, 480, 160, 2)\n",
        "        self.block5 = nn.Sequential(\n",
        "            *[Block(5, 160, 672, 160, 1, 4) for _ in range(residual_blocks * 4)]\n",
        "        )\n",
        "        #  out 160\n",
        "\n",
        "        # self.up1 = nn.Sequential(\n",
        "        #     nn.Conv2d(16, 16, 1, 1, 0, bias=True),\n",
        "        #     nn.Tanh(),\n",
        "        #     # nn.Upsample(scale_factor=2 << 0, mode='bilinear')\n",
        "        # )\n",
        "\n",
        "        self.channel_reduce4 = nn.Sequential(\n",
        "            nn.Conv2d(160, 80, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce3 = nn.Sequential(\n",
        "            nn.Conv2d(80, 40, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce2 = nn.Sequential(\n",
        "            nn.Conv2d(40, 24, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.channel_reduce1 = nn.Sequential(\n",
        "            nn.Conv2d(24, 16, 3, 1, 1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        self.smooth4 = nn.Conv2d(80, 80, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3 = nn.Conv2d(40, 40, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2 = nn.Conv2d(24, 24, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth1 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            *[Block(5, 80, 160, 80, 1) for _ in range(residual_blocks * 4)],  # 3x3 original:residual_blocks*2\n",
        "            Block(5, 80, 48, 32, 1)\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.block_fusion = nn.Sequential(\n",
        "            *[Block(5, 32, 48, 32, 1) for _ in range(residual_blocks * 4)]  # 3x3 original:residual_blocks*2\n",
        "            # nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "        self.final = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # self.final =Block(5, 32, 48, 3,1)      #kernel_size, in_size, expand_size, out_size, stride\n",
        "        #  out 160\n",
        "\n",
        "    #     self.init_params()\n",
        "    #\n",
        "    # def init_params(self):\n",
        "    #     for m in self.modules():\n",
        "    #         if isinstance(m, nn.Conv2d):\n",
        "    #             init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.BatchNorm2d):\n",
        "    #             init.constant_(m.weight, 1)\n",
        "    #             init.constant_(m.bias, 0)\n",
        "    #         elif isinstance(m, nn.Linear):\n",
        "    #             init.normal_(m.weight, std=0.001)\n",
        "    #             if m.bias is not None:\n",
        "    #                 init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, images, masks):\n",
        "        x = torch.cat((images, masks), dim=1)\n",
        "        x1 = self.block1(self.conv1(x))\n",
        "        x2 = self.block2(self.conv2(x1))\n",
        "        x3 = self.block3(self.conv3(x2))\n",
        "        x4 = self.block4(self.conv4(x3))\n",
        "        x5 = self.block5(self.conv5(x4))\n",
        "        c4=self.smooth4(self._upsample_add( self.channel_reduce4(x5),x4))\n",
        "        c3=self.smooth3(self._upsample_add( self.channel_reduce3(c4),x3))\n",
        "        c2=self.smooth2(self._upsample_add( self.channel_reduce2(c3),x2))\n",
        "        c1=self.smooth1(self._upsample_add( self.channel_reduce1(c2),x1))\n",
        "        x = self.final(c1)\n",
        "        out = (torch.tanh(x) + 1) / 2\n",
        "        return out\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        '''Upsample and add two feature maps.\n",
        "        Args:\n",
        "          x: (Variable) top feature map to be upsampled.\n",
        "          y: (Variable) lateral feature map.\n",
        "        Returns:\n",
        "          (Variable) added feature map.\n",
        "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
        "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
        "        maybe not equal to the lateral feature map size.\n",
        "        e.g.\n",
        "        original input size: [N,_,15,15] ->\n",
        "        conv2d feature map size: [N,_,8,8] ->\n",
        "        upsampled feature map size: [N,_,16,16]\n",
        "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
        "        '''\n",
        "        _, _, H, W = y.size()\n",
        "        return F.upsample(x, size=(H, W), mode='bilinear') + y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxdARh5M7fHN",
        "cellView": "form"
      },
      "source": [
        "#@title [partial_arch.py](https://github.com/jacobaustin123/pytorch-inpainting-partial-conv) (2018)\n",
        "\"\"\"\n",
        "model.py (24-12-20)\n",
        "https://github.com/jacobaustin123/pytorch-inpainting-partial-conv/blob/master/model.py\n",
        "\"\"\"\n",
        "\n",
        "from torch import nn, cuda\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class PartialLayer(pl.LightningModule):\n",
        "    def __init__(self, in_size, out_size, kernel_size, stride, non_linearity=\"relu\", bn=True, multi_channel=False):\n",
        "        super(PartialLayer, self).__init__()\n",
        "\n",
        "        self.conv = PartialConv2d(in_size, out_size, kernel_size, stride, return_mask=True, padding=(kernel_size - 1) // 2, multi_channel=multi_channel, bias=not bn)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_size) if bn else None\n",
        "\n",
        "        if non_linearity == \"relu\":\n",
        "            self.non_linearity = nn.ReLU()\n",
        "        elif non_linearity == \"leaky\":\n",
        "           self.non_linearity = nn.LeakyReLU(negative_slope=0.2)\n",
        "        elif non_linearity == 'sigmoid':\n",
        "            self.non_linearity = nn.Sigmoid()\n",
        "        elif non_linearity == 'tanh':\n",
        "            self.non_linearity = nn.Tanh()\n",
        "        elif non_linearity is None:\n",
        "            self.non_linearity = None\n",
        "        else:\n",
        "            raise ValueError(\"unexpected value for non_linearity\")\n",
        "\n",
        "    def forward(self, x, mask_in=None, return_mask=True):\n",
        "        x, mask = self.conv(x, mask_in=mask_in)\n",
        "\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "\n",
        "        if self.non_linearity:\n",
        "            x = self.non_linearity(x)\n",
        "\n",
        "        if return_mask:\n",
        "            return x, mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "class Model(pl.LightningModule):\n",
        "    def __init__(self, freeze_bn=False):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.freeze_bn = freeze_bn # freeze bn layers for fine tuning\n",
        "\n",
        "        self.conv1 = PartialLayer(3, 64, 7, 2) # encoder for UNET,  use relu for encoder\n",
        "        self.conv2 = PartialLayer(64, 128, 5, 2)\n",
        "        self.conv3 = PartialLayer(128, 256, 5, 2)\n",
        "        self.conv4 = PartialLayer(256, 512, 3, 2)\n",
        "        self.conv5 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv6 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv7 = PartialLayer(512, 512, 3, 2)\n",
        "        self.conv8 = PartialLayer(512, 512, 3, 2)\n",
        "\n",
        "        self.conv9 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True) # decoder for UNET\n",
        "        self.conv10 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv11 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv12 = PartialLayer(2 * 512, 512, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv13 = PartialLayer(512 + 256, 256, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv14 = PartialLayer(256 + 128, 128, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv15 = PartialLayer(128 + 64, 64, 3, 1, non_linearity=\"leaky\", multi_channel=True)\n",
        "        self.conv16 = PartialLayer(64 + 3, 3, 3, 1, non_linearity=\"tanh\", bn=False, multi_channel=True)\n",
        "    def concat(self, input, prev):\n",
        "        return torch.cat([F.interpolate(input, scale_factor=2), prev], dim=1)\n",
        "\n",
        "    def repeat(self, mask, size1, size2):\n",
        "        return torch.cat([mask[:,0].unsqueeze(1).repeat(1, size1, 1, 1), mask[:,1].unsqueeze(1).repeat(1, size2, 1, 1)], dim=1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1, mask1 = self.conv1(x.type(torch.cuda.FloatTensor), mask_in=mask.type(torch.cuda.FloatTensor))\n",
        "        x2, mask2 = self.conv2(x1, mask_in=mask1)\n",
        "        x3, mask3 = self.conv3(x2, mask_in=mask2)\n",
        "        x4, mask4 = self.conv4(x3, mask_in=mask3)\n",
        "        x5, mask5 = self.conv5(x4, mask_in=mask4)\n",
        "        x6, mask6 = self.conv6(x5, mask_in=mask5)\n",
        "        x7, mask7 = self.conv7(x6, mask_in=mask6)\n",
        "        x8, mask8 = self.conv8(x7, mask_in=mask7)\n",
        "\n",
        "        x9, mask9 = self.conv9(self.concat(x8, x7), mask_in=self.repeat(self.concat(mask8, mask7), 512, 512))\n",
        "        x10, mask10 = self.conv10(self.concat(x9, x6), mask_in=self.repeat(self.concat(mask9, mask6), 512, 512))\n",
        "        x11, mask11 = self.conv11(self.concat(x10, x5), mask_in=self.repeat(self.concat(mask10, mask5), 512, 512))\n",
        "        x12, mask12 = self.conv12(self.concat(x11, x4), mask_in=self.repeat(self.concat(mask11, mask4), 512, 512))\n",
        "        x13, mask13 = self.conv13(self.concat(x12, x3), mask_in=self.repeat(self.concat(mask12, mask3), 512, 256))\n",
        "        x14, mask14 = self.conv14(self.concat(x13, x2), mask_in=self.repeat(self.concat(mask13, mask2), 256, 128))\n",
        "        x15, mask15 = self.conv15(self.concat(x14, x1), mask_in=self.repeat(self.concat(mask14, mask1), 128, 64))\n",
        "        out, mask16 = self.conv16(self.concat(x15, x), mask_in=self.repeat(self.concat(mask15, mask), 64, 3))\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fosumm9E7xkK"
      },
      "source": [
        "Here are some generators with two outputs. If you decide to use one of them, then make sure you calculate the first stage loss inside ``CustomTrainClass``. Custom combinations are possible, but currently there is mostly just l1 for the first stage / other image.\n",
        "\n",
        "With two outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzGGVAjL8XhR",
        "cellView": "form"
      },
      "source": [
        "#@title [crfill_arch.py](https://github.com/zengxianyu/crfill) (not sure if broken, loss not nan but quick testing didn't show results, only solid colors, needs long testing) (2020)\n",
        "\"\"\"\n",
        "convnet.py (16-12-20)\n",
        "https://github.com/zengxianyu/crfill/blob/4ed07a6a373398fcaa4c45fe926c83b20116b967/networks/convnet.py\n",
        "\n",
        "utils.py (16-12-20)\n",
        "https://github.com/zengxianyu/crfill/blob/4ed07a6a373398fcaa4c45fe926c83b20116b967/networks/utils.py\n",
        "\"\"\"\n",
        "\n",
        "from torch.nn.functional import normalize\n",
        "import numpy as np\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear) or isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif isinstance(m, nn.ConvTranspose2d) and m.in_channels == m.out_channels:\n",
        "        initial_weight = get_upsampling_weight(\n",
        "            m.in_channels, m.out_channels, m.kernel_size[0])\n",
        "        m.weight.data.copy_(initial_weight)\n",
        "\n",
        "\n",
        "class gen_conv(nn.Conv2d):\n",
        "    def __init__(self, cin, cout, ksize, stride=1, rate=1, activation=nn.ELU()):\n",
        "        \"\"\"Define conv for generator\n",
        "\n",
        "        Args:\n",
        "            cin: Input Channel number.\n",
        "            cout: output Channel number.\n",
        "            ksize: Kernel size.\n",
        "            Stride: Convolution stride.\n",
        "            rate: Rate for or dilated conv.\n",
        "            activation: Activation function after convolution.\n",
        "        \"\"\"\n",
        "        p = int(rate*(ksize-1)/2)\n",
        "        super(gen_conv, self).__init__(in_channels=cin, out_channels=cout, kernel_size=ksize, stride=stride, padding=p, dilation=rate, groups=1, bias=True)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super(gen_conv, self).forward(x)\n",
        "        if self.out_channels == 3 or self.activation is None:\n",
        "            return x\n",
        "        x, y = torch.split(x, int(self.out_channels/2), dim=1)\n",
        "        x = self.activation(x)\n",
        "        y = torch.sigmoid(y)\n",
        "        x = x * y\n",
        "        return x\n",
        "\n",
        "class gen_deconv(gen_conv):\n",
        "    def __init__(self, cin, cout):\n",
        "        \"\"\"Define deconv for generator.\n",
        "        The deconv is defined to be a x2 resize_nearest_neighbor operation with\n",
        "        additional gen_conv operation.\n",
        "\n",
        "        Args:\n",
        "            cin: Input Channel number.\n",
        "            cout: output Channel number.\n",
        "            ksize: Kernel size.\n",
        "        \"\"\"\n",
        "        super(gen_deconv, self).__init__(cin, cout, ksize=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.interpolate(x, scale_factor=2)\n",
        "        x = super(gen_deconv, self).forward(x)\n",
        "        return x\n",
        "\n",
        "class dis_conv(nn.Conv2d):\n",
        "    def __init__(self, cin, cout, ksize=5, stride=2):\n",
        "        \"\"\"Define conv for discriminator.\n",
        "        Activation is set to leaky_relu.\n",
        "\n",
        "        Args:\n",
        "            cin: Input Channel number.\n",
        "            cout: output Channel number.\n",
        "            ksize: Kernel size.\n",
        "            Stride: Convolution stride.\n",
        "        \"\"\"\n",
        "        p = int((ksize-1)/2)\n",
        "        super(dis_conv, self).__init__(in_channels=cin, out_channels=cout, kernel_size=ksize, stride=stride, padding=p, dilation=1, groups=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super(dis_conv, self).forward(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        return x\n",
        "\n",
        "def batch_conv2d(x, weight, bias=None, stride=1, padding=0, dilation=1):\n",
        "    \"\"\"Define batch convolution to use different conv. kernels in a batch.\n",
        "\n",
        "    Args:\n",
        "        x: input feature maps of shape (batch, channel, height, width)\n",
        "        weight: conv.kernels of shape (batch, out_channel, in_channels, kernel_size, kernel_size)\n",
        "    \"\"\"\n",
        "    if bias is None:\n",
        "        assert x.shape[0] == weight.shape[0], \"dim=0 of x must be equal in size to dim=0 of weight\"\n",
        "    else:\n",
        "        assert x.shape[0] == weight.shape[0] and bias.shape[0] == weight.shape[\n",
        "            0], \"dim=0 of bias must be equal in size to dim=0 of weight\"\n",
        "\n",
        "    b_i, c, h, w = x.shape\n",
        "    b_i, out_channels, in_channels, kernel_height_size, kernel_width_size = weight.shape\n",
        "\n",
        "    out = x[None, ...].view(1, b_i * c, h, w)\n",
        "    weight = weight.contiguous().view(b_i * out_channels, in_channels, kernel_height_size, kernel_width_size)\n",
        "\n",
        "    out = F.conv2d(out, weight=weight, bias=None, stride=stride, dilation=dilation, groups=b_i,\n",
        "                   padding=padding)\n",
        "\n",
        "    out = out.view(b_i, out_channels, out.shape[-2], out.shape[-1])\n",
        "\n",
        "    if bias is not None:\n",
        "        out = out + bias.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def batch_transposeconv2d(x, weight, bias=None, stride=1, padding=0, output_padding=0, dilation=1):\n",
        "    \"\"\"Define batch transposed convolution to use different conv. kernels in a batch.\n",
        "\n",
        "    Args:\n",
        "        x: input feature maps of shape (batch, channel, height, width)\n",
        "        weight: conv.kernels of shape (batch, in_channel, out_channels, kernel_size, kernel_size)\n",
        "    \"\"\"\n",
        "    if bias is None:\n",
        "        assert x.shape[0] == weight.shape[0], \"dim=0 of x must be equal in size to dim=0 of weight\"\n",
        "    else:\n",
        "        assert x.shape[0] == weight.shape[0] and bias.shape[0] == weight.shape[\n",
        "            0], \"dim=0 of bias must be equal in size to dim=0 of weight\"\n",
        "\n",
        "    b_i, c, h, w = x.shape\n",
        "    b_i, in_channels, out_channels, kernel_height_size, kernel_width_size = weight.shape\n",
        "\n",
        "    out = x[None, ...].view(1, b_i * c, h, w)\n",
        "    weight = weight.contiguous().view(in_channels*b_i, out_channels, kernel_height_size, kernel_width_size)\n",
        "\n",
        "    out = F.conv_transpose2d(out, weight=weight, bias=None, stride=stride, dilation=dilation, groups=b_i,\n",
        "                   padding=padding, output_padding=output_padding)\n",
        "\n",
        "    out = out.view(b_i, out_channels, out.shape[-2], out.shape[-1])\n",
        "\n",
        "    if bias is not None:\n",
        "        out = out + bias.unsqueeze(2).unsqueeze(3)\n",
        "    return out\n",
        "\n",
        "\n",
        "class InpaintGenerator(pl.LightningModule):\n",
        "    def __init__(self, cnum=48):\n",
        "        super(InpaintGenerator, self).__init__()\n",
        "        self.cnum = cnum\n",
        "        # stage1\n",
        "        self.conv1 = gen_conv(5, cnum, 5, 1)\n",
        "        self.conv2_downsample = gen_conv(int(cnum/2), 2*cnum, 3, 2)\n",
        "        self.conv3 = gen_conv(cnum, 2*cnum, 3, 1)\n",
        "        self.conv4_downsample = gen_conv(cnum, 4*cnum, 3, 2)\n",
        "        self.conv5 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.conv6 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.conv7_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=2)\n",
        "        self.conv8_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=4)\n",
        "        self.conv9_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=8)\n",
        "        self.conv10_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=16)\n",
        "        self.conv11 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.conv12 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.conv13_upsample_conv = gen_deconv(2*cnum, 2*cnum)\n",
        "        self.conv14 = gen_conv(cnum, 2*cnum, 3, 1)\n",
        "        self.conv15_upsample_conv = gen_deconv(cnum, cnum)\n",
        "        self.conv16 = gen_conv(cnum//2, cnum//2, 3, 1)\n",
        "        self.conv17 = gen_conv(cnum//4, 3, 3, 1, activation=None)\n",
        "\n",
        "        # stage2\n",
        "        self.xconv1 = gen_conv(3, cnum, 5, 1)\n",
        "        self.xconv2_downsample = gen_conv(cnum//2, cnum, 3, 2)\n",
        "        self.xconv3 = gen_conv(cnum//2, 2*cnum, 3, 1)\n",
        "        self.xconv4_downsample = gen_conv(cnum, 2*cnum, 3, 2)\n",
        "        self.xconv5 = gen_conv(cnum, 4*cnum, 3, 1)\n",
        "        self.xconv6 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.xconv7_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=2)\n",
        "        self.xconv8_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=4)\n",
        "        self.xconv9_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=8)\n",
        "        self.xconv10_atrous = gen_conv(2*cnum, 4*cnum, 3, rate=16)\n",
        "        self.pmconv1 = gen_conv(3, cnum, 5, 1)\n",
        "        self.pmconv2_downsample = gen_conv(cnum//2, cnum, 3, 2)\n",
        "        self.pmconv3 = gen_conv(cnum//2, 2*cnum, 3, 1)\n",
        "        self.pmconv4_downsample = gen_conv(cnum, 4*cnum, 3, 2)\n",
        "        self.pmconv5 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.pmconv6 = gen_conv(2*cnum, 4*cnum, 3, 1,\n",
        "                            activation=nn.ReLU())\n",
        "        self.pmconv9 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.pmconv10 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "\n",
        "        self.allconv11 = gen_conv(4*cnum, 4*cnum, 3, 1)\n",
        "        self.allconv12 = gen_conv(2*cnum, 4*cnum, 3, 1)\n",
        "        self.allconv13_upsample_conv = gen_deconv(2*cnum, 2*cnum)\n",
        "        self.allconv14 = gen_conv(cnum, 2*cnum, 3, 1)\n",
        "        self.allconv15_upsample_conv = gen_deconv(cnum, cnum)\n",
        "        self.allconv16 = gen_conv(cnum//2, cnum//2, 3, 1)\n",
        "        self.allconv17 = gen_conv(cnum//4, 3, 3, 1, activation=None)\n",
        "\n",
        "        self.apply(weight_init)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        xin = x\n",
        "        bsize, ch, height, width = x.shape\n",
        "        ones_x = torch.ones(bsize, 1, height, width).to(x.device)\n",
        "        x = torch.cat([x, ones_x, ones_x*mask], 1)\n",
        "\n",
        "        # two stage network\n",
        "        ## stage1\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2_downsample(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4_downsample(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7_atrous(x)\n",
        "        x = self.conv8_atrous(x)\n",
        "        x = self.conv9_atrous(x)\n",
        "        x = self.conv10_atrous(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13_upsample_conv(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15_upsample_conv(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.conv17(x)\n",
        "        x = torch.tanh(x)\n",
        "        x_stage1 = x\n",
        "\n",
        "        x = x*mask + xin[:, 0:3, :, :]*(1.-mask)\n",
        "        xnow = x\n",
        "\n",
        "        ###\n",
        "        x = self.xconv1(xnow)\n",
        "        x = self.xconv2_downsample(x)\n",
        "        x = self.xconv3(x)\n",
        "        x = self.xconv4_downsample(x)\n",
        "        x = self.xconv5(x)\n",
        "        x = self.xconv6(x)\n",
        "        x = self.xconv7_atrous(x)\n",
        "        x = self.xconv8_atrous(x)\n",
        "        x = self.xconv9_atrous(x)\n",
        "        x = self.xconv10_atrous(x)\n",
        "        x_hallu = x\n",
        "\n",
        "        ###\n",
        "        x = self.pmconv1(xnow)\n",
        "        x = self.pmconv2_downsample(x)\n",
        "        x = self.pmconv3(x)\n",
        "        x = self.pmconv4_downsample(x)\n",
        "        x = self.pmconv5(x)\n",
        "        x = self.pmconv6(x)\n",
        "\n",
        "        x = self.pmconv9(x)\n",
        "        x = self.pmconv10(x)\n",
        "        pm = x\n",
        "        x = torch.cat([x_hallu, pm], 1)\n",
        "\n",
        "        x = self.allconv11(x)\n",
        "        x = self.allconv12(x)\n",
        "        x = self.allconv13_upsample_conv(x)\n",
        "        x = self.allconv14(x)\n",
        "        x = self.allconv15_upsample_conv(x)\n",
        "        x = self.allconv16(x)\n",
        "        x = self.allconv17(x)\n",
        "        x_stage2 = torch.tanh(x)\n",
        "\n",
        "        return x_stage2, x_stage1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls5OpYV08KL-",
        "cellView": "form"
      },
      "source": [
        "#@title [deepfillv2_arch.py](https://github.com/zhaoyuzhi/deepfillv2) (2019)\n",
        "\"\"\"\n",
        "network.py (15-12-20)\n",
        "https://github.com/zhaoyuzhi/deepfillv2/blob/master/deepfillv2/network.py\n",
        "\n",
        "network_module.py (15-12-20)\n",
        "https://github.com/zhaoyuzhi/deepfillv2/blob/master/deepfillv2/network_module.py\n",
        "\"\"\"\n",
        "\n",
        "#from network_module import *\n",
        "#from .convolutions import partialconv2d\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import functional as F\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "logger = logging.getLogger('base')\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                Normal ConvBlock\n",
        "#-----------------------------------------------\n",
        "class Conv2dLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):\n",
        "        super(Conv2dLayer, self).__init__()\n",
        "        # Initialize the padding scheme\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        # Initialize the normalization type\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        elif norm == 'ln':\n",
        "            self.norm = LayerNorm(out_channels)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        # Initialize the activation funtion\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace = True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        # Initialize the convolution layers\n",
        "        if sn:\n",
        "            print(\"sn\")\n",
        "            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "        else:\n",
        "            if conv_type == 'normal':\n",
        "              self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            elif conv_type == 'partial':\n",
        "              self.conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            else:\n",
        "              print(\"conv_type not implemented\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pad(x)\n",
        "        x = self.conv2d(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class TransposeConv2dLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False, scale_factor = 2):\n",
        "        super(TransposeConv2dLayer, self).__init__()\n",
        "        # Initialize the conv scheme\n",
        "        self.scale_factor = scale_factor\n",
        "        self.conv2d = Conv2dLayer(in_channels, out_channels, conv_type, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
        "        x = self.conv2d(x)\n",
        "        return x\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                Gated ConvBlock\n",
        "#-----------------------------------------------\n",
        "class GatedConv2d(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'reflect', activation = 'lrelu', norm = 'none', sn = False):\n",
        "        super(GatedConv2d, self).__init__()\n",
        "        # Initialize the padding scheme\n",
        "        if pad_type == 'reflect':\n",
        "            self.pad = nn.ReflectionPad2d(padding)\n",
        "        elif pad_type == 'replicate':\n",
        "            self.pad = nn.ReplicationPad2d(padding)\n",
        "        elif pad_type == 'zero':\n",
        "            self.pad = nn.ZeroPad2d(padding)\n",
        "        else:\n",
        "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
        "\n",
        "        # Initialize the normalization type\n",
        "        if norm == 'bn':\n",
        "            self.norm = nn.BatchNorm2d(out_channels)\n",
        "        elif norm == 'in':\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        elif norm == 'ln':\n",
        "            self.norm = LayerNorm(out_channels)\n",
        "        elif norm == 'none':\n",
        "            self.norm = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
        "\n",
        "        # Initialize the activation funtion\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace = True)\n",
        "        elif activation == 'lrelu':\n",
        "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU(inplace = True)\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'none':\n",
        "            self.activation = None\n",
        "        else:\n",
        "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
        "\n",
        "        # Initialize the convolution layers\n",
        "        if sn:\n",
        "            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "            self.mask_conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
        "        else:\n",
        "            if conv_type == 'normal':\n",
        "              self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "              self.mask_conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            elif conv_type == 'partial':\n",
        "              self.conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "              self.mask_conv2d = PartialConv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
        "            else:\n",
        "              print(\"conv_type not implemented\")\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pad(x)\n",
        "        conv = self.conv2d(x)\n",
        "        mask = self.mask_conv2d(x)\n",
        "        gated_mask = self.sigmoid(mask)\n",
        "        x = conv * gated_mask\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class TransposeGatedConv2d(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, conv_type, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = True, scale_factor = 2):\n",
        "        super(TransposeGatedConv2d, self).__init__()\n",
        "        # Initialize the conv scheme\n",
        "        self.scale_factor = scale_factor\n",
        "        self.gated_conv2d = GatedConv2d(in_channels, out_channels, conv_type, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
        "        x = self.gated_conv2d(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------\n",
        "#               Layer Norm\n",
        "# ----------------------------------------\n",
        "class LayerNorm(pl.LightningModule):\n",
        "    def __init__(self, num_features, eps = 1e-8, affine = True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.affine = affine\n",
        "        self.eps = eps\n",
        "\n",
        "        if self.affine:\n",
        "            self.gamma = Parameter(torch.Tensor(num_features).uniform_())\n",
        "            self.beta = Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # layer norm\n",
        "        shape = [-1] + [1] * (x.dim() - 1)                                  # for 4d input: [-1, 1, 1, 1]\n",
        "        if x.size(0) == 1:\n",
        "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
        "            mean = x.view(-1).mean().view(*shape)\n",
        "            std = x.view(-1).std().view(*shape)\n",
        "        else:\n",
        "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
        "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "        # if it is learnable\n",
        "        if self.affine:\n",
        "            shape = [1, -1] + [1] * (x.dim() - 2)                          # for 4d input: [1, -1, 1, 1]\n",
        "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
        "        return x\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                  SpectralNorm\n",
        "#-----------------------------------------------\n",
        "def l2normalize(v, eps = 1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "def deepfillv2_weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)\n",
        "\n",
        "#-----------------------------------------------\n",
        "#                   Generator\n",
        "#-----------------------------------------------\n",
        "# Input: masked image + mask\n",
        "# Output: filled image\n",
        "\n",
        "#https://github.com/zhaoyuzhi/deepfillv2/blob/62dad2c601400e14d79f4d1e090c2effcb9bf3eb/deepfillv2/train.py\n",
        "class GatedGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_channels = 4, out_channels = 3, latent_channels = 64, pad_type = 'zero', activation = 'lrelu', norm = 'in', conv_type = 'normal'):\n",
        "        super(GatedGenerator, self).__init__()\n",
        "\n",
        "        self.coarse = nn.Sequential(\n",
        "            # encoder\n",
        "            GatedConv2d(in_channels, latent_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = activation, norm = 'none'),\n",
        "            GatedConv2d(latent_channels, latent_channels * 2, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # Bottleneck\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 2, dilation = 2, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 4, dilation = 4, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 8, dilation = 8, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 16, dilation = 16, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # decoder\n",
        "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 2, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels, out_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = 'tanh', norm = 'none')\n",
        "        )\n",
        "        self.refinement = nn.Sequential(\n",
        "            # encoder\n",
        "            GatedConv2d(in_channels, latent_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = activation, norm = 'none'),\n",
        "            GatedConv2d(latent_channels, latent_channels * 2, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 4, 2, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # Bottleneck\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 2, dilation = 2, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 4, dilation = 4, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 8, dilation = 8, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 16, dilation = 16, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 4, latent_channels * 4, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            # decoder\n",
        "            TransposeGatedConv2d(latent_channels * 4, latent_channels * 2, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels * 2, latent_channels * 2, conv_type, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            TransposeGatedConv2d(latent_channels * 2, latent_channels, 3, 1, 1, pad_type = pad_type, activation = activation, norm = norm),\n",
        "            GatedConv2d(latent_channels, out_channels, conv_type, 7, 1, 3, pad_type = pad_type, activation = 'tanh', norm = 'none')\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img, mask):\n",
        "        # img: entire img\n",
        "        # mask: 1 for mask region; 0 for unmask region\n",
        "        # 1 - mask: unmask\n",
        "        # img * (1 - mask): ground truth unmask region\n",
        "        # Coarse\n",
        "        #print(img.shape, mask.shape)\n",
        "        first_masked_img = img * (1 - mask) + mask\n",
        "        first_in = torch.cat((first_masked_img, mask), 1)       # in: [B, 4, H, W]\n",
        "        first_out = self.coarse(first_in)                       # out: [B, 3, H, W]\n",
        "        # Refinement\n",
        "        second_masked_img = img * (1 - mask) + first_out * mask\n",
        "        second_in = torch.cat((second_masked_img, mask), 1)     # in: [B, 4, H, W]\n",
        "        second_out = self.refinement(second_in)                 # out: [B, 3, H, W]\n",
        "        #return first_out, second_out\n",
        "        #return second_out\n",
        "        return second_out, first_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T9ZSdpP8EJL",
        "cellView": "form"
      },
      "source": [
        "#@title [deepfillv1_arch.py](https://github.com/avalonstrel/GatedConvolution_pytorch) (2018)\n",
        "\"\"\"\n",
        "networks.py (12-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/networks.py\n",
        "\n",
        "sa_gan.py (13-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/sa_gan.py\n",
        "\n",
        "spectral.py (13-12-20)\n",
        "https://github.com/avalonstrel/GatedConvolution_pytorch/blob/master/models/spectral.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def get_pad(in_,  ksize, stride, atrous=1):\n",
        "    out_ = np.ceil(float(in_)/stride)\n",
        "    return int(((out_ - 1) * stride + atrous*(ksize-1) + 1 - in_)/2)\n",
        "\n",
        "\n",
        "class GatedConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated Convlution layer with activation (default activation:LeakyReLU)\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedConv2dWithActivation, self).__init__()\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def gated(self, mask):\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "        return self.sigmoid(mask)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class GatedDeConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True,activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(GatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = GatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)\n",
        "\n",
        "class SNGatedConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated Convolution with spetral normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.mask_conv2d = torch.nn.utils.spectral_norm(self.mask_conv2d)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "    def gated(self, mask):\n",
        "        return self.sigmoid(mask)\n",
        "        #return torch.clamp(mask, -1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        mask = self.mask_conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x) * self.gated(mask)\n",
        "        else:\n",
        "            x = x * self.gated(mask)\n",
        "        if self.batch_norm:\n",
        "            return self.batch_norm2d(x)\n",
        "        else:\n",
        "            return x\n",
        "class SNGatedDeConv2dWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n",
        "    resize + conv\n",
        "    Params: same as conv2d\n",
        "    Input: The feature from last layer \"I\"\n",
        "    Output:\\phi(f(I))*\\sigmoid(g(I))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNGatedDeConv2dWithActivation, self).__init__()\n",
        "        self.conv2d = SNGatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, batch_norm, activation)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(input.size())\n",
        "        x = F.interpolate(input, scale_factor=2)\n",
        "        return self.conv2d(x)\n",
        "\n",
        "class SNConvWithActivation(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    SN convolution for spetral normalization conv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n",
        "        super(SNConvWithActivation, self).__init__()\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.conv2d = torch.nn.utils.spectral_norm(self.conv2d)\n",
        "        self.activation = activation\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "    def forward(self, input):\n",
        "        x = self.conv2d(input)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "#from .spectral import SpectralNorm\n",
        "#from .networks import GatedConv2dWithActivation, GatedDeConv2dWithActivation, SNConvWithActivation, get_pad\n",
        "class Self_Attn(pl.LightningModule):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation,with_attn=False):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "        self.with_attn = with_attn\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        if self.with_attn:\n",
        "            return out ,attention\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "class SAGenerator(pl.LightningModule):\n",
        "    \"\"\"Generator.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.imsize = image_size\n",
        "        layer1 = []\n",
        "        layer2 = []\n",
        "        layer3 = []\n",
        "        last = []\n",
        "\n",
        "        repeat_num = int(np.log2(self.imsize)) - 3\n",
        "        mult = 2 ** repeat_num # 8\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n",
        "        layer1.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = conv_dim * mult\n",
        "\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer2.append(nn.ReLU())\n",
        "\n",
        "        curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "        layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "        layer3.append(nn.ReLU())\n",
        "\n",
        "        if self.imsize == 64:\n",
        "            layer4 = []\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim / 2), 4, 2, 1)))\n",
        "            layer4.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
        "            layer4.append(nn.ReLU())\n",
        "            self.l4 = nn.Sequential(*layer4)\n",
        "            curr_dim = int(curr_dim / 2)\n",
        "\n",
        "        self.l1 = nn.Sequential(*layer1)\n",
        "        self.l2 = nn.Sequential(*layer2)\n",
        "        self.l3 = nn.Sequential(*layer3)\n",
        "\n",
        "        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
        "        last.append(nn.Tanh())\n",
        "        self.last = nn.Sequential(*last)\n",
        "\n",
        "        self.attn1 = Self_Attn( 128, 'relu')\n",
        "        self.attn2 = Self_Attn( 64,  'relu')\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        out=self.l1(z)\n",
        "        out=self.l2(out)\n",
        "        out=self.l3(out)\n",
        "        out,p1 = self.attn1(out)\n",
        "        out=self.l4(out)\n",
        "        out,p2 = self.attn2(out)\n",
        "        out=self.last(out)\n",
        "\n",
        "        return out, p1, p2\n",
        "\n",
        "class InpaintSANet(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Inpaint generator, input should be 5*256*256, where 3*256*256 is the masked image, 1*256*256 for mask, 1*256*256 is the guidence\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in_channel=5):\n",
        "        super(InpaintSANet, self).__init__()\n",
        "        cnum = 32\n",
        "        self.coarse_net = nn.Sequential(\n",
        "            #input is 5*256*256, but it is full convolution network, so it can be larger than 256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample 128\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #downsample to 64\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # atrous convlution\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            #Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            # upsample\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            #Self_Attn(2*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            #Self_Attn(cnum//2, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(128, 3, 1), activation=None)\n",
        "        )\n",
        "\n",
        "        self.refine_conv_net = nn.Sequential(\n",
        "            # input is 5*256*256\n",
        "            GatedConv2dWithActivation(n_in_channel, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(cnum, cnum, 4, 2, padding=get_pad(256, 4, 2)),\n",
        "            GatedConv2dWithActivation(cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            # downsample\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n",
        "            GatedConv2dWithActivation(2*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n",
        "            #Self_Attn(4*cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16))\n",
        "        )\n",
        "        self.refine_attn = Self_Attn(4*cnum, 'relu', with_attn=False)\n",
        "        self.refine_upsample_net = nn.Sequential(\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n",
        "            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "\n",
        "            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n",
        "            #Self_Attn(cnum, 'relu'),\n",
        "            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(256, 3, 1), activation=None),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, imgs, masks, img_exs=None):\n",
        "        # Coarse\n",
        "        #masked_imgs =  imgs * (1 - masks) + masks\n",
        "\n",
        "        if img_exs == None:\n",
        "            input_imgs = torch.cat([imgs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        else:\n",
        "            input_imgs = torch.cat([imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        #print(input_imgs.size(), imgs.size(), masks.size())\n",
        "        x = self.coarse_net(input_imgs)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "        coarse_x = x\n",
        "        # Refine\n",
        "        masked_imgs = imgs * (1 - masks) + coarse_x * masks\n",
        "        if img_exs is None:\n",
        "            input_imgs = torch.cat([masked_imgs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        else:\n",
        "            input_imgs = torch.cat([masked_imgs, img_exs, masks, torch.full_like(masks, 1.)], dim=1)\n",
        "        x = self.refine_conv_net(input_imgs)\n",
        "        x= self.refine_attn(x)\n",
        "        #print(x.size(), attention.size())\n",
        "        x = self.refine_upsample_net(x)\n",
        "        x = torch.clamp(x, -1., 1.)\n",
        "\n",
        "        return x, coarse_x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ceB_mrdmaVs"
      },
      "source": [
        "Special:\n",
        "\n",
        "Needs more attention because of amount of outputs, custom inputs or custom loss calculation. Assumes you can configure everything correctly inside ``CustomTrainClass``. Example usage is [here](https://github.com/styler00dollar/Colab-BasicSR/blob/master/codes/models/inpaint_model.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPsYK_bnGZU",
        "cellView": "form"
      },
      "source": [
        "#@title [Pluralistic_arch.py](https://github.com/lyndonzheng/Pluralistic-Inpainting) (should work, but ``CustomTrainClass`` is not ready) (2019)\n",
        "\"\"\"\n",
        "network.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/network.py\n",
        "\n",
        "external_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py\n",
        "\n",
        "base_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/base_function.py\n",
        "\n",
        "external_function.py (13-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py\n",
        "\n",
        "task.py (16-12-20)\n",
        "https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py\n",
        "\"\"\"\n",
        "\n",
        "from PIL import Image\n",
        "from random import randint\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import cv2\n",
        "import functools\n",
        "import logging\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "logger = logging.getLogger('base')\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "###################################################################\n",
        "# multi scale for image generation\n",
        "###################################################################\n",
        "\n",
        "\n",
        "def scale_img(img, size):\n",
        "    scaled_img = F.interpolate(img, size=size, mode='bilinear', align_corners=True)\n",
        "    return scaled_img\n",
        "\n",
        "\n",
        "def scale_pyramid(img, num_scales):\n",
        "    scaled_imgs = [img]\n",
        "\n",
        "    s = img.size()\n",
        "\n",
        "    h = s[2]\n",
        "    w = s[3]\n",
        "\n",
        "    for i in range(1, num_scales):\n",
        "        ratio = 2**i\n",
        "        nh = h // ratio\n",
        "        nw = w // ratio\n",
        "        scaled_img = scale_img(img, size=[nh, nw])\n",
        "        scaled_imgs.append(scaled_img)\n",
        "\n",
        "    scaled_imgs.reverse()\n",
        "    return scaled_imgs\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# spectral normalization layer to decouple the magnitude of a weight tensor\n",
        "####################################################################################################\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    spectral normalization\n",
        "    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'\n",
        "    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
        "    \"\"\"\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# neural style transform loss from neural_style_tutorial of pytorch\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def GramMatrix(input):\n",
        "    s = input.size()\n",
        "    features = input.view(s[0], s[1], s[2]*s[3])\n",
        "    features_t = torch.transpose(features, 1, 2)\n",
        "    G = torch.bmm(features, features_t).div(s[1]*s[2]*s[3])\n",
        "    return G\n",
        "\n",
        "\n",
        "def img_crop(input, size=224):\n",
        "    input_cropped = F.upsample(input, size=(size, size), mode='bilinear', align_corners=True)\n",
        "    return input_cropped\n",
        "\n",
        "\n",
        "class Normalization(pl.LightningModule):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (input-self.mean) / self.std\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "# base function for network structure\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "def pluralistic_init_weights(net, init_type='normal', gain=0.02):\n",
        "    \"\"\"Get different initial method for the network weights\"\"\"\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv')!=-1 or classname.find('Linear')!=-1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    #print('initialize network with %s' % init_type)\n",
        "    logger.info('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type='batch'):\n",
        "    \"\"\"Get the normalization layer for the networks\"\"\"\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, momentum=0.1, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=True)\n",
        "    elif norm_type == 'none':\n",
        "        norm_layer = None\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "def get_nonlinearity_layer(activation_type='PReLU'):\n",
        "    \"\"\"Get the activation layer for the networks\"\"\"\n",
        "    if activation_type == 'ReLU':\n",
        "        nonlinearity_layer = nn.ReLU()\n",
        "    elif activation_type == 'SELU':\n",
        "        nonlinearity_layer = nn.SELU()\n",
        "    elif activation_type == 'LeakyReLU':\n",
        "        nonlinearity_layer = nn.LeakyReLU(0.1)\n",
        "    elif activation_type == 'PReLU':\n",
        "        nonlinearity_layer = nn.PReLU()\n",
        "    else:\n",
        "        raise NotImplementedError('activation layer [%s] is not found' % activation_type)\n",
        "    return nonlinearity_layer\n",
        "\n",
        "def print_network(net):\n",
        "    \"\"\"print the network\"\"\"\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('total number of parameters: %.3f M' % (num_params/1e6))\n",
        "\n",
        "\n",
        "def init_net(net, init_type='normal', activation='relu', gpu_ids=[]):\n",
        "    \"\"\"print the network structure and initial the network\"\"\"\n",
        "    print_network(net)\n",
        "\n",
        "    if len(gpu_ids) > 0:\n",
        "        assert(torch.cuda.is_available())\n",
        "        net.cuda()\n",
        "        net = torch.nn.DataParallel(net, gpu_ids)\n",
        "    init_weights(net, init_type)\n",
        "    return net\n",
        "\n",
        "\n",
        "def _freeze(*args):\n",
        "    \"\"\"freeze the network for forward process\"\"\"\n",
        "    for module in args:\n",
        "        if module:\n",
        "            for p in module.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "\n",
        "def _unfreeze(*args):\n",
        "    \"\"\" unfreeze the network for parameter update\"\"\"\n",
        "    for module in args:\n",
        "        if module:\n",
        "            for p in module.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "\n",
        "def spectral_norm(module, use_spect=True):\n",
        "    \"\"\"use spectral normal layer to stable the training process\"\"\"\n",
        "    if use_spect:\n",
        "        return SpectralNorm(module)\n",
        "    else:\n",
        "        return module\n",
        "\n",
        "\n",
        "def coord_conv(input_nc, output_nc, use_spect=False, use_coord=False, with_r=False, **kwargs):\n",
        "    \"\"\"use coord convolution layer to add position information\"\"\"\n",
        "    if use_coord:\n",
        "        return CoordConv(input_nc, output_nc, with_r, use_spect, **kwargs)\n",
        "    else:\n",
        "        return spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "# Network basic function\n",
        "######################################################################################\n",
        "class AddCoords(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Add Coords to a tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, with_r=False):\n",
        "        super(AddCoords, self).__init__()\n",
        "        self.with_r = with_r\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: shape (batch, channel, x_dim, y_dim)\n",
        "        :return: shape (batch, channel+2, x_dim, y_dim)\n",
        "        \"\"\"\n",
        "        B, _, x_dim, y_dim = x.size()\n",
        "\n",
        "        # coord calculate\n",
        "        xx_channel = torch.arange(x_dim).repeat(B, 1, y_dim, 1).type_as(x)\n",
        "        yy_cahnnel = torch.arange(y_dim).repeat(B, 1, x_dim, 1).permute(0, 1, 3, 2).type_as(x)\n",
        "        # normalization\n",
        "        xx_channel = xx_channel.float() / (x_dim-1)\n",
        "        yy_cahnnel = yy_cahnnel.float() / (y_dim-1)\n",
        "        xx_channel = xx_channel * 2 - 1\n",
        "        yy_cahnnel = yy_cahnnel * 2 - 1\n",
        "\n",
        "        ret = torch.cat([x, xx_channel, yy_cahnnel], dim=1)\n",
        "\n",
        "        if self.with_r:\n",
        "            rr = torch.sqrt(xx_channel ** 2 + yy_cahnnel ** 2)\n",
        "            ret = torch.cat([ret, rr], dim=1)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class CoordConv(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    CoordConv operation\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, with_r=False, use_spect=False, **kwargs):\n",
        "        super(CoordConv, self).__init__()\n",
        "        self.addcoords = AddCoords(with_r=with_r)\n",
        "        input_nc = input_nc + 2\n",
        "        if with_r:\n",
        "            input_nc = input_nc + 1\n",
        "        self.conv = spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ret = self.addcoords(x)\n",
        "        ret = self.conv(ret)\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "class ResBlock(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define an Residual block for different types\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 sample_type='none', use_spect=False, use_coord=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
        "        self.sample = True\n",
        "        if sample_type == 'none':\n",
        "            self.sample = False\n",
        "        elif sample_type == 'up':\n",
        "            output_nc = output_nc * 4\n",
        "            self.pool = nn.PixelShuffle(upscale_factor=2)\n",
        "        elif sample_type == 'down':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        else:\n",
        "            raise NotImplementedError('sample type [%s] is not found' % sample_type)\n",
        "\n",
        "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
        "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, hidden_nc, use_spect, use_coord, **kwargs)\n",
        "        self.conv2 = coord_conv(hidden_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
        "\n",
        "        self.shortcut = nn.Sequential(self.bypass,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.sample:\n",
        "            out = self.pool(self.model(x)) + self.pool(self.shortcut(x))\n",
        "        else:\n",
        "            out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResBlockEncoderOptimized(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define an Encoder block for the first layer of the discriminator and representation network\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(), use_spect=False, use_coord=False):\n",
        "        super(ResBlockEncoderOptimized, self).__init__()\n",
        "\n",
        "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
        "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.conv2 = coord_conv(output_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(self.conv1, nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            self.model = nn.Sequential(self.conv1, norm_layer(output_nc), nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.shortcut = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), self.bypass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResBlockDecoder(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define a decoder block\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 use_spect=False, use_coord=False):\n",
        "        super(ResBlockDecoder, self).__init__()\n",
        "\n",
        "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
        "\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(input_nc, hidden_nc, kernel_size=3, stride=1, padding=1), use_spect)\n",
        "        self.conv2 = spectral_norm(nn.ConvTranspose2d(hidden_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
        "        self.bypass = spectral_norm(nn.ConvTranspose2d(input_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
        "\n",
        "        self.shortcut = nn.Sequential(self.bypass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x) + self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Output(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Define the output layer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, kernel_size = 3, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
        "                 use_spect=False, use_coord=False):\n",
        "        super(Output, self).__init__()\n",
        "\n",
        "        kwargs = {'kernel_size': kernel_size, 'padding':0, 'bias': True}\n",
        "\n",
        "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
        "\n",
        "        if type(norm_layer) == type(None):\n",
        "            self.model = nn.Sequential(nonlinearity, nn.ReflectionPad2d(int(kernel_size/2)), self.conv1, nn.Tanh())\n",
        "        else:\n",
        "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, nn.ReflectionPad2d(int(kernel_size / 2)), self.conv1, nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Auto_Attn(pl.LightningModule):\n",
        "    \"\"\" Short+Long attention Layer\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, norm_layer=nn.BatchNorm2d):\n",
        "        super(Auto_Attn, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "\n",
        "        self.query_conv = nn.Conv2d(input_nc, input_nc // 4, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.alpha = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.model = ResBlock(int(input_nc*2), input_nc, input_nc, norm_layer=norm_layer, use_spect=True)\n",
        "\n",
        "    def forward(self, x, pre=None, mask=None):\n",
        "        \"\"\"\n",
        "        inputs :\n",
        "            x : input feature maps( B X C X W X H)\n",
        "        returns :\n",
        "            out : self attention value + input feature\n",
        "            attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        B, C, W, H = x.size()\n",
        "        proj_query = self.query_conv(x).view(B, -1, W * H)  # B X (N)X C\n",
        "        proj_key = proj_query  # B X C x (N)\n",
        "\n",
        "        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)  # transpose check\n",
        "        attention = self.softmax(energy)  # BX (N) X (N)\n",
        "        proj_value = x.view(B, -1, W * H)  # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(B, C, W, H)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "\n",
        "        if type(pre) != type(None):\n",
        "            # using long distance attention layer to copy information from valid regions\n",
        "            context_flow = torch.bmm(pre.view(B, -1, W*H), attention.permute(0, 2, 1)).view(B, -1, W, H)\n",
        "            context_flow = self.alpha * (1-mask) * context_flow + (mask) * pre\n",
        "            out = self.model(torch.cat([out, context_flow], dim=1))\n",
        "\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# spectral normalization layer to decouple the magnitude of a weight tensor\n",
        "####################################################################################################\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    spectral normalization\n",
        "    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'\n",
        "    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
        "    \"\"\"\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# adversarial loss for different gan mode\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "class GANLoss(pl.LightningModule):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "    The GANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
        "        \"\"\" Initialize the GANLoss class.\n",
        "        Parameters:\n",
        "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
        "            target_real_label (bool) - - label for a real image\n",
        "            target_fake_label (bool) - - label of a fake image\n",
        "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
        "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
        "        \"\"\"\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        elif gan_mode == 'wgangp':\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real, is_disc=False):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
        "            labels = (self.real_label if target_is_real else self.fake_label).expand_as(prediction).type_as(prediction)\n",
        "            loss = self.loss(prediction, labels)\n",
        "        elif self.gan_mode in ['hinge', 'wgangp']:\n",
        "            if is_disc:\n",
        "                if target_is_real:\n",
        "                    prediction = -prediction\n",
        "                if self.gan_mode == 'hinge':\n",
        "                    loss = self.loss(1 + prediction).mean()\n",
        "                elif self.gan_mode == 'wgangp':\n",
        "                    loss = prediction.mean()\n",
        "            else:\n",
        "                loss = -prediction.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def cal_gradient_penalty(netD, real_data, fake_data, type='mixed', constant=1.0, lambda_gp=10.0):\n",
        "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
        "    Arguments:\n",
        "        netD (network)              -- discriminator network\n",
        "        real_data (tensor array)    -- real images\n",
        "        fake_data (tensor array)    -- generated images from the generator\n",
        "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
        "        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n",
        "        lambda_gp (float)           -- weight for this loss\n",
        "    Returns the gradient penalty loss\n",
        "    \"\"\"\n",
        "    if lambda_gp > 0.0:\n",
        "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
        "            interpolatesv = real_data\n",
        "        elif type == 'fake':\n",
        "            interpolatesv = fake_data\n",
        "        elif type == 'mixed':\n",
        "            alpha = torch.rand(real_data.shape[0], 1)\n",
        "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
        "            alpha = alpha.type_as(real_data)\n",
        "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "        else:\n",
        "            raise NotImplementedError('{} not implemented'.format(type))\n",
        "        interpolatesv.requires_grad_(True)\n",
        "        disc_interpolates = netD(interpolatesv)\n",
        "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
        "                                        grad_outputs=torch.ones(disc_interpolates.size()).type_as(real_data),\n",
        "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
        "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
        "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
        "        return gradient_penalty, gradients\n",
        "    else:\n",
        "        return 0.0, None\n",
        "\n",
        "\n",
        "####################################################################################################\n",
        "# neural style transform loss from neural_style_tutorial of pytorch\n",
        "####################################################################################################\n",
        "\n",
        "\n",
        "def ContentLoss(input, target):\n",
        "    target = target.detach()\n",
        "    loss = F.l1_loss(input, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def GramMatrix(input):\n",
        "    s = input.size()\n",
        "    features = input.view(s[0], s[1], s[2]*s[3])\n",
        "    features_t = torch.transpose(features, 1, 2)\n",
        "    G = torch.bmm(features, features_t).div(s[1]*s[2]*s[3])\n",
        "    return G\n",
        "\n",
        "\n",
        "def StyleLoss(input, target):\n",
        "    target = GramMatrix(target).detach()\n",
        "    input = GramMatrix(input)\n",
        "    loss = F.l1_loss(input, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def img_crop(input, size=224):\n",
        "    input_cropped = F.upsample(input, size=(size, size), mode='bilinear', align_corners=True)\n",
        "    return input_cropped\n",
        "\n",
        "\n",
        "class Normalization(pl.LightningModule):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = mean.view(-1, 1, 1)\n",
        "        self.std = std.view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (input-self.mean) / self.std\n",
        "\n",
        "\n",
        "class get_features(pl.LightningModule):\n",
        "    def __init__(self, cnn):\n",
        "        super(get_features, self).__init__()\n",
        "\n",
        "        vgg = copy.deepcopy(cnn)\n",
        "\n",
        "        self.conv1 = nn.Sequential(vgg[0], vgg[1], vgg[2], vgg[3], vgg[4])\n",
        "        self.conv2 = nn.Sequential(vgg[5], vgg[6], vgg[7], vgg[8], vgg[9])\n",
        "        self.conv3 = nn.Sequential(vgg[10], vgg[11], vgg[12], vgg[13], vgg[14], vgg[15], vgg[16])\n",
        "        self.conv4 = nn.Sequential(vgg[17], vgg[18], vgg[19], vgg[20], vgg[21], vgg[22], vgg[23])\n",
        "        self.conv5 = nn.Sequential(vgg[24], vgg[25], vgg[26], vgg[27], vgg[28], vgg[29], vgg[30])\n",
        "\n",
        "    def forward(self, input, layers):\n",
        "        input = img_crop(input)\n",
        "        output = []\n",
        "        for i in range(1, layers):\n",
        "            layer = getattr(self, 'conv'+str(i))\n",
        "            input = layer(input)\n",
        "            output.append(input)\n",
        "        return output\n",
        "\n",
        "\n",
        "##############################################################################################################\n",
        "# Network function\n",
        "##############################################################################################################\n",
        "def define_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
        "             use_coord=False, init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    net = ResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "def define_g(output_nc=3, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
        "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    net = ResGenerator(output_nc, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "def define_d(input_nc=3, ndf=64, img_f=512, layers=6, norm='none', activation='LeakyReLU', use_spect=True, use_coord=False,\n",
        "             use_attn=True,  model_type='ResDis', init_type='orthogonal', gpu_ids=[]):\n",
        "\n",
        "    if model_type == 'ResDis':\n",
        "        net = ResDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
        "    elif model_type == 'PatchDis':\n",
        "        net = PatchDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
        "\n",
        "    return init_net(net, init_type, activation, gpu_ids)\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "# Network structure\n",
        "#############################################################################################################\n",
        "class ResEncoder(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    ResNet Encoder Network\n",
        "    :param input_nc: number of channels in input\n",
        "    :param ngf: base filter channel\n",
        "    :param z_nc: latent channels\n",
        "    :param img_f: the largest feature channels\n",
        "    :param L: Number of refinements of density\n",
        "    :param layers: down and up sample layers\n",
        "    :param norm: normalization function 'instance, batch, group'\n",
        "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc=3, ngf=64, z_nc=128, img_f=1024, L=6, layers=6, norm='none', activation='ReLU',\n",
        "                 use_spect=True, use_coord=False):\n",
        "        super(ResEncoder, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.z_nc = z_nc\n",
        "        self.L = L\n",
        "\n",
        "        norm_layer = get_norm_layer(norm_type=norm)\n",
        "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
        "        # encoder part\n",
        "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "\n",
        "        mult = 1\n",
        "        for i in range(layers-1):\n",
        "            mult_prev = mult\n",
        "            mult = min(2 ** (i + 1), img_f // ngf)\n",
        "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
        "            setattr(self, 'encoder' + str(i), block)\n",
        "\n",
        "        # inference part\n",
        "        for i in range(self.L):\n",
        "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "            setattr(self, 'infer_prior' + str(i), block)\n",
        "\n",
        "        self.posterior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "        self.prior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
        "\n",
        "    def forward(self, img_m, img_c=None):\n",
        "        \"\"\"\n",
        "        :param img_m: image with mask regions I_m\n",
        "        :param img_c: complement of I_m, the mask regions\n",
        "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
        "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
        "        \"\"\"\n",
        "\n",
        "        if type(img_c) != type(None):\n",
        "            img = torch.cat([img_m, img_c], dim=0)\n",
        "        else:\n",
        "            img = img_m\n",
        "\n",
        "        # encoder part\n",
        "        out = self.block0(img)\n",
        "        feature = [out]\n",
        "        for i in range(self.layers-1):\n",
        "            model = getattr(self, 'encoder' + str(i))\n",
        "            out = model(out)\n",
        "            feature.append(out)\n",
        "\n",
        "        # infer part\n",
        "        # during the training, we have two paths, during the testing, we only have one paths\n",
        "        if type(img_c) != type(None):\n",
        "            distribution = self.two_paths(out)\n",
        "            return distribution, feature\n",
        "        else:\n",
        "            distribution = self.one_path(out)\n",
        "            return distribution, feature\n",
        "\n",
        "    def one_path(self, f_in):\n",
        "        \"\"\"one path for baseline training or testing\"\"\"\n",
        "        f_m = f_in\n",
        "        distribution = []\n",
        "\n",
        "        # infer state\n",
        "        for i in range(self.L):\n",
        "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
        "            f_m = infer_prior(f_m)\n",
        "\n",
        "        # get distribution\n",
        "        o = self.prior(f_m)\n",
        "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
        "        distribution.append([q_mu, F.softplus(q_std)])\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def two_paths(self, f_in):\n",
        "        \"\"\"two paths for the training\"\"\"\n",
        "        f_m, f_c = f_in.chunk(2)\n",
        "        distributions = []\n",
        "\n",
        "        # get distribution\n",
        "        o = self.posterior(f_c)\n",
        "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
        "        distribution = self.one_path(f_m)\n",
        "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
        "\n",
        "        return distributions\n",
        "\n",
        "\n",
        "class ResGenerator(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    ResNet Generator Network\n",
        "    :param output_nc: number of channels in output\n",
        "    :param ngf: base filter channel\n",
        "    :param z_nc: latent channels\n",
        "    :param img_f: the largest feature channels\n",
        "    :param L: Number of refinements of density\n",
        "    :param layers: down and up sample layers\n",
        "    :param norm: normalization function 'instance, batch, group'\n",
        "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
        "    :param output_scale: Different output scales\n",
        "    \"\"\"\n",
        "    def __init__(self, output_nc=3, ngf=64, z_nc=128, img_f=1024, L=1, layers=6, norm='batch', activation='ReLU',\n",
        "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
        "        super(ResGenerator, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.L = L\n",
        "        self.output_scale = output_scale\n",
        "        self.use_attn = use_attn\n",
        "\n",
        "        norm_layer = get_norm_layer(norm_type=norm)\n",
        "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
        "        # latent z to feature\n",
        "        mult = min(2 ** (layers-1), img_f // ngf)\n",
        "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
        "\n",
        "        # transform\n",
        "        for i in range(self.L):\n",
        "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
        "            setattr(self, 'generator' + str(i), block)\n",
        "\n",
        "        # decoder part\n",
        "        for i in range(layers):\n",
        "            mult_prev = mult\n",
        "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
        "            if i > layers - output_scale:\n",
        "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
        "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "            else:\n",
        "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
        "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
        "            setattr(self, 'decoder' + str(i), upconv)\n",
        "            # output part\n",
        "            if i > layers - output_scale - 1:\n",
        "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
        "                setattr(self, 'out' + str(i), outconv)\n",
        "            # short+long term attention part\n",
        "            if i == 1 and use_attn:\n",
        "                attn = Auto_Attn(ngf*mult, None)\n",
        "                setattr(self, 'attn' + str(i), attn)\n",
        "\n",
        "    def forward(self, z, f_m=None, f_e=None, mask=None):\n",
        "        \"\"\"\n",
        "        ResNet Generator Network\n",
        "        :param z: latent vector\n",
        "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
        "        :param f_e: previous encoder feature for short+long term attention layer\n",
        "        :return results: different scale generation outputs\n",
        "        \"\"\"\n",
        "\n",
        "        f = self.generator(z)\n",
        "        for i in range(self.L):\n",
        "             generator = getattr(self, 'generator' + str(i))\n",
        "             f = generator(f)\n",
        "\n",
        "        # the features come from mask regions and valid regions, we directly add them together\n",
        "        out = f_m + f\n",
        "        results= []\n",
        "        attn = 0\n",
        "        for i in range(self.layers):\n",
        "            model = getattr(self, 'decoder' + str(i))\n",
        "            out = model(out)\n",
        "            if i == 1 and self.use_attn:\n",
        "                # auto attention\n",
        "                model = getattr(self, 'attn' + str(i))\n",
        "                out, attn = model(out, f_e, mask)\n",
        "            if i > self.layers - self.output_scale - 1:\n",
        "                model = getattr(self, 'out' + str(i))\n",
        "                output = model(out)\n",
        "                results.append(output)\n",
        "                out = torch.cat([out, output], dim=1)\n",
        "\n",
        "        return results, attn\n",
        "\n",
        "# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/model/pluralistic_model.py\n",
        "# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py\n",
        "class PluralisticGenerator(pl.LightningModule):\n",
        "    def __init__(self, ngf_E=32, z_nc_E=128, img_f_E=128, layers_E=5, norm_E='none', activation_E='LeakyReLU',\n",
        "                 ngf_G=32, z_nc_G=128, img_f_G=128, L_G=0, output_scale_G=1, norm_G='instance', activation_G='LeakyReLU', train_paths='two'):\n",
        "        super().__init__()\n",
        "        self.net_E = ResEncoder(ngf=ngf_E, z_nc=z_nc_E, img_f=img_f_E, layers=layers_E, norm=norm_E, activation=activation_E)\n",
        "        self.net_G = ResGenerator(ngf=ngf_G, z_nc=z_nc_G, img_f=img_f_G, L=L_G, layers=5, output_scale=output_scale_G,\n",
        "                                      norm=norm_G, activation=activation_G)\n",
        "        self.train_paths = train_paths\n",
        "    def get_distribution(self, distributions, mask):\n",
        "        \"\"\"Calculate encoder distribution for img_m, img_c\"\"\"\n",
        "        # get distribution\n",
        "        sum_valid = (torch.mean(mask.view(mask.size(0), -1), dim=1) - 1e-5).view(-1, 1, 1, 1)\n",
        "        m_sigma = 1 / (1 + ((sum_valid - 0.8) * 8).exp_())\n",
        "        p_distribution, q_distribution, kl_rec, kl_g = 0, 0, 0, 0\n",
        "        self.distribution = []\n",
        "        for distribution in distributions:\n",
        "            p_mu, p_sigma, q_mu, q_sigma = distribution\n",
        "            # the assumption distribution for different mask regions\n",
        "            m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma))\n",
        "            # m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), torch.ones_like(p_sigma))\n",
        "            # the post distribution from mask regions\n",
        "            p_distribution = torch.distributions.Normal(p_mu, p_sigma)\n",
        "            p_distribution_fix = torch.distributions.Normal(p_mu.detach(), p_sigma.detach())\n",
        "            # the prior distribution from valid region\n",
        "            q_distribution = torch.distributions.Normal(q_mu, q_sigma)\n",
        "\n",
        "            # kl divergence\n",
        "            kl_rec += torch.distributions.kl_divergence(m_distribution, p_distribution)\n",
        "            if self.train_paths == \"one\":\n",
        "                kl_g += torch.distributions.kl_divergence(m_distribution, q_distribution)\n",
        "            elif self.train_paths == \"two\":\n",
        "                kl_g += torch.distributions.kl_divergence(p_distribution_fix, q_distribution)\n",
        "            self.distribution.append([torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma), p_mu, p_sigma, q_mu, q_sigma])\n",
        "\n",
        "        return p_distribution, q_distribution, kl_rec, kl_g\n",
        "\n",
        "    def get_G_inputs(self, p_distribution, q_distribution, f, mask):\n",
        "        \"\"\"Process the encoder feature and distributions for generation network\"\"\"\n",
        "        f_m = torch.cat([f[-1].chunk(2)[0], f[-1].chunk(2)[0]], dim=0)\n",
        "        f_e = torch.cat([f[2].chunk(2)[0], f[2].chunk(2)[0]], dim=0)\n",
        "        scale_mask = scale_img(mask, size=[f_e.size(2), f_e.size(3)])\n",
        "        mask = torch.cat([scale_mask.chunk(3, dim=1)[0], scale_mask.chunk(3, dim=1)[0]], dim=0)\n",
        "        z_p = p_distribution.rsample()\n",
        "        z_q = q_distribution.rsample()\n",
        "        z = torch.cat([z_p, z_q], dim=0)\n",
        "        return z, f_m, f_e, mask\n",
        "\n",
        "    def forward(self, images, img_inverted, masks):\n",
        "      distributions, f = self.net_E(images, img_inverted)\n",
        "      p_distribution, q_distribution, kl_rec, kl_g = self.get_distribution(distributions, masks)\n",
        "      z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f, masks)\n",
        "      results, attn = self.net_G(z, f_m, f_e, mask)\n",
        "\n",
        "      self.img_rec = []\n",
        "      self.img_g = []\n",
        "      for result in results:\n",
        "          img_rec, img_g = result.chunk(2)\n",
        "          self.img_rec.append(img_rec)\n",
        "          self.img_g.append(img_g)\n",
        "\n",
        "      return self.img_g[-1].detach(), kl_rec, kl_g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBy2VzOxnUCh",
        "cellView": "form"
      },
      "source": [
        "#@title [EdgeConnect_arch.py](https://github.com/knazeri/edge-connect) (2019)\n",
        "\"\"\"\n",
        "edge_connect.py (12-12-20)\n",
        "https://github.com/knazeri/edge-connect/blob/master/src/edge_connect.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch.optim as optim\n",
        "\n",
        "#from models.modules.architectures.convolutions.partialconv2d import PartialConv2d\n",
        "#from models.modules.architectures.convolutions.deformconv2d import DeformConv2d\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class InpaintGenerator(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks=8, init_weights=True, conv_type='deform'):\n",
        "        super(InpaintGenerator, self).__init__()\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              nn.Conv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'partial':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              PartialConv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              PartialConv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              PartialConv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'deform':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              DeformConv2d(in_nc=4, out_nc=64, kernel_size=7, padding=0),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              DeformConv2d(in_nc=64, out_nc=128, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              DeformConv2d(in_nc=128, out_nc=256, kernel_size=4, stride=2, padding=1),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            block = ResnetBlock(256, 2)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        x = (torch.tanh(x) + 1) / 2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeGenerator(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks=8, use_spectral_norm=True, init_weights=True, conv_type='normal'):\n",
        "        super(EdgeGenerator, self).__init__()\n",
        "\n",
        "        if conv_type == 'normal':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'partial':\n",
        "          self.encoder = nn.Sequential(\n",
        "              nn.ReflectionPad2d(3),\n",
        "              spectral_norm(PartialConv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(PartialConv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "              nn.ReLU(True),\n",
        "\n",
        "              spectral_norm(PartialConv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "              nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "              nn.ReLU(True)\n",
        "          )\n",
        "        elif conv_type == 'deform':\n",
        "            # without spectral_norm\n",
        "            self.encoder = nn.Sequential(\n",
        "                nn.ReflectionPad2d(3),\n",
        "                DeformConv2d(in_nc=3, out_nc=64, kernel_size=7, padding=0),\n",
        "                nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                DeformConv2d(in_nc=64, out_nc=128, kernel_size=4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                DeformConv2d(in_nc=128, out_nc=256, kernel_size=4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(256, track_running_stats=False),\n",
        "                nn.ReLU(True)\n",
        "            )\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(residual_blocks):\n",
        "            block = ResnetBlock(256, 2, use_spectral_norm=use_spectral_norm)\n",
        "            blocks.append(block)\n",
        "\n",
        "        self.middle = nn.Sequential(*blocks)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            spectral_norm(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            spectral_norm(nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=7, padding=0),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.middle(x)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(pl.LightningModule):\n",
        "    def __init__(self, dim, dilation=1, use_spectral_norm=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ReflectionPad2d(1),\n",
        "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
        "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "\n",
        "        # Remove ReLU at the end of the residual block\n",
        "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def spectral_norm(module, mode=True):\n",
        "    if mode:\n",
        "        return nn.utils.spectral_norm(module)\n",
        "\n",
        "    return module\n",
        "\n",
        "class EdgeConnectModel(pl.LightningModule):\n",
        "    def __init__(self, residual_blocks_edge=8, residual_blocks_inpaint=8, use_spectral_norm=True, conv_type_edge='normal', conv_type_inpaint='normal'):\n",
        "        super().__init__()\n",
        "        self.EdgeGenerator = EdgeGenerator(residual_blocks=residual_blocks_edge, use_spectral_norm=use_spectral_norm, conv_type=conv_type_edge)\n",
        "        self.InpaintGenerator = InpaintGenerator(residual_blocks=residual_blocks_inpaint, conv_type=conv_type_inpaint)\n",
        "\n",
        "    def forward(self, images, edges, grayscale, masks):\n",
        "        images = images.type(torch.cuda.FloatTensor)\n",
        "        edges = edges.type(torch.cuda.FloatTensor)\n",
        "        grayscale = grayscale.type(torch.cuda.FloatTensor)\n",
        "        masks = masks.type(torch.cuda.FloatTensor)\n",
        "        \n",
        "\n",
        "        # edge\n",
        "        edges_masked = (edges * masks)\n",
        "        grayscale_masked = grayscale * masks\n",
        "\n",
        "        inputs = torch.cat((grayscale_masked, edges_masked, masks), dim=1)\n",
        "        outputs_edge = self.EdgeGenerator(inputs)                                      # in: [grayscale(1) + edge(1) + mask(1)]\n",
        "\n",
        "        # inpaint\n",
        "        images_masked = (images * masks).float() + (1-masks)\n",
        "        inputs = torch.cat((images_masked, outputs_edge), dim=1)\n",
        "        outputs = self.InpaintGenerator(inputs)                                    # in: [rgb(3) + edge(1)]\n",
        "        return outputs, outputs_edge\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcKSIOD_naJV",
        "cellView": "form"
      },
      "source": [
        "#@title [FRRN_arch.py](https://github.com/ZongyuGuo/Inpainting_FRRN/) [no AMP] (2019)\n",
        "\"\"\"\n",
        "networks.py (18-12-20)\n",
        "https://github.com/ZongyuGuo/Inpainting_FRRN/blob/master/src/networks.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class FRRNet(pl.LightningModule):\n",
        "    def __init__(self, block_num=16):\n",
        "        super(FRRNet, self).__init__()\n",
        "        self.block_num = block_num\n",
        "        self.dilation_num = block_num // 2\n",
        "        blocks = []\n",
        "        for _ in range(self.block_num):\n",
        "            blocks.append(FRRBlock())\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = x.type(torch.cuda.FloatTensor)\n",
        "        mask = mask.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        mid_x = []\n",
        "        mid_m = []\n",
        "\n",
        "        mask_new = mask\n",
        "        for index in range(self.dilation_num):\n",
        "            x, _ = self.blocks[index * 2](x, mask_new, mask)\n",
        "            x, mask_new = self.blocks[index * 2 + 1](x, mask_new, mask)\n",
        "            mid_x.append(x)\n",
        "            mid_m.append(mask_new)\n",
        "\n",
        "        return x, mid_x, mid_m\n",
        "\n",
        "\n",
        "class FRRBlock(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(FRRBlock, self).__init__()\n",
        "        self.full_conv1 = PConvLayer(3,  32, kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.full_conv2 = PConvLayer(32, 32, kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.full_conv3 = PConvLayer(32, 3,  kernel_size=5, stride=1, padding=2, use_norm=False)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.branch_conv1 = PConvLayer(3,   64,  kernel_size=3, stride=2, padding=1, use_norm=False)\n",
        "        self.branch_conv2 = PConvLayer(64,  96,  kernel_size=3, stride=2, padding=1)\n",
        "        self.branch_conv3 = PConvLayer(96,  128, kernel_size=3, stride=2, padding=1)\n",
        "        self.branch_conv4 = PConvLayer(128, 96,  kernel_size=3, stride=1, padding=1, act='LeakyReLU')\n",
        "        self.branch_conv5 = PConvLayer(96,  64,  kernel_size=3, stride=1, padding=1, act='LeakyReLU')\n",
        "        self.branch_conv6 = PConvLayer(64,  3,   kernel_size=3, stride=1, padding=1, act='Tanh')\n",
        "\n",
        "    def forward(self, input, mask, mask_ori):\n",
        "        x = input\n",
        "        out_f, mask_f = self.full_conv1(x, mask)\n",
        "        out_f, mask_f = self.full_conv2(out_f, mask_f)\n",
        "        out_f, mask_f = self.full_conv3(out_f, mask_f)\n",
        "\n",
        "        out_b, mask_b = self.branch_conv1(x, mask)\n",
        "        out_b, mask_b = self.branch_conv2(out_b, mask_b)\n",
        "        out_b, mask_b = self.branch_conv3(out_b, mask_b)\n",
        "\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv4(out_b, mask_b)\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv5(out_b, mask_b)\n",
        "        out_b = self.upsample(out_b)\n",
        "        mask_b = self.upsample(mask_b)\n",
        "        out_b, mask_b = self.branch_conv6(out_b, mask_b)\n",
        "\n",
        "        mask_new = mask_f * mask_b\n",
        "        out = (out_f * mask_new + out_b * mask_new) / 2 * (1 - mask_ori) + input\n",
        "        #out = (out_f * mask_new + out_b * mask_new) / 2 * (1 - mask_ori) + input * mask_ori\n",
        "        return out, mask_new\n",
        "\n",
        "\n",
        "\n",
        "class PConvLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act='ReLU', use_norm=True):\n",
        "        super(PConvLayer, self).__init__()\n",
        "        self.conv = PartialConv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride, padding=padding, return_mask=True)\n",
        "        self.norm = nn.InstanceNorm2d(out_channels, track_running_stats=False)\n",
        "        self.use_norm = use_norm\n",
        "        if act == 'ReLU':\n",
        "            self.act = nn.ReLU(True)\n",
        "        elif act == 'LeakyReLU':\n",
        "            self.act = nn.LeakyReLU(0.2, True)\n",
        "        elif act == 'Tanh':\n",
        "            self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        x, mask_update = self.conv(x, mask)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        x = self.act(x)\n",
        "        return x, mask_update\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDxfzr73nfuC",
        "cellView": "form"
      },
      "source": [
        "#@title [PRVS_arch.py](https://github.com/jingyuanli001/PRVS-Image-Inpainting) [no AMP] (2019)\n",
        "\"\"\"\n",
        "model.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/model.py\n",
        "\n",
        "PRVSNet.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/PRVSNet.py\n",
        "\n",
        "partialconv2d.py (18-12-20) # using their partconv2d to avoid dimension errors\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/partialconv2d.py\n",
        "\n",
        "PConvLayer.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/PConvLayer.py\n",
        "\n",
        "VSRLayer.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/VSRLayer.py\n",
        "\n",
        "Attention.py (18-12-20)\n",
        "https://github.com/jingyuanli001/PRVS-Image-Inpainting/blob/master/modules/Attention.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchvision import models\n",
        "#from .convolutions import partialconv2d\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "###############################################################################\n",
        "# BSD 3-Clause License\n",
        "#\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# Author & Contact: Guilin Liu (guilinl@nvidia.com)\n",
        "###############################################################################\n",
        "\n",
        "class PartialConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "\n",
        "        # whether the mask is multi-channel or not\n",
        "        if 'multi_channel' in kwargs:\n",
        "            self.multi_channel = kwargs['multi_channel']\n",
        "            kwargs.pop('multi_channel')\n",
        "        else:\n",
        "            self.multi_channel = False\n",
        "\n",
        "        self.return_mask = True\n",
        "\n",
        "        super(PartialConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if self.multi_channel:\n",
        "            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        else:\n",
        "            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])\n",
        "\n",
        "        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]\n",
        "\n",
        "        self.last_size = (None, None)\n",
        "        self.update_mask = None\n",
        "        self.mask_ratio = None\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "\n",
        "        if mask is not None or self.last_size != (input.data.shape[2], input.data.shape[3]):\n",
        "            self.last_size = (input.data.shape[2], input.data.shape[3])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != input.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)\n",
        "\n",
        "                if mask is None:\n",
        "                    # if mask is not provided, create a mask\n",
        "                    if self.multi_channel:\n",
        "                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)\n",
        "                    else:\n",
        "                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)\n",
        "\n",
        "                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)\n",
        "\n",
        "                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)\n",
        "                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)\n",
        "                self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        if self.update_mask.type() != input.type() or self.mask_ratio.type() != input.type():\n",
        "            self.update_mask.to(input)\n",
        "            self.mask_ratio.to(input)\n",
        "\n",
        "        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask is not None else input)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view\n",
        "            output = torch.mul(output, self.update_mask)\n",
        "        else:\n",
        "            output = torch.mul(raw_out, self.mask_ratio)\n",
        "\n",
        "\n",
        "        if self.return_mask:\n",
        "            return output, self.update_mask\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "PartialConv = PartialConv2d\n",
        "\n",
        "class AttentionModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, patch_size = 3, propagate_size = 3, stride = 1):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.propagate_size = propagate_size\n",
        "        self.stride = stride\n",
        "        self.prop_kernels = None\n",
        "\n",
        "    def forward(self, foreground, masks):\n",
        "        ###assume the masked area has value 1\n",
        "        bz, nc, w, h = foreground.size()\n",
        "        if masks.size(3) != foreground.size(3):\n",
        "            masks = F.interpolate(masks, foreground.size()[2:])\n",
        "        background = foreground.clone()\n",
        "        background = background * masks\n",
        "        background = F.pad(background, [self.patch_size//2, self.patch_size//2, self.patch_size//2, self.patch_size//2])\n",
        "        conv_kernels_all = background.unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride).contiguous().view(bz, nc, -1, self.patch_size, self.patch_size)\n",
        "        conv_kernels_all = conv_kernels_all.transpose(2, 1)\n",
        "        output_tensor = []\n",
        "        for i in range(bz):\n",
        "            mask = masks[i:i+1]\n",
        "            feature_map = foreground[i:i+1]\n",
        "            #form convolutional kernels\n",
        "            conv_kernels = conv_kernels_all[i] + 0.0000001\n",
        "            norm_factor = torch.sum(conv_kernels**2, [1, 2, 3], keepdim = True)**0.5\n",
        "            conv_kernels = conv_kernels/norm_factor\n",
        "\n",
        "            conv_result = F.conv2d(feature_map, conv_kernels, padding = self.patch_size//2)\n",
        "            if self.propagate_size != 1:\n",
        "                if self.prop_kernels is None:\n",
        "                    self.prop_kernels = torch.ones([conv_result.size(1), 1, self.propagate_size, self.propagate_size])\n",
        "                    self.prop_kernels.requires_grad = False\n",
        "                    self.prop_kernels = self.prop_kernels.cuda()\n",
        "                conv_result = F.conv2d(conv_result, self.prop_kernels, stride = 1, padding = 1, groups = conv_result.size(1))\n",
        "            attention_scores = F.softmax(conv_result, dim = 1)\n",
        "            ##propagate the scores\n",
        "            recovered_foreground = F.conv_transpose2d(attention_scores, conv_kernels, stride = 1, padding = self.patch_size//2)\n",
        "            #average the recovered value, at the same time make non-masked area 0\n",
        "            recovered_foreground = (recovered_foreground * (1 - mask))/(self.patch_size ** 2)\n",
        "            #recover the image\n",
        "            final_output = recovered_foreground + feature_map * mask\n",
        "            output_tensor.append(final_output)\n",
        "        return torch.cat(output_tensor, dim = 0)\n",
        "\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class EdgeGenerator(pl.LightningModule):\n",
        "    def __init__(self, in_channels_feature, kernel_s = 3, add_last_edge = True):\n",
        "        super(EdgeGenerator, self).__init__()\n",
        "\n",
        "        self.p_conv = PartialConv2d(in_channels_feature + 1, 64, kernel_size = kernel_s, stride = 1, padding = kernel_s // 2, multi_channel = True, bias = False)\n",
        "\n",
        "        self.edge_resolver = Bottleneck(64, 16)\n",
        "        self.out_layer = nn.Conv2d(64, 1, 1, bias = False)\n",
        "\n",
        "    def forward(self, in_x, mask):\n",
        "        x, mask_updated = self.p_conv(in_x, mask)\n",
        "        x = self.edge_resolver(x)\n",
        "        edge_out = self.out_layer(x)\n",
        "        return edge_out, mask_updated\n",
        "\n",
        "class VSRLayer(pl.LightningModule):\n",
        "    def __init__(self, in_channel, out_channel, stride = 2, kernel_size = 3, batch_norm = True, activation = \"ReLU\", deconv = False):\n",
        "        super(VSRLayer, self).__init__()\n",
        "        self.edge_generator = EdgeGenerator(in_channel, kernel_s = kernel_size)\n",
        "        self.feat_rec = PartialConv(in_channel+1, out_channel, stride = stride, kernel_size = kernel_size, padding = kernel_size//2, multi_channel = True)\n",
        "        if deconv:\n",
        "            self.deconv = nn.ConvTranspose2d(out_channel, out_channel, 4, 2, 1)\n",
        "        else:\n",
        "            self.deconv = lambda x:x\n",
        "\n",
        "        if batch_norm:\n",
        "            self.batchnorm = nn.BatchNorm2d(out_channel)\n",
        "        else:\n",
        "            self.batchnorm = lambda x:x\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        if activation == \"ReLU\":\n",
        "            self.activation = nn.ReLU(True)\n",
        "        elif activation == \"Leaky\":\n",
        "            self.activation = nn.LeakyReLU(0.2, True)\n",
        "        else:\n",
        "            self.activation = lambda x:x\n",
        "\n",
        "    def forward(self, feat_in, mask_in, edge_in):\n",
        "        edge_in = F.interpolate(edge_in, size = feat_in.size()[2:])\n",
        "        edge_updated, mask_updated = self.edge_generator(torch.cat([feat_in, edge_in], dim = 1), torch.cat([mask_in, mask_in[:,:1,:,:]], dim = 1))\n",
        "        edge_reconstructed = edge_in * mask_in[:,:1,:,:] + edge_updated * (mask_updated[:,:1,:,:] - mask_in[:,:1,:,:])\n",
        "        feat_out, feat_mask = self.feat_rec(torch.cat([feat_in, edge_reconstructed], dim = 1), torch.cat([mask_in, mask_updated[:,:1,:,:]], dim = 1))\n",
        "        feat_out = self.deconv(feat_out)\n",
        "        feat_out = self.batchnorm(feat_out)\n",
        "        feat_out = self.activation(feat_out)\n",
        "        mask_updated = F.interpolate(mask_updated, size = feat_out.size()[2:])\n",
        "        feat_mask = F.interpolate(feat_mask, size = feat_out.size()[2:])\n",
        "        return feat_out, feat_mask*mask_updated[:,0:1,:,:], edge_reconstructed\n",
        "\n",
        "\n",
        "class PConvLayer(pl.LightningModule):\n",
        "    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='relu',\n",
        "                 conv_bias=False, deconv = False):\n",
        "        super().__init__()\n",
        "        if sample == 'down-5':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 5, 2, 2, bias=conv_bias, multi_channel = True)\n",
        "        elif sample == 'down-7':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 7, 2, 3, bias=conv_bias, multi_channel = True)\n",
        "        elif sample == 'down-3':\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias, multi_channel = True)\n",
        "        else:\n",
        "            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias, multi_channel = True)\n",
        "        if deconv:\n",
        "            self.deconv = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1, bias = conv_bias)\n",
        "        else:\n",
        "            self.deconv = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(out_ch)\n",
        "        if activ == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activ == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, input, input_mask):\n",
        "        h, h_mask = self.conv(input, input_mask)\n",
        "        if self.deconv is not None:\n",
        "            h = self.deconv(h)\n",
        "        if hasattr(self, 'bn'):\n",
        "            h = self.bn(h)\n",
        "        if hasattr(self, 'activation'):\n",
        "            h = self.activation(h)\n",
        "        h_mask = F.interpolate(h_mask, size = h.size()[2:])\n",
        "        return h, h_mask\n",
        "\n",
        "class VGG16FeatureExtractor(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(3):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(3):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "class Bottleneck(pl.LightningModule):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PRVSNet(pl.LightningModule):\n",
        "    def __init__(self, layer_size=8, input_channels=3, att = False):\n",
        "        super().__init__()\n",
        "        self.layer_size = layer_size\n",
        "        self.enc_1 = VSRLayer(3, 64, kernel_size = 7)\n",
        "        self.enc_2 = VSRLayer(64, 128, kernel_size = 5)\n",
        "        self.enc_3 = PConvLayer(128, 256, sample='down-5')\n",
        "        self.enc_4 = PConvLayer(256, 512, sample='down-3')\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'enc_{:d}'.format(i + 1)\n",
        "            setattr(self, name, PConvLayer(512, 512, sample='down-3'))\n",
        "        self.deconv = nn.ConvTranspose2d(512, 512, 4, 2, 1)\n",
        "        for i in range(4, self.layer_size):\n",
        "            name = 'dec_{:d}'.format(i + 1)\n",
        "            setattr(self, name, PConvLayer(512 + 512, 512, activ='leaky', deconv = True))\n",
        "        self.dec_4 = PConvLayer(512 + 256, 256, activ='leaky', deconv = True)\n",
        "        if att:\n",
        "            self.att = Attention.AttentionModule()\n",
        "        else:\n",
        "            self.att = lambda x:x\n",
        "        self.dec_3 = PConvLayer(256 + 128, 128, activ='leaky', deconv = True)\n",
        "        self.dec_2 = VSRLayer(128 + 64, 64, stride = 1, activation='leaky', deconv = True)\n",
        "        self.dec_1 = VSRLayer(64 + input_channels, 64, stride = 1, activation = None, batch_norm = False)\n",
        "        self.resolver = Bottleneck(64,16)\n",
        "        self.output = nn.Conv2d(128, 3, 1)\n",
        "\n",
        "    def forward(self, input, input_mask, input_edge):\n",
        "        input = input.type(torch.cuda.FloatTensor)\n",
        "        input_mask = input_mask.type(torch.cuda.FloatTensor)\n",
        "        input_edge = input_edge.type(torch.cuda.FloatTensor)\n",
        "\n",
        "        input = input * input_mask[:,0:1,:,:]\n",
        "        input_edge = input_edge * input_mask[:,0:1,:,:]\n",
        "        input_mask = torch.cat([input_mask]*3, dim = 1)\n",
        "        input_mask = input_mask[:,:3,:,:]\n",
        "\n",
        "\n",
        "        h_dict = {}  # for the output of enc_N\n",
        "        h_mask_dict = {}  # for the output of enc_N\n",
        "        h_edge_list = []\n",
        "        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n",
        "        edge = input_edge\n",
        "\n",
        "        h_key_prev = 'h_0'\n",
        "        for i in range(1, self.layer_size + 1):\n",
        "            l_key = 'enc_{:d}'.format(i)\n",
        "            h_key = 'h_{:d}'.format(i)\n",
        "            if i not in [1, 2]:\n",
        "                h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(h_dict[h_key_prev], h_mask_dict[h_key_prev])\n",
        "            else:\n",
        "                h_dict[h_key], h_mask_dict[h_key], edge = getattr(self, l_key)(h_dict[h_key_prev], h_mask_dict[h_key_prev], edge)\n",
        "                h_edge_list.append(edge)\n",
        "            h_key_prev = h_key\n",
        "\n",
        "        h_key = 'h_{:d}'.format(self.layer_size)\n",
        "        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n",
        "        h = self.deconv(h)\n",
        "        h_mask = F.interpolate(h_mask, scale_factor = 2)\n",
        "\n",
        "        for i in range(self.layer_size, 0, -1):\n",
        "            enc_h_key = 'h_{:d}'.format(i - 1)\n",
        "            dec_l_key = 'dec_{:d}'.format(i)\n",
        "            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim = 1)\n",
        "            h = torch.cat([h, h_dict[enc_h_key]], dim = 1)\n",
        "            if i not in [2, 1]:\n",
        "                h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n",
        "            else:\n",
        "                edge = h_edge_list[i-1]\n",
        "                h, h_mask, edge = getattr(self, dec_l_key)(h, h_mask, edge)\n",
        "                h_edge_list.append(edge)\n",
        "            if i == 4:\n",
        "                h = self.att(h)\n",
        "        h_out = self.resolver(h)\n",
        "        h_out = torch.cat([h_out, h], dim = 1)\n",
        "        h_out = self.output(h_out)\n",
        "        return h_out, h_mask, h_edge_list[-2], h_edge_list[-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAFa5pvrntKK",
        "cellView": "form"
      },
      "source": [
        "#@title [CSA_arch.py](https://github.com/Yukariin/CSA_pytorch) (2019)\n",
        "\"\"\"\n",
        "model.py (13-12-20)\n",
        "https://github.com/Yukariin/CSA_pytorch/blob/master/model.py\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def get_norm(name, out_channels):\n",
        "    if name == 'batch':\n",
        "        norm = nn.BatchNorm2d(out_channels)\n",
        "    elif name == 'instance':\n",
        "        norm = nn.InstanceNorm2d(out_channels)\n",
        "    else:\n",
        "        norm = None\n",
        "    return norm\n",
        "\n",
        "\n",
        "def get_act(name):\n",
        "    if name == 'relu':\n",
        "        activation = nn.ReLU(inplace=True)\n",
        "    elif name == 'elu':\n",
        "        activation == nn.ELU(inplace=True)\n",
        "    elif name == 'leaky_relu':\n",
        "        activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "    elif name == 'tanh':\n",
        "        activation = nn.Tanh()\n",
        "    elif name == 'sigmoid':\n",
        "        activation = nn.Sigmoid()\n",
        "    else:\n",
        "        activation = None\n",
        "    return activation\n",
        "\n",
        "\n",
        "class CoarseEncodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class CoarseDecodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class CoarseNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3,\n",
        "                 norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        cnum = 64\n",
        "\n",
        "        self.en_1 = nn.Conv2d(c_img, cnum, 4, 2, padding=1)\n",
        "        self.en_2 = CoarseEncodeBlock(cnum, cnum*2, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_3 = CoarseEncodeBlock(cnum*2, cnum*4, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_4 = CoarseEncodeBlock(cnum*4, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_5 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_6 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_7 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_en)\n",
        "        self.en_8 = CoarseEncodeBlock(cnum*8, cnum*8, 4, 2, activation=act_en)\n",
        "\n",
        "        self.de_8 = CoarseDecodeBlock(cnum*8, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_7 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_6 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_5 = CoarseDecodeBlock(cnum*8*2, cnum*8, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_4 = CoarseDecodeBlock(cnum*8*2, cnum*4, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_3 = CoarseDecodeBlock(cnum*4*2, cnum*2, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_2 = CoarseDecodeBlock(cnum*2*2, cnum, 4, 2, normalization=norm, activation=act_de)\n",
        "        self.de_1 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*2, c_img, 4, 2, padding=1),\n",
        "            get_act('tanh'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1 = self.en_1(x)\n",
        "        out_2 = self.en_2(out_1)\n",
        "        out_3 = self.en_3(out_2)\n",
        "        out_4 = self.en_4(out_3)\n",
        "        out_5 = self.en_5(out_4)\n",
        "        out_6 = self.en_6(out_5)\n",
        "        out_7 = self.en_7(out_6)\n",
        "        out_8 = self.en_8(out_7)\n",
        "\n",
        "        dout_8 = self.de_8(out_8)\n",
        "        dout_8_out_7 = torch.cat([dout_8, out_7], 1)\n",
        "        dout_7 = self.de_7(dout_8_out_7)\n",
        "        dout_7_out_6 = torch.cat([dout_7, out_6], 1)\n",
        "        dout_6 = self.de_6(dout_7_out_6)\n",
        "        dout_6_out_5 = torch.cat([dout_6, out_5], 1)\n",
        "        dout_5 = self.de_5(dout_6_out_5)\n",
        "        dout_5_out_4 = torch.cat([dout_5, out_4], 1)\n",
        "        dout_4 = self.de_4(dout_5_out_4)\n",
        "        dout_4_out_3 = torch.cat([dout_4, out_3], 1)\n",
        "        dout_3 = self.de_3(dout_4_out_3)\n",
        "        dout_3_out_2 = torch.cat([dout_3, out_2], 1)\n",
        "        dout_2 = self.de_2(dout_3_out_2)\n",
        "        dout_2_out_1 = torch.cat([dout_2, out_1], 1)\n",
        "        dout_1 = self.de_1(dout_2_out_1)\n",
        "\n",
        "        return dout_1\n",
        "\n",
        "\n",
        "class RefineEncodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, in_channels, 4, 2, dilation=2, padding=3))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.encode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "class RefineDecodeBlock(pl.LightningModule):\n",
        "    def __init__(self, in_channels, out_channels,\n",
        "                 normalization=None, activation=None):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 3, 1, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "\n",
        "        if activation:\n",
        "            layers.append(get_act(activation))\n",
        "        layers.append(\n",
        "            nn.ConvTranspose2d(out_channels, out_channels, 4, 2, padding=1))\n",
        "        if normalization:\n",
        "            layers.append(get_norm(normalization, out_channels))\n",
        "        self.decode = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class RefineNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3,\n",
        "                 norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        c_in = c_img + c_img\n",
        "        cnum = 64\n",
        "\n",
        "        self.en_1 = nn.Conv2d(c_in, cnum, 3, 1, padding=1)\n",
        "        self.en_2 = RefineEncodeBlock(cnum, cnum*2, normalization=norm, activation=act_en)\n",
        "        self.en_3 = RefineEncodeBlock(cnum*2, cnum*4, normalization=norm, activation=act_en)\n",
        "        self.en_4 = RefineEncodeBlock(cnum*4, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_5 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_6 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_7 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_8 = RefineEncodeBlock(cnum*8, cnum*8, normalization=norm, activation=act_en)\n",
        "        self.en_9 = nn.Sequential(\n",
        "            get_act(act_en),\n",
        "            nn.Conv2d(cnum*8, cnum*8, 4, 2, padding=1))\n",
        "\n",
        "        self.de_9 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*8, cnum*8, 4, 2, padding=1),\n",
        "            get_norm(norm, cnum*8))\n",
        "        self.de_8 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_7 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_6 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_5 = RefineDecodeBlock(cnum*8*2, cnum*8, normalization=norm, activation=act_de)\n",
        "        self.de_4 = RefineDecodeBlock(cnum*8*2, cnum*4, normalization=norm, activation=act_de)\n",
        "        self.de_3 = RefineDecodeBlock(cnum*4*2, cnum*2, normalization=norm, activation=act_de)\n",
        "        self.de_2 = RefineDecodeBlock(cnum*2*2, cnum, normalization=norm, activation=act_de)\n",
        "\n",
        "        self.de_1 = nn.Sequential(\n",
        "            get_act(act_de),\n",
        "            nn.ConvTranspose2d(cnum*2, c_img, 3, 1, padding=1))\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = torch.cat([x1, x2], 1)\n",
        "        out_1 = self.en_1(x)\n",
        "        out_2 = self.en_2(out_1)\n",
        "        out_3 = self.en_3(out_2)\n",
        "        out_4 = self.en_4(out_3)\n",
        "        out_5 = self.en_5(out_4)\n",
        "        out_6 = self.en_6(out_5)\n",
        "        out_7 = self.en_7(out_6)\n",
        "        out_8 = self.en_8(out_7)\n",
        "        out_9 = self.en_9(out_8)\n",
        "\n",
        "        dout_9 = self.de_9(out_9)\n",
        "        dout_9_out_8 = torch.cat([dout_9, out_8], 1)\n",
        "        dout_8 = self.de_8(dout_9_out_8)\n",
        "        dout_8_out_7 = torch.cat([dout_8, out_7], 1)\n",
        "        dout_7 = self.de_7(dout_8_out_7)\n",
        "        dout_7_out_6 = torch.cat([dout_7, out_6], 1)\n",
        "        dout_6 = self.de_6(dout_7_out_6)\n",
        "        dout_6_out_5 = torch.cat([dout_6, out_5], 1)\n",
        "        dout_5 = self.de_5(dout_6_out_5)\n",
        "        dout_5_out_4 = torch.cat([dout_5, out_4], 1)\n",
        "        dout_4 = self.de_4(dout_5_out_4)\n",
        "        dout_4_out_3 = torch.cat([dout_4, out_3], 1)\n",
        "        dout_3 = self.de_3(dout_4_out_3)\n",
        "        dout_3_out_2 = torch.cat([dout_3, out_2], 1)\n",
        "        dout_2 = self.de_2(dout_3_out_2)\n",
        "        dout_2_out_1 = torch.cat([dout_2, out_1], 1)\n",
        "        dout_1 = self.de_1(dout_2_out_1)\n",
        "\n",
        "        return dout_1, out_4, dout_5\n",
        "\n",
        "\n",
        "class CSA(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        return x\n",
        "\n",
        "\n",
        "class InpaintNet(pl.LightningModule):\n",
        "    def __init__(self, c_img=3, norm='instance', act_en='leaky_relu', act_de='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.coarse = CoarseNet(c_img=c_img, norm=norm, act_en=act_en, act_de=act_de)\n",
        "        self.refine = RefineNet(c_img=c_img, norm=norm, act_en=act_en, act_de=act_de)\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        out_c = self.coarse(image)\n",
        "        out_c = image * (1. - mask) + out_c * mask\n",
        "        out_r, csa, csa_d = self.refine(out_c, image)\n",
        "        return out_c, out_r, csa, csa_d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy958bvZqGqs",
        "cellView": "form"
      },
      "source": [
        "#@title CSA_loss.py\n",
        "\"\"\"\n",
        "# needs to be merged with loss.py\n",
        "loss.py (16-12-20)\n",
        "https://github.com/Yukariin/CSA_pytorch/blob/master/loss.py\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2 # [-1,1] -> [0,1]\n",
        "    return out.clamp_(0, 1)\n",
        "\n",
        "\n",
        "class VGG16FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "        self.enc_4 = nn.Sequential(*vgg16.features[17:23])\n",
        "\n",
        "        #print(self.enc_1)\n",
        "        #print(self.enc_2)\n",
        "        #print(self.enc_3)\n",
        "        #print(self.enc_4)\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(4):\n",
        "            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, image):\n",
        "        results = [image]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "\n",
        "class ConsistencyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        self.vgg = VGG16FeatureExtractor()\n",
        "        self.vgg.cuda()\n",
        "\n",
        "        self.l2 = nn.MSELoss()\n",
        "\n",
        "    def forward(self, csa, csa_d, target, mask):\n",
        "        # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "        # Pre-trained VGG16 model expect input images normalized in the same way.\n",
        "        # The images have to be loaded in to a range of [0, 1]\n",
        "        # and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "        t = denorm(target) # [-1,1] -> [0,1]\n",
        "        t = self.normalize(t[0]) # BxCxHxW -> CxHxW -> normalize\n",
        "        t = t.unsqueeze(0) # CxHxW -> BxCxHxW\n",
        "\n",
        "        vgg_gt = self.vgg(t)\n",
        "        vgg_gt = vgg_gt[-1]\n",
        "\n",
        "        mask_r = F.interpolate(mask, size=csa.size()[2:])\n",
        "\n",
        "        lossvalue = self.l2(csa*mask_r, vgg_gt*mask_r) + self.l2(csa_d*mask_r, vgg_gt*mask_r)\n",
        "        return lossvalue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ybOcG4cN-57"
      },
      "source": [
        "Broken generators:\n",
        "\n",
        "Generators that are not included here since I can't seem to make them work properly:\n",
        "\n",
        "PenNet [no AMP] (2019): [researchmm/PEN-Net-for-Inpainting](https://github.com/researchmm/PEN-Net-for-Inpainting/)\n",
        "- Always outputs white for some reason.\n",
        "\n",
        "CRA [no AMP] (2019): [wangyx240/High-Resolution-Image-Inpainting-GAN](https://github.com/wangyx240/High-Resolution-Image-Inpainting-GAN)\n",
        "- Likes to create the color pink.\n",
        "\n",
        "Global [no AMP] (2017): [SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting](https://github.com/SayedNadim/Global-and-Local-Attention-Based-Free-Form-Image-Inpainting)\n",
        "- Always outputs white for some reason.\n",
        "\n",
        "---------------------------\n",
        "\n",
        "Non-Pytorch generators:\n",
        "\n",
        "R-MNet (2021): [Jireh-Jam/R-MNet-Inpainting-keras](https://github.com/Jireh-Jam/R-MNet-Inpainting-keras)\n",
        "- Not sure if there is much new and interesting stuff.\n",
        "\n",
        "\n",
        "Hypergraphs (2021): [GouravWadhwa/Hypergraphs-Image-Inpainting](https://github.com/GouravWadhwa/Hypergraphs-Image-Inpainting)\n",
        "- Uses custom conv layer (that is implemented with tensorflow). It sounds interesting, but I got errors when I tried to port it to pytorch.\n",
        "\n",
        "PEPSI (2019): [Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network](https://github.com/Forty-lock/PEPSI-Fast_image_inpainting_with_parallel_decoding_network)\n",
        "- The net dcpV2 uses.\n",
        "\n",
        "Region (2019): [vickyFox/Region-wise-Inpainting](https://github.com/vickyFox/Region-wise-Inpainting)\n",
        "\n",
        "---------------------------\n",
        "\n",
        "Pytorch generators that I never tested:\n",
        "\n",
        "VCNET (2020): [birdortyedi/vcnet-blind-image-inpainting](https://github.com/birdortyedi/vcnet-blind-image-inpainting)\n",
        "- Blind image inpainting without masks.\n",
        "\n",
        "DFMA (2020): [mprzewie/dmfa_inpainting](https://github.com/mprzewie/dmfa_inpainting)\n",
        "\n",
        "GMCNN (2018): [shepnerd/inpainting_gmcnn](https://github.com/shepnerd/inpainting_gmcnn)\n",
        "- The net dcpV1 used iirc.\n",
        "\n",
        "ShiftNet (2018): [Zhaoyi-Yan/Shift-Net_pytorch](https://github.com/Zhaoyi-Yan/Shift-Net_pytorch)\n",
        "\n",
        "StructureFlow (2019): [RenYurui/StructureFlow](https://github.com/RenYurui/StructureFlow)\n",
        "- Needs special files.\n",
        "\n",
        "GIN (2020): [rlct1/gin-sg](https://github.com/rlct1/gin-sg) and [rlct1/gin](https://github.com/rlct1/gin)\n",
        "\n",
        "---------------------------\n",
        "\n",
        "No training code:\n",
        "\n",
        "SC-FEGAN (2019): [run-youngjoo/SC-FEGAN](https://github.com/run-youngjoo/SC-FEGAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZc7QSEN2uoR"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4fr0m2LkHhf"
      },
      "source": [
        "# delete validation, logs and checkpoints if needed\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/validation_output\n",
        "!sudo rm -rf /content/lightning_logs\n",
        "!find . -name \"*.ckpt\" -type f -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYW1pF5RzQc"
      },
      "source": [
        "# Training\n",
        "%cd /content/\n",
        "dm = DFNetDataModule(training_path = '/content/data/images/', validation_path = '/content/validation/', batch_size=4)\n",
        "model = CustomTrainClass()\n",
        "#model = model.load_from_checkpoint('/content/Checkpoint_0_450.ckpt') # start training from checkpoint, warning: apperantly global_step will be reset to zero and overwriting validation images, you could manually make an offset\n",
        "\n",
        "#weights_init(model, 'kaiming')\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=150, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xREJTZSbsEa8"
      },
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcHYqzCGleIX"
      },
      "source": [
        "# testing the model\n",
        "dm = DS_green_from_mask('/content/test')\n",
        "model = CustomTrainClass()\n",
        "# GPU\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# GPU with AMP (amp_level='O1' = mixed precision)\n",
        "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O1', max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "# TPU\n",
        "#trainer = pl.Trainer(tpu_cores=8, max_epochs=10, progress_bar_refresh_rate=20, default_root_dir='/content/', callbacks=[CheckpointEveryNSteps(save_step_frequency=1000, save_path='/content/')])\n",
        "trainer.test(model, dm, ckpt_path='/content/Checkpoint_2_2250.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}