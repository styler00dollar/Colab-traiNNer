"""
network.py (13-12-20)
https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/network.py

external_function.py (13-12-20)
https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py

base_function.py (13-12-20)
https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/base_function.py

external_function.py (13-12-20)
https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/master/model/external_function.py

task.py (16-12-20)
https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py
"""

from torch import nn
from torch.nn import Parameter
from torch.nn import init
import copy
import functools
import logging
import torch
import torch.nn.functional as F

logger = logging.getLogger("base")
import pytorch_lightning as pl

###################################################################
# multi scale for image generation
###################################################################


def scale_img(img, size):
    scaled_img = F.interpolate(img, size=size, mode="bilinear", align_corners=True)
    return scaled_img


def scale_pyramid(img, num_scales):
    scaled_imgs = [img]

    s = img.size()

    h = s[2]
    w = s[3]

    for i in range(1, num_scales):
        ratio = 2**i
        nh = h // ratio
        nw = w // ratio
        scaled_img = scale_img(img, size=[nh, nw])
        scaled_imgs.append(scaled_img)

    scaled_imgs.reverse()
    return scaled_imgs


####################################################################################################
# spectral normalization layer to decouple the magnitude of a weight tensor
####################################################################################################


def l2normalize(v, eps=1e-12):
    return v / (v.norm() + eps)


class SpectralNorm(pl.LightningModule):
    """
    spectral normalization
    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'
    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan
    """

    def __init__(self, module, name="weight", power_iterations=1):
        super(SpectralNorm, self).__init__()
        self.module = module
        self.name = name
        self.power_iterations = power_iterations
        if not self._made_params():
            self._make_params()

    def _update_u_v(self):
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        w = getattr(self.module, self.name + "_bar")

        height = w.data.shape[0]
        for _ in range(self.power_iterations):
            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))
            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))

        sigma = u.dot(w.view(height, -1).mv(v))
        setattr(self.module, self.name, w / sigma.expand_as(w))

    def _made_params(self):
        try:
            getattr(self.module, self.name + "_u")
            getattr(self.module, self.name + "_v")
            getattr(self.module, self.name + "_bar")
            return True
        except AttributeError:
            return False

    def _make_params(self):
        w = getattr(self.module, self.name)

        height = w.data.shape[0]
        width = w.view(height, -1).data.shape[1]

        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)
        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)
        u.data = l2normalize(u.data)
        v.data = l2normalize(v.data)
        w_bar = Parameter(w.data)

        del self.module._parameters[self.name]

        self.module.register_parameter(self.name + "_u", u)
        self.module.register_parameter(self.name + "_v", v)
        self.module.register_parameter(self.name + "_bar", w_bar)

    def forward(self, *args):
        self._update_u_v()
        return self.module.forward(*args)


####################################################################################################
# neural style transform loss from neural_style_tutorial of pytorch
####################################################################################################


def GramMatrix(input):
    s = input.size()
    features = input.view(s[0], s[1], s[2] * s[3])
    features_t = torch.transpose(features, 1, 2)
    G = torch.bmm(features, features_t).div(s[1] * s[2] * s[3])
    return G


def img_crop(input, size=224):
    input_cropped = F.upsample(
        input, size=(size, size), mode="bilinear", align_corners=True
    )
    return input_cropped


class Normalization(pl.LightningModule):
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        self.mean = mean.view(-1, 1, 1)
        self.std = std.view(-1, 1, 1)

    def forward(self, input):
        return (input - self.mean) / self.std


######################################################################################
# base function for network structure
######################################################################################


def pluralistic_init_weights(net, init_type="normal", gain=0.02):
    """Get different initial method for the network weights"""

    def init_func(m):
        classname = m.__class__.__name__
        if hasattr(m, "weight") and (
            classname.find("Conv") != -1 or classname.find("Linear") != -1
        ):
            if init_type == "normal":
                init.normal_(m.weight.data, 0.0, gain)
            elif init_type == "xavier":
                init.xavier_normal_(m.weight.data, gain=gain)
            elif init_type == "kaiming":
                init.kaiming_normal_(m.weight.data, a=0, mode="fan_in")
            elif init_type == "orthogonal":
                init.orthogonal_(m.weight.data, gain=gain)
            else:
                raise NotImplementedError(
                    "initialization method [%s] is not implemented" % init_type
                )
            if hasattr(m, "bias") and m.bias is not None:
                init.constant_(m.bias.data, 0.0)
        elif classname.find("BatchNorm2d") != -1:
            init.normal_(m.weight.data, 1.0, 0.02)
            init.constant_(m.bias.data, 0.0)

    # print('initialize network with %s' % init_type)
    logger.info("Initialization method [{:s}]".format(init_type))
    net.apply(init_func)


def get_norm_layer(norm_type="batch"):
    """Get the normalization layer for the networks"""
    if norm_type == "batch":
        norm_layer = functools.partial(nn.BatchNorm2d, momentum=0.1, affine=True)
    elif norm_type == "instance":
        norm_layer = functools.partial(nn.InstanceNorm2d, affine=True)
    elif norm_type == "none":
        norm_layer = None
    else:
        raise NotImplementedError("normalization layer [%s] is not found" % norm_type)
    return norm_layer


def get_nonlinearity_layer(activation_type="PReLU"):
    """Get the activation layer for the networks"""
    if activation_type == "ReLU":
        nonlinearity_layer = nn.ReLU()
    elif activation_type == "SELU":
        nonlinearity_layer = nn.SELU()
    elif activation_type == "LeakyReLU":
        nonlinearity_layer = nn.LeakyReLU(0.1)
    elif activation_type == "PReLU":
        nonlinearity_layer = nn.PReLU()
    else:
        raise NotImplementedError(
            "activation layer [%s] is not found" % activation_type
        )
    return nonlinearity_layer


def print_network(net):
    """print the network"""
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print(net)
    print("total number of parameters: %.3f M" % (num_params / 1e6))


def init_net(net, init_type="normal", activation="relu", gpu_ids=[]):
    """print the network structure and initial the network"""
    print_network(net)

    if len(gpu_ids) > 0:
        assert torch.cuda.is_available()
        net.cuda()
        net = torch.nn.DataParallel(net, gpu_ids)
    init_weights(net, init_type)
    return net


def _freeze(*args):
    """freeze the network for forward process"""
    for module in args:
        if module:
            for p in module.parameters():
                p.requires_grad = False


def _unfreeze(*args):
    """unfreeze the network for parameter update"""
    for module in args:
        if module:
            for p in module.parameters():
                p.requires_grad = True


def spectral_norm(module, use_spect=True):
    """use spectral normal layer to stable the training process"""
    if use_spect:
        return SpectralNorm(module)
    else:
        return module


def coord_conv(
    input_nc, output_nc, use_spect=False, use_coord=False, with_r=False, **kwargs
):
    """use coord convolution layer to add position information"""
    if use_coord:
        return CoordConv(input_nc, output_nc, with_r, use_spect, **kwargs)
    else:
        return spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)


######################################################################################
# Network basic function
######################################################################################
class AddCoords(pl.LightningModule):
    """
    Add Coords to a tensor
    """

    def __init__(self, with_r=False):
        super(AddCoords, self).__init__()
        self.with_r = with_r

    def forward(self, x):
        """
        :param x: shape (batch, channel, x_dim, y_dim)
        :return: shape (batch, channel+2, x_dim, y_dim)
        """
        B, _, x_dim, y_dim = x.size()

        # coord calculate
        xx_channel = torch.arange(x_dim).repeat(B, 1, y_dim, 1).type_as(x)
        yy_cahnnel = (
            torch.arange(y_dim).repeat(B, 1, x_dim, 1).permute(0, 1, 3, 2).type_as(x)
        )
        # normalization
        xx_channel = xx_channel.float() / (x_dim - 1)
        yy_cahnnel = yy_cahnnel.float() / (y_dim - 1)
        xx_channel = xx_channel * 2 - 1
        yy_cahnnel = yy_cahnnel * 2 - 1

        ret = torch.cat([x, xx_channel, yy_cahnnel], dim=1)

        if self.with_r:
            rr = torch.sqrt(xx_channel**2 + yy_cahnnel**2)
            ret = torch.cat([ret, rr], dim=1)

        return ret


class CoordConv(pl.LightningModule):
    """
    CoordConv operation
    """

    def __init__(self, input_nc, output_nc, with_r=False, use_spect=False, **kwargs):
        super(CoordConv, self).__init__()
        self.addcoords = AddCoords(with_r=with_r)
        input_nc = input_nc + 2
        if with_r:
            input_nc = input_nc + 1
        self.conv = spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)

    def forward(self, x):
        ret = self.addcoords(x)
        ret = self.conv(ret)

        return ret


class ResBlock(pl.LightningModule):
    """
    Define an Residual block for different types
    """

    def __init__(
        self,
        input_nc,
        output_nc,
        hidden_nc=None,
        norm_layer=nn.BatchNorm2d,
        nonlinearity=nn.LeakyReLU(),
        sample_type="none",
        use_spect=False,
        use_coord=False,
    ):
        super(ResBlock, self).__init__()

        hidden_nc = output_nc if hidden_nc is None else hidden_nc
        self.sample = True
        if sample_type == "none":
            self.sample = False
        elif sample_type == "up":
            output_nc = output_nc * 4
            self.pool = nn.PixelShuffle(upscale_factor=2)
        elif sample_type == "down":
            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)
        else:
            raise NotImplementedError("sample type [%s] is not found" % sample_type)

        kwargs = {"kernel_size": 3, "stride": 1, "padding": 1}
        kwargs_short = {"kernel_size": 1, "stride": 1, "padding": 0}

        self.conv1 = coord_conv(input_nc, hidden_nc, use_spect, use_coord, **kwargs)
        self.conv2 = coord_conv(hidden_nc, output_nc, use_spect, use_coord, **kwargs)
        self.bypass = coord_conv(
            input_nc, output_nc, use_spect, use_coord, **kwargs_short
        )

        if type(norm_layer) == type(None):
            self.model = nn.Sequential(
                nonlinearity,
                self.conv1,
                nonlinearity,
                self.conv2,
            )
        else:
            self.model = nn.Sequential(
                norm_layer(input_nc),
                nonlinearity,
                self.conv1,
                norm_layer(hidden_nc),
                nonlinearity,
                self.conv2,
            )

        self.shortcut = nn.Sequential(
            self.bypass,
        )

    def forward(self, x):
        if self.sample:
            out = self.pool(self.model(x)) + self.pool(self.shortcut(x))
        else:
            out = self.model(x) + self.shortcut(x)

        return out


class ResBlockEncoderOptimized(pl.LightningModule):
    """
    Define an Encoder block for the first layer of the discriminator and representation network
    """

    def __init__(
        self,
        input_nc,
        output_nc,
        norm_layer=nn.BatchNorm2d,
        nonlinearity=nn.LeakyReLU(),
        use_spect=False,
        use_coord=False,
    ):
        super(ResBlockEncoderOptimized, self).__init__()

        kwargs = {"kernel_size": 3, "stride": 1, "padding": 1}
        kwargs_short = {"kernel_size": 1, "stride": 1, "padding": 0}

        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)
        self.conv2 = coord_conv(output_nc, output_nc, use_spect, use_coord, **kwargs)
        self.bypass = coord_conv(
            input_nc, output_nc, use_spect, use_coord, **kwargs_short
        )

        if type(norm_layer) == type(None):
            self.model = nn.Sequential(
                self.conv1,
                nonlinearity,
                self.conv2,
                nn.AvgPool2d(kernel_size=2, stride=2),
            )
        else:
            self.model = nn.Sequential(
                self.conv1,
                norm_layer(output_nc),
                nonlinearity,
                self.conv2,
                nn.AvgPool2d(kernel_size=2, stride=2),
            )

        self.shortcut = nn.Sequential(
            nn.AvgPool2d(kernel_size=2, stride=2), self.bypass
        )

    def forward(self, x):
        out = self.model(x) + self.shortcut(x)

        return out


class ResBlockDecoder(pl.LightningModule):
    """
    Define a decoder block
    """

    def __init__(
        self,
        input_nc,
        output_nc,
        hidden_nc=None,
        norm_layer=nn.BatchNorm2d,
        nonlinearity=nn.LeakyReLU(),
        use_spect=False,
        use_coord=False,
    ):
        super(ResBlockDecoder, self).__init__()

        hidden_nc = output_nc if hidden_nc is None else hidden_nc

        self.conv1 = spectral_norm(
            nn.Conv2d(input_nc, hidden_nc, kernel_size=3, stride=1, padding=1),
            use_spect,
        )
        self.conv2 = spectral_norm(
            nn.ConvTranspose2d(
                hidden_nc,
                output_nc,
                kernel_size=3,
                stride=2,
                padding=1,
                output_padding=1,
            ),
            use_spect,
        )
        self.bypass = spectral_norm(
            nn.ConvTranspose2d(
                input_nc,
                output_nc,
                kernel_size=3,
                stride=2,
                padding=1,
                output_padding=1,
            ),
            use_spect,
        )

        if type(norm_layer) == type(None):
            self.model = nn.Sequential(
                nonlinearity,
                self.conv1,
                nonlinearity,
                self.conv2,
            )
        else:
            self.model = nn.Sequential(
                norm_layer(input_nc),
                nonlinearity,
                self.conv1,
                norm_layer(hidden_nc),
                nonlinearity,
                self.conv2,
            )

        self.shortcut = nn.Sequential(self.bypass)

    def forward(self, x):
        out = self.model(x) + self.shortcut(x)

        return out


class Output(pl.LightningModule):
    """
    Define the output layer
    """

    def __init__(
        self,
        input_nc,
        output_nc,
        kernel_size=3,
        norm_layer=nn.BatchNorm2d,
        nonlinearity=nn.LeakyReLU(),
        use_spect=False,
        use_coord=False,
    ):
        super(Output, self).__init__()

        kwargs = {"kernel_size": kernel_size, "padding": 0, "bias": True}

        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)

        if type(norm_layer) == type(None):
            self.model = nn.Sequential(
                nonlinearity,
                nn.ReflectionPad2d(int(kernel_size / 2)),
                self.conv1,
                nn.Tanh(),
            )
        else:
            self.model = nn.Sequential(
                norm_layer(input_nc),
                nonlinearity,
                nn.ReflectionPad2d(int(kernel_size / 2)),
                self.conv1,
                nn.Tanh(),
            )

    def forward(self, x):
        out = self.model(x)

        return out


class Auto_Attn(pl.LightningModule):
    """Short+Long attention Layer"""

    def __init__(self, input_nc, norm_layer=nn.BatchNorm2d):
        super(Auto_Attn, self).__init__()
        self.input_nc = input_nc

        self.query_conv = nn.Conv2d(input_nc, input_nc // 4, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))
        self.alpha = nn.Parameter(torch.zeros(1))

        self.softmax = nn.Softmax(dim=-1)

        self.model = ResBlock(
            int(input_nc * 2), input_nc, input_nc, norm_layer=norm_layer, use_spect=True
        )

    def forward(self, x, pre=None, mask=None):
        """
        inputs :
            x : input feature maps( B X C X W X H)
        returns :
            out : self attention value + input feature
            attention: B X N X N (N is Width*Height)
        """
        B, C, W, H = x.size()
        proj_query = self.query_conv(x).view(B, -1, W * H)  # B X (N)X C
        proj_key = proj_query  # B X C x (N)

        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)  # transpose check
        attention = self.softmax(energy)  # BX (N) X (N)
        proj_value = x.view(B, -1, W * H)  # B X C X N

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(B, C, W, H)

        out = self.gamma * out + x

        if type(pre) != type(None):
            # using long distance attention layer to copy information from valid regions
            context_flow = torch.bmm(
                pre.view(B, -1, W * H), attention.permute(0, 2, 1)
            ).view(B, -1, W, H)
            context_flow = self.alpha * (1 - mask) * context_flow + (mask) * pre
            out = self.model(torch.cat([out, context_flow], dim=1))

        return out, attention


from torch import nn


####################################################################################################
# spectral normalization layer to decouple the magnitude of a weight tensor
####################################################################################################


def l2normalize(v, eps=1e-12):
    return v / (v.norm() + eps)


class SpectralNorm(pl.LightningModule):
    """
    spectral normalization
    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'
    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan
    """

    def __init__(self, module, name="weight", power_iterations=1):
        super(SpectralNorm, self).__init__()
        self.module = module
        self.name = name
        self.power_iterations = power_iterations
        if not self._made_params():
            self._make_params()

    def _update_u_v(self):
        u = getattr(self.module, self.name + "_u")
        v = getattr(self.module, self.name + "_v")
        w = getattr(self.module, self.name + "_bar")

        height = w.data.shape[0]
        for _ in range(self.power_iterations):
            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))
            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))

        sigma = u.dot(w.view(height, -1).mv(v))
        setattr(self.module, self.name, w / sigma.expand_as(w))

    def _made_params(self):
        try:
            getattr(self.module, self.name + "_u")
            getattr(self.module, self.name + "_v")
            getattr(self.module, self.name + "_bar")
            return True
        except AttributeError:
            return False

    def _make_params(self):
        w = getattr(self.module, self.name)

        height = w.data.shape[0]
        width = w.view(height, -1).data.shape[1]

        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)
        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)
        u.data = l2normalize(u.data)
        v.data = l2normalize(v.data)
        w_bar = Parameter(w.data)

        del self.module._parameters[self.name]

        self.module.register_parameter(self.name + "_u", u)
        self.module.register_parameter(self.name + "_v", v)
        self.module.register_parameter(self.name + "_bar", w_bar)

    def forward(self, *args):
        self._update_u_v()
        return self.module.forward(*args)


####################################################################################################
# adversarial loss for different gan mode
####################################################################################################


class GANLoss(pl.LightningModule):
    """Define different GAN objectives.
    The GANLoss class abstracts away the need to create the target label tensor
    that has the same size as the input.
    """

    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):
        """Initialize the GANLoss class.
        Parameters:
            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.
            target_real_label (bool) - - label for a real image
            target_fake_label (bool) - - label of a fake image
        Note: Do not use sigmoid as the last layer of Discriminator.
        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.
        """
        super(GANLoss, self).__init__()
        self.register_buffer("real_label", torch.tensor(target_real_label))
        self.register_buffer("fake_label", torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        if gan_mode == "lsgan":
            self.loss = nn.MSELoss()
        elif gan_mode == "vanilla":
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode == "hinge":
            self.loss = nn.ReLU()
        elif gan_mode == "wgangp":
            self.loss = None
        else:
            raise NotImplementedError("gan mode %s not implemented" % gan_mode)

    def __call__(self, prediction, target_is_real, is_disc=False):
        """Calculate loss given Discriminator's output and grount truth labels.
        Parameters:
            prediction (tensor) - - tpyically the prediction output from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images
        Returns:
            the calculated loss.
        """
        if self.gan_mode in ["lsgan", "vanilla"]:
            labels = (
                (self.real_label if target_is_real else self.fake_label)
                .expand_as(prediction)
                .type_as(prediction)
            )
            loss = self.loss(prediction, labels)
        elif self.gan_mode in ["hinge", "wgangp"]:
            if is_disc:
                if target_is_real:
                    prediction = -prediction
                if self.gan_mode == "hinge":
                    loss = self.loss(1 + prediction).mean()
                elif self.gan_mode == "wgangp":
                    loss = prediction.mean()
            else:
                loss = -prediction.mean()
        return loss


def cal_gradient_penalty(
    netD, real_data, fake_data, type="mixed", constant=1.0, lambda_gp=10.0
):
    """Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028
    Arguments:
        netD (network)              -- discriminator network
        real_data (tensor array)    -- real images
        fake_data (tensor array)    -- generated images from the generator
        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].
        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2
        lambda_gp (float)           -- weight for this loss
    Returns the gradient penalty loss
    """
    if lambda_gp > 0.0:
        if (
            type == "real"
        ):  # either use real images, fake images, or a linear interpolation of two.
            interpolatesv = real_data
        elif type == "fake":
            interpolatesv = fake_data
        elif type == "mixed":
            alpha = torch.rand(real_data.shape[0], 1)
            alpha = (
                alpha.expand(
                    real_data.shape[0], real_data.nelement() // real_data.shape[0]
                )
                .contiguous()
                .view(*real_data.shape)
            )
            alpha = alpha.type_as(real_data)
            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)
        else:
            raise NotImplementedError("{} not implemented".format(type))
        interpolatesv.requires_grad_(True)
        disc_interpolates = netD(interpolatesv)
        gradients = torch.autograd.grad(
            outputs=disc_interpolates,
            inputs=interpolatesv,
            grad_outputs=torch.ones(disc_interpolates.size()).type_as(real_data),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )
        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data
        gradient_penalty = (
            ((gradients + 1e-16).norm(2, dim=1) - constant) ** 2
        ).mean() * lambda_gp  # added eps
        return gradient_penalty, gradients
    else:
        return 0.0, None


####################################################################################################
# neural style transform loss from neural_style_tutorial of pytorch
####################################################################################################


def ContentLoss(input, target):
    target = target.detach()
    loss = F.l1_loss(input, target)
    return loss


def GramMatrix(input):
    s = input.size()
    features = input.view(s[0], s[1], s[2] * s[3])
    features_t = torch.transpose(features, 1, 2)
    G = torch.bmm(features, features_t).div(s[1] * s[2] * s[3])
    return G


def StyleLoss(input, target):
    target = GramMatrix(target).detach()
    input = GramMatrix(input)
    loss = F.l1_loss(input, target)
    return loss


def img_crop(input, size=224):
    input_cropped = F.upsample(
        input, size=(size, size), mode="bilinear", align_corners=True
    )
    return input_cropped


class Normalization(pl.LightningModule):
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        self.mean = mean.view(-1, 1, 1)
        self.std = std.view(-1, 1, 1)

    def forward(self, input):
        return (input - self.mean) / self.std


class get_features(pl.LightningModule):
    def __init__(self, cnn):
        super(get_features, self).__init__()

        vgg = copy.deepcopy(cnn)

        self.conv1 = nn.Sequential(vgg[0], vgg[1], vgg[2], vgg[3], vgg[4])
        self.conv2 = nn.Sequential(vgg[5], vgg[6], vgg[7], vgg[8], vgg[9])
        self.conv3 = nn.Sequential(
            vgg[10], vgg[11], vgg[12], vgg[13], vgg[14], vgg[15], vgg[16]
        )
        self.conv4 = nn.Sequential(
            vgg[17], vgg[18], vgg[19], vgg[20], vgg[21], vgg[22], vgg[23]
        )
        self.conv5 = nn.Sequential(
            vgg[24], vgg[25], vgg[26], vgg[27], vgg[28], vgg[29], vgg[30]
        )

    def forward(self, input, layers):
        input = img_crop(input)
        output = []
        for i in range(1, layers):
            layer = getattr(self, "conv" + str(i))
            input = layer(input)
            output.append(input)
        return output


##############################################################################################################
# Network function
##############################################################################################################
def define_e(
    input_nc=3,
    ngf=64,
    z_nc=512,
    img_f=512,
    L=6,
    layers=5,
    norm="none",
    activation="ReLU",
    use_spect=True,
    use_coord=False,
    init_type="orthogonal",
    gpu_ids=[],
):
    net = ResEncoder(
        input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord
    )

    return init_net(net, init_type, activation, gpu_ids)


def define_g(
    output_nc=3,
    ngf=64,
    z_nc=512,
    img_f=512,
    L=1,
    layers=5,
    norm="instance",
    activation="ReLU",
    output_scale=1,
    use_spect=True,
    use_coord=False,
    use_attn=True,
    init_type="orthogonal",
    gpu_ids=[],
):
    net = ResGenerator(
        output_nc,
        ngf,
        z_nc,
        img_f,
        L,
        layers,
        norm,
        activation,
        output_scale,
        use_spect,
        use_coord,
        use_attn,
    )

    return init_net(net, init_type, activation, gpu_ids)


def define_d(
    input_nc=3,
    ndf=64,
    img_f=512,
    layers=6,
    norm="none",
    activation="LeakyReLU",
    use_spect=True,
    use_coord=False,
    use_attn=True,
    model_type="ResDis",
    init_type="orthogonal",
    gpu_ids=[],
):
    if model_type == "ResDis":
        net = ResDiscriminator(
            input_nc,
            ndf,
            img_f,
            layers,
            norm,
            activation,
            use_spect,
            use_coord,
            use_attn,
        )
    elif model_type == "PatchDis":
        net = PatchDiscriminator(
            input_nc,
            ndf,
            img_f,
            layers,
            norm,
            activation,
            use_spect,
            use_coord,
            use_attn,
        )

    return init_net(net, init_type, activation, gpu_ids)


#############################################################################################################
# Network structure
#############################################################################################################
class ResEncoder(pl.LightningModule):
    """
    ResNet Encoder Network
    :param input_nc: number of channels in input
    :param ngf: base filter channel
    :param z_nc: latent channels
    :param img_f: the largest feature channels
    :param L: Number of refinements of density
    :param layers: down and up sample layers
    :param norm: normalization function 'instance, batch, group'
    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'
    """

    def __init__(
        self,
        input_nc=3,
        ngf=64,
        z_nc=128,
        img_f=1024,
        L=6,
        layers=6,
        norm="none",
        activation="ReLU",
        use_spect=True,
        use_coord=False,
    ):
        super(ResEncoder, self).__init__()

        self.layers = layers
        self.z_nc = z_nc
        self.L = L

        norm_layer = get_norm_layer(norm_type=norm)
        nonlinearity = get_nonlinearity_layer(activation_type=activation)
        # encoder part
        self.block0 = ResBlockEncoderOptimized(
            input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord
        )

        mult = 1
        for i in range(layers - 1):
            mult_prev = mult
            mult = min(2 ** (i + 1), img_f // ngf)
            block = ResBlock(
                ngf * mult_prev,
                ngf * mult,
                ngf * mult_prev,
                norm_layer,
                nonlinearity,
                "down",
                use_spect,
                use_coord,
            )
            setattr(self, "encoder" + str(i), block)

        # inference part
        for i in range(self.L):
            block = ResBlock(
                ngf * mult,
                ngf * mult,
                ngf * mult,
                norm_layer,
                nonlinearity,
                "none",
                use_spect,
                use_coord,
            )
            setattr(self, "infer_prior" + str(i), block)

        self.posterior = ResBlock(
            ngf * mult,
            2 * z_nc,
            ngf * mult,
            norm_layer,
            nonlinearity,
            "none",
            use_spect,
            use_coord,
        )
        self.prior = ResBlock(
            ngf * mult,
            2 * z_nc,
            ngf * mult,
            norm_layer,
            nonlinearity,
            "none",
            use_spect,
            use_coord,
        )

    def forward(self, img_m, img_c=None):
        """
        :param img_m: image with mask regions I_m
        :param img_c: complement of I_m, the mask regions
        :return distribution: distribution of mask regions, for training we have two paths, testing one path
        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention
        """

        if type(img_c) != type(None):
            img = torch.cat([img_m, img_c], dim=0)
        else:
            img = img_m

        # encoder part
        out = self.block0(img)
        feature = [out]
        for i in range(self.layers - 1):
            model = getattr(self, "encoder" + str(i))
            out = model(out)
            feature.append(out)

        # infer part
        # during the training, we have two paths, during the testing, we only have one paths
        if type(img_c) != type(None):
            distribution = self.two_paths(out)
            return distribution, feature
        else:
            distribution = self.one_path(out)
            return distribution, feature

    def one_path(self, f_in):
        """one path for baseline training or testing"""
        f_m = f_in
        distribution = []

        # infer state
        for i in range(self.L):
            infer_prior = getattr(self, "infer_prior" + str(i))
            f_m = infer_prior(f_m)

        # get distribution
        o = self.prior(f_m)
        q_mu, q_std = torch.split(o, self.z_nc, dim=1)
        distribution.append([q_mu, F.softplus(q_std)])

        return distribution

    def two_paths(self, f_in):
        """two paths for the training"""
        f_m, f_c = f_in.chunk(2)
        distributions = []

        # get distribution
        o = self.posterior(f_c)
        p_mu, p_std = torch.split(o, self.z_nc, dim=1)
        distribution = self.one_path(f_m)
        distributions.append(
            [p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]]
        )

        return distributions


class ResGenerator(pl.LightningModule):
    """
    ResNet Generator Network
    :param output_nc: number of channels in output
    :param ngf: base filter channel
    :param z_nc: latent channels
    :param img_f: the largest feature channels
    :param L: Number of refinements of density
    :param layers: down and up sample layers
    :param norm: normalization function 'instance, batch, group'
    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'
    :param output_scale: Different output scales
    """

    def __init__(
        self,
        output_nc=3,
        ngf=64,
        z_nc=128,
        img_f=1024,
        L=1,
        layers=6,
        norm="batch",
        activation="ReLU",
        output_scale=1,
        use_spect=True,
        use_coord=False,
        use_attn=True,
    ):
        super(ResGenerator, self).__init__()

        self.layers = layers
        self.L = L
        self.output_scale = output_scale
        self.use_attn = use_attn

        norm_layer = get_norm_layer(norm_type=norm)
        nonlinearity = get_nonlinearity_layer(activation_type=activation)
        # latent z to feature
        mult = min(2 ** (layers - 1), img_f // ngf)
        self.generator = ResBlock(
            z_nc,
            ngf * mult,
            ngf * mult,
            None,
            nonlinearity,
            "none",
            use_spect,
            use_coord,
        )

        # transform
        for i in range(self.L):
            block = ResBlock(
                ngf * mult,
                ngf * mult,
                ngf * mult,
                None,
                nonlinearity,
                "none",
                use_spect,
                use_coord,
            )
            setattr(self, "generator" + str(i), block)

        # decoder part
        for i in range(layers):
            mult_prev = mult
            mult = min(2 ** (layers - i - 1), img_f // ngf)
            if i > layers - output_scale:
                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)
                upconv = ResBlockDecoder(
                    ngf * mult_prev + output_nc,
                    ngf * mult,
                    ngf * mult,
                    norm_layer,
                    nonlinearity,
                    use_spect,
                    use_coord,
                )
            else:
                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)
                upconv = ResBlockDecoder(
                    ngf * mult_prev,
                    ngf * mult,
                    ngf * mult,
                    norm_layer,
                    nonlinearity,
                    use_spect,
                    use_coord,
                )
            setattr(self, "decoder" + str(i), upconv)
            # output part
            if i > layers - output_scale - 1:
                outconv = Output(
                    ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord
                )
                setattr(self, "out" + str(i), outconv)
            # short+long term attention part
            if i == 1 and use_attn:
                attn = Auto_Attn(ngf * mult, None)
                setattr(self, "attn" + str(i), attn)

    def forward(self, z, f_m=None, f_e=None, mask=None):
        """
        ResNet Generator Network
        :param z: latent vector
        :param f_m: feature of valid regions for conditional VAG-GAN
        :param f_e: previous encoder feature for short+long term attention layer
        :return results: different scale generation outputs
        """

        f = self.generator(z)
        for i in range(self.L):
            generator = getattr(self, "generator" + str(i))
            f = generator(f)

        # the features come from mask regions and valid regions, we directly add them together
        out = f_m + f
        results = []
        attn = 0
        for i in range(self.layers):
            model = getattr(self, "decoder" + str(i))
            out = model(out)
            if i == 1 and self.use_attn:
                # auto attention
                model = getattr(self, "attn" + str(i))
                out, attn = model(out, f_e, mask)
            if i > self.layers - self.output_scale - 1:
                model = getattr(self, "out" + str(i))
                output = model(out)
                results.append(output)
                out = torch.cat([out, output], dim=1)

        return results, attn


# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/model/pluralistic_model.py
# https://github.com/lyndonzheng/Pluralistic-Inpainting/blob/1ca1855615fed8b686ca218c6494f455860f9996/util/task.py
class PluralisticGenerator(pl.LightningModule):
    def __init__(
        self,
        ngf_E=32,
        z_nc_E=128,
        img_f_E=128,
        layers_E=5,
        norm_E="none",
        activation_E="LeakyReLU",
        ngf_G=32,
        z_nc_G=128,
        img_f_G=128,
        L_G=0,
        output_scale_G=1,
        norm_G="instance",
        activation_G="LeakyReLU",
        train_paths="two",
    ):
        super().__init__()
        self.net_E = ResEncoder(
            ngf=ngf_E,
            z_nc=z_nc_E,
            img_f=img_f_E,
            layers=layers_E,
            norm=norm_E,
            activation=activation_E,
        )
        self.net_G = ResGenerator(
            ngf=ngf_G,
            z_nc=z_nc_G,
            img_f=img_f_G,
            L=L_G,
            layers=5,
            output_scale=output_scale_G,
            norm=norm_G,
            activation=activation_G,
        )
        self.train_paths = train_paths

    def get_distribution(self, distributions, mask):
        """Calculate encoder distribution for img_m, img_c"""
        # get distribution
        sum_valid = (torch.mean(mask.view(mask.size(0), -1), dim=1) - 1e-5).view(
            -1, 1, 1, 1
        )
        m_sigma = 1 / (1 + ((sum_valid - 0.8) * 8).exp_())
        p_distribution, q_distribution, kl_rec, kl_g = 0, 0, 0, 0
        self.distribution = []
        for distribution in distributions:
            p_mu, p_sigma, q_mu, q_sigma = distribution
            # the assumption distribution for different mask regions
            m_distribution = torch.distributions.Normal(
                torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma)
            )
            # m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), torch.ones_like(p_sigma))
            # the post distribution from mask regions
            p_distribution = torch.distributions.Normal(p_mu, p_sigma)
            p_distribution_fix = torch.distributions.Normal(
                p_mu.detach(), p_sigma.detach()
            )
            # the prior distribution from valid region
            q_distribution = torch.distributions.Normal(q_mu, q_sigma)

            # kl divergence
            kl_rec += torch.distributions.kl_divergence(m_distribution, p_distribution)
            if self.train_paths == "one":
                kl_g += torch.distributions.kl_divergence(
                    m_distribution, q_distribution
                )
            elif self.train_paths == "two":
                kl_g += torch.distributions.kl_divergence(
                    p_distribution_fix, q_distribution
                )
            self.distribution.append(
                [
                    torch.zeros_like(p_mu),
                    m_sigma * torch.ones_like(p_sigma),
                    p_mu,
                    p_sigma,
                    q_mu,
                    q_sigma,
                ]
            )

        return p_distribution, q_distribution, kl_rec, kl_g

    def get_G_inputs(self, p_distribution, q_distribution, f, mask):
        """Process the encoder feature and distributions for generation network"""
        f_m = torch.cat([f[-1].chunk(2)[0], f[-1].chunk(2)[0]], dim=0)
        f_e = torch.cat([f[2].chunk(2)[0], f[2].chunk(2)[0]], dim=0)
        scale_mask = scale_img(mask, size=[f_e.size(2), f_e.size(3)])
        mask = torch.cat(
            [scale_mask.chunk(3, dim=1)[0], scale_mask.chunk(3, dim=1)[0]], dim=0
        )
        z_p = p_distribution.rsample()
        z_q = q_distribution.rsample()
        z = torch.cat([z_p, z_q], dim=0)
        return z, f_m, f_e, mask

    def forward(self, images, img_inverted, masks):
        distributions, f = self.net_E(images, img_inverted)
        p_distribution, q_distribution, kl_rec, kl_g = self.get_distribution(
            distributions, masks
        )
        z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f, masks)
        results, attn = self.net_G(z, f_m, f_e, mask)

        self.img_rec = []
        self.img_g = []
        for result in results:
            img_rec, img_g = result.chunk(2)
            self.img_rec.append(img_rec)
            self.img_g.append(img_g)

        return self.img_g[-1].detach(), kl_rec, kl_g
